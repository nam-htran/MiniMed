# run/5_ingest_custom_data.py
import os
import json
import logging
import glob
import re
from pathlib import Path
from src.utils.neo4j_connect import db_connector
from src.utils.local_llm import local_llm

# --- C·∫§U H√åNH ---
DATA_DIR = Path("data/custom_knowledge")
DATA_DIR.mkdir(parents=True, exist_ok=True)

# Logging ra m√†n h√¨nh
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)
formatter = logging.Formatter('%(asctime)s - %(message)s', datefmt='%H:%M:%S')
console_handler.setFormatter(formatter)

logger = logging.getLogger("KG_BUILDER")
logger.setLevel(logging.INFO)
if not logger.handlers:
    logger.addHandler(console_handler)

class KnowledgeExtractor:
    def __init__(self):
        try:
            local_llm.load_model()
        except Exception as e:
            logger.error(f"‚ùå Kh√¥ng th·ªÉ load Local LLM: {e}")
            raise e

    def clean_json_response(self, text):
        """L√†m s·∫°ch chu·ªói JSON t·ª´ output c·ªßa LLM (Robust Version)"""
        # 1. Lo·∫°i b·ªè th·∫ª <think> v√† markdown
        text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)
        text = re.sub(r'```json', '', text)
        text = re.sub(r'```', '', text)
        
        # 2. T√¨m kh·ªëi JSON th√¥ t·ª´ d·∫•u { ƒë·∫ßu ti√™n ƒë·∫øn d·∫•u } cu·ªëi c√πng
        start_idx = text.find('{')
        end_idx = text.rfind('}')
        
        if start_idx == -1 or end_idx == -1 or end_idx <= start_idx:
            return "{}"
        
        json_str = text[start_idx : end_idx + 1]
        
        # 3. Fix l·ªói ph·ªï bi·∫øn: D·∫•u ph·∫©y th·ª´a ·ªü cu·ªëi list/dict (VD: {"a": 1,})
        # Regex n√†y t√¨m d·∫•u ph·∫©y ƒë·ª©ng tr∆∞·ªõc d·∫•u ƒë√≥ng ngo·∫∑c v√† x√≥a n√≥
        json_str = re.sub(r',\s*([\]}])', r'\1', json_str)
        
        return json_str.strip()

    def extract_graph_from_text(self, text_chunk):
        # One-shot Prompt: Cung c·∫•p v√≠ d·ª• c·ª• th·ªÉ ƒë·ªÉ ƒë·ªãnh h∆∞·ªõng model
        prompt = f"""
        You are a medical data extractor. Convert the text into a JSON Knowledge Graph.
        
        ### EXAMPLE:
        Text: "Metformin treats Type 2 Diabetes but may cause Nausea."
        JSON Output:
        {{
            "nodes": [
                {{"id": "Metformin", "label": "Drug"}},
                {{"id": "Type 2 Diabetes", "label": "Disease"}},
                {{"id": "Nausea", "label": "Symptom"}}
            ],
            "edges": [
                {{"source": "Metformin", "target": "Type 2 Diabetes", "type": "TREATS"}},
                {{"source": "Metformin", "target": "Nausea", "type": "CAUSES"}}
            ]
        }}
        
        ### TASK:
        Text: "{text_chunk}"
        
        Required: Output VALID JSON only. No explanations.
        """
        
        try:
            # --- T·ªêI ∆ØU G·ªåI H√ÄM ---
            messages = [
                {"role": "system", "content": "You are a JSON extractor. Output valid JSON only."},
                {"role": "user", "content": prompt}
            ]
            
            input_ids = local_llm.tokenizer.apply_chat_template(
                messages, add_generation_prompt=True, return_tensors="pt"
            ).to(local_llm.model.device)
            
            attention_mask = import_torch().ones_like(input_ids)
            
            print("   ‚Ü≥ ü§ñ AI ƒëang suy nghƒ©...", end="\r")
            
            with import_torch().no_grad():
                outputs = local_llm.model.generate(
                    input_ids,
                    attention_mask=attention_mask,
                    pad_token_id=local_llm.tokenizer.pad_token_id,
                    max_new_tokens=1024, # ƒê·ªß d√†i cho JSON
                    temperature=0.1,     # Th·∫•p ƒë·ªÉ ·ªïn ƒë·ªãnh
                    do_sample=False      # Greedy search
                )
            
            raw_response = local_llm.tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)
            print("   ‚Ü≥ ‚úÖ AI ƒë√£ tr·∫£ l·ªùi!       ") 

            clean_json = self.clean_json_response(raw_response)
            
            # --- PARSE JSON ---
            try:
                graph_data = json.loads(clean_json)
            except json.JSONDecodeError as e:
                print(f"   ‚Ü≥ ‚ö†Ô∏è JSON Parse Error: {str(e)[:50]}")
                # print(f"DEBUG: {clean_json}") # Uncomment ƒë·ªÉ debug
                return None
            
            # Chu·∫©n h√≥a keys
            if "nodes" not in graph_data: graph_data["nodes"] = []
            if "edges" not in graph_data: graph_data["edges"] = []
            
            return graph_data
            
        except Exception as e:
            print(f"   ‚Ü≥ ‚ùå L·ªói h·ªá th·ªëng: {str(e)[:50]}...")
            return None

    def ingest_to_neo4j(self, graph_data):
        if not graph_data or not db_connector: return

        # ƒê·∫£m b·∫£o source/target trong edges ƒë·ªÅu t·ªìn t·∫°i trong nodes ƒë·ªÉ tr√°nh l·ªói orphan edges
        # Trong th·ª±c t·∫ø, c√≥ th·ªÉ c·∫ßn merge nodes tr∆∞·ªõc
        
        node_query = """
        UNWIND $nodes AS n MERGE (node {name: n.id}) 
        ON CREATE SET node.id = n.id, node.source='User_Upload' 
        WITH node, n CALL apoc.create.addLabels(node, [n.label]) YIELD node as l RETURN count(l)
        """
        edge_query = """
        UNWIND $edges AS e MATCH (s {name: e.source}), (t {name: e.target}) 
        MERGE (s)-[r:RELATED {type: e.type, provenance:'User_Upload'}]->(t) RETURN count(r)
        """

        try:
            if graph_data.get("nodes"):
                db_connector.run_query(node_query, {"nodes": graph_data["nodes"]})
            if graph_data.get("edges"):
                db_connector.run_query(edge_query, {"edges": graph_data["edges"]})
            logger.info(f"   + DB: Saved {len(graph_data.get('nodes', []))} nodes, {len(graph_data.get('edges', []))} edges.")
        except Exception as e:
            logger.error(f"‚ùå DB Error: {e}")

def import_torch():
    import torch
    return torch

def main():
    print("üöÄ B·∫Øt ƒë·∫ßu n·∫°p d·ªØ li·ªáu (Robust Mode)")
    if db_connector is None: 
        print("‚ùå Kh√¥ng c√≥ k·∫øt n·ªëi DB.")
        return

    files = glob.glob(str(DATA_DIR / "*.txt"))
    if not files:
        print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file .txt n√†o trong data/custom_knowledge")
        print("   -> T·∫°o file sample...")
        sample_file = DATA_DIR / "sample_vn.txt"
        with open(sample_file, "w", encoding="utf-8") as f:
            f.write("C√¢y ch√≥ ƒë·∫ª (Di·ªáp h·∫° ch√¢u) h·ªó tr·ª£ tr·ªã vi√™m gan B nh∆∞ng g√¢y h·∫° huy·∫øt √°p.")
        files = [str(sample_file)]

    extractor = KnowledgeExtractor()
    
    for file_path in files:
        logger.info(f"üìÇ File: {os.path.basename(file_path)}")
        with open(file_path, "r", encoding="utf-8") as f: text = f.read()
        
        # Chia nh·ªè text
        chunk_size = 800 
        chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]
        
        for idx, chunk in enumerate(chunks):
            logger.info(f"   Processing chunk {idx+1}/{len(chunks)}...")
            graph_data = extractor.extract_graph_from_text(chunk)
            if graph_data: extractor.ingest_to_neo4j(graph_data)

    local_llm.unload()
    if db_connector: db_connector.close()
    print("\nüéâ Ho√†n t·∫•t!")

if __name__ == "__main__":
    main()