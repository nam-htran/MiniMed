# run/build_faiss_index.py
import json
import logging
import numpy as np
import faiss
import os
from pathlib import Path
from tqdm import tqdm
from sentence_transformers import SentenceTransformer
from src.utils.neo4j_connect import db_connector

# --- CONFIG ---
MODEL_NAME = "BAAI/bge-small-en-v1.5" # Model nh·ªè, nhanh, hi·ªáu qu·∫£
OUTPUT_DIR = Path("data/kg_index")
INDEX_PATH = OUTPUT_DIR / "kg_faiss.index"
META_PATH = OUTPUT_DIR / "kg_nodes_meta.json"
BATCH_SIZE = 5000  # X·ª≠ l√Ω 5000 node m·ªói l·∫ßn ƒë·ªÉ ti·∫øt ki·ªám RAM

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger("FAISS_BUILDER")

def main():
    # 1. Ki·ªÉm tra n·∫øu Index ƒë√£ t·ªìn t·∫°i th√¨ Skip
    if INDEX_PATH.exists() and META_PATH.exists():
        print(f"\n‚è© [SKIP] FAISS Index ƒë√£ t·ªìn t·∫°i t·∫°i: {OUTPUT_DIR}")
        print("üëâ N·∫øu b·∫°n v·ª´a n·∫°p d·ªØ li·ªáu m·ªõi v√† mu·ªën build l·∫°i, h√£y x√≥a th∆∞ m·ª•c 'data/kg_index' r·ªìi ch·∫°y l·∫°i script n√†y.")
        return

    # 2. Ki·ªÉm tra k·∫øt n·ªëi DB
    if db_connector is None:
        logger.error("‚ùå Kh√¥ng c√≥ k·∫øt n·ªëi Neo4j. Vui l√≤ng ki·ªÉm tra Docker.")
        return

    # T·∫°o th∆∞ m·ª•c output
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

    # 3. ƒê·∫øm t·ªïng s·ªë node ƒë·ªÉ hi·ªÉn th·ªã thanh ti·∫øn tr√¨nh
    logger.info("üìä ƒêang ƒë·∫øm t·ªïng s·ªë node c·∫ßn index...")
    count_query = "MATCH (n) WHERE n.name IS NOT NULL RETURN count(n) as total"
    try:
        res = db_connector.run_query(count_query)
        total_nodes = res[0]['total']
        logger.info(f"   -> T·ªïng s·ªë node: {total_nodes}")
    except Exception as e:
        logger.error(f"‚ùå L·ªói khi ƒë·∫øm node: {e}")
        return

    # 4. Kh·ªüi t·∫°o Model & Index
    logger.info(f"üß† Loading SentenceTransformer: {MODEL_NAME}")
    encoder = SentenceTransformer(MODEL_NAME)
    
    # S·ª≠ d·ª•ng IndexFlatIP (Inner Product) cho cosine similarity (khi vectors ƒë√£ normalize)
    # Lo·∫°i n√†y ti·∫øt ki·ªám RAM h∆°n HNSW v√† v·∫´n ƒë·ªß nhanh cho v√†i tri·ªáu node.
    embedding_dim = 384
    index = faiss.IndexFlatIP(embedding_dim) 

    all_meta = []
    
    # 5. V√≤ng l·∫∑p Batch Processing (Ti·∫øt ki·ªám RAM)
    logger.info("üöÄ B·∫Øt ƒë·∫ßu qu√° tr√¨nh Indexing theo batch...")
    
    query = """
    MATCH (n)
    WHERE n.name IS NOT NULL
    RETURN elementId(n) AS node_id, labels(n) AS labels, n.name AS name
    ORDER BY elementId(n)
    SKIP $skip LIMIT $limit
    """
    
    skip = 0
    pbar = tqdm(total=total_nodes, desc="Indexing Nodes", unit="node")

    while skip < total_nodes:
        # A. Fetch Batch t·ª´ Neo4j
        rows = db_connector.run_query(query, {"skip": skip, "limit": BATCH_SIZE})
        if not rows:
            break
            
        batch_meta = []
        batch_texts = []
        
        # B. Prepare Data
        for r in rows:
            # X·ª≠ l√Ω an to√†n d·ªØ li·ªáu
            lbls = r.get("labels", [])
            lbl = lbls[0] if lbls else "Unknown"
            name = r.get("name", "Unknown")
            nid = str(r.get("node_id"))
            
            # L∆∞u metadata g·ªçn nh·∫π
            meta_item = {
                "node_id": nid,
                "labels": lbls,
                "name": name
            }
            batch_meta.append(meta_item)
            
            # Text ƒë·ªÉ embed: "Name (Label)"
            batch_texts.append(f"{name} ({lbl})")
        
        # C. Encode Batch (GPU/CPU)
        if batch_texts:
            embeddings = encoder.encode(
                batch_texts,
                batch_size=256,
                show_progress_bar=False,
                normalize_embeddings=True # Quan tr·ªçng cho FlatIP/Cosine
            )
            
            # D. Add to FAISS Index
            index.add(np.asarray(embeddings, dtype="float32"))
            
            # E. Append Meta
            all_meta.extend(batch_meta)
        
        skip += BATCH_SIZE
        pbar.update(len(rows))

    pbar.close()

    # 6. L∆∞u xu·ªëng ƒëƒ©a
    logger.info(f"üíæ ƒêang l∆∞u FAISS index v√†o {INDEX_PATH}...")
    faiss.write_index(index, str(INDEX_PATH))

    logger.info(f"üíæ ƒêang l∆∞u Metadata v√†o {META_PATH}...")
    with open(META_PATH, "w", encoding="utf-8") as f:
        json.dump(all_meta, f, ensure_ascii=False, indent=None) # indent=None cho file nh·ªè g·ªçn

    logger.info("üéâ Ho√†n t·∫•t build FAISS index!")
    if db_connector:
        db_connector.close()

if __name__ == "__main__":
    main()