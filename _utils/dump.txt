
===== .\build_umls.py =====
# T·ªáp: build_umls.py (PHI√äN B·∫¢N S·ª¨A L·ªñI L·ªåC)

import sqlite3
import logging
from pathlib import Path
from tqdm import tqdm

# --- C·∫§U H√åNH ---
MRCONSO_PATH = Path("data/umls/MRCONSO.RRF")
MRSTY_PATH = Path("data/umls/MRSTY.RRF")
OUTPUT_DB_PATH = Path("data/umls/umls_lookup.db")

# Ghi ch√∫: Ch√∫ng ta s·∫Ω kh√¥ng d√πng TARGET_SABS n·ªØa ƒë·ªÉ ƒë·∫£m b·∫£o kh√¥ng b·ªè s√≥t d·ªØ li·ªáu
# TARGET_SABS = {"SNOMEDCT_US", "RXNORM", "MSH", "NCI", "HGNC", "GO", "MDR"}

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger("UMLS_BUILDER")

def build_db():
    if OUTPUT_DB_PATH.exists():
        logger.warning(f"ƒê√£ t√¨m th·∫•y file DB c≈© t·∫°i '{OUTPUT_DB_PATH}'. S·∫Ω x√≥a v√† x√¢y d·ª±ng l·∫°i.")
        OUTPUT_DB_PATH.unlink()

    if not MRCONSO_PATH.exists() or not MRSTY_PATH.exists():
        logger.error("Kh√¥ng t√¨m th·∫•y file MRCONSO.RRF ho·∫∑c MRSTY.RRF. Vui l√≤ng ki·ªÉm tra l·∫°i ƒë∆∞·ªùng d·∫´n.")
        return

    logger.info(f"B·∫Øt ƒë·∫ßu x√¢y d·ª±ng c∆° s·ªü d·ªØ li·ªáu UMLS. Vi·ªác n√†y s·∫Ω m·∫•t R·∫§T NHI·ªÄU th·ªùi gian...")
    
    conn = sqlite3.connect(OUTPUT_DB_PATH)
    cursor = conn.cursor()

    logger.info("ƒêang t·∫°o b·∫£ng...")
    cursor.execute('''
        CREATE TABLE atoms (
            cui TEXT NOT NULL,
            str TEXT NOT NULL,
            str_lower TEXT NOT NULL,
            is_pref INTEGER NOT NULL,
            sab TEXT NOT NULL,
            tty TEXT
        )
    ''')
    cursor.execute('''
        CREATE TABLE semantic_types (
            cui TEXT NOT NULL,
            tui TEXT NOT NULL,
            sty TEXT NOT NULL
        )
    ''')
    conn.commit()

    logger.info("ƒêang x·ª≠ l√Ω MRCONSO.RRF... (B∆∞·ªõc l√¢u nh·∫•t)")
    try:
        with open(MRCONSO_PATH, 'r', encoding='utf-8') as f:
            batch = []
            batch_size = 100000
            for line in tqdm(f, desc="MRCONSO", unit=" lines"):
                fields = line.strip().split('|')
                
                # --- S·ª¨A ƒê·ªîI QUAN TR·ªåNG ---
                # B·ªè ƒëi·ªÅu ki·ªán l·ªçc theo TARGET_SABS, ch·ªâ gi·ªØ l·∫°i c√°c thu·∫≠t ng·ªØ ti·∫øng Anh
                if len(fields) > 13 and fields[1] == 'ENG':
                    cui, is_pref, sab, tty, str_val = fields[0], fields[4], fields[10], fields[11], fields[13]
                    is_pref_int = 1 if is_pref == 'Y' else 0
                    batch.append((cui, str_val, str_val.lower(), is_pref_int, sab, tty))
                
                if len(batch) >= batch_size:
                    cursor.executemany("INSERT INTO atoms VALUES (?, ?, ?, ?, ?, ?)", batch)
                    conn.commit()
                    batch = []
            
            if batch:
                cursor.executemany("INSERT INTO atoms VALUES (?, ?, ?, ?, ?, ?)", batch)
                conn.commit()

    except Exception as e:
        logger.error(f"L·ªói khi x·ª≠ l√Ω MRCONSO.RRF: {e}")
        conn.close()
        return

    logger.info("ƒêang x·ª≠ l√Ω MRSTY.RRF...")
    try:
        with open(MRSTY_PATH, 'r', encoding='utf-8') as f:
            batch = []
            batch_size = 100000
            for line in tqdm(f, desc="MRSTY", unit=" lines"):
                fields = line.strip().split('|')
                if len(fields) > 3:
                    cui, tui, sty = fields[0], fields[1], fields[3]
                    batch.append((cui, tui, sty))
                
                if len(batch) >= batch_size:
                    cursor.executemany("INSERT INTO semantic_types VALUES (?, ?, ?)", batch)
                    conn.commit()
                    batch = []
            
            if batch:
                cursor.executemany("INSERT INTO semantic_types VALUES (?, ?, ?)", batch)
                conn.commit()
    except Exception as e:
        logger.error(f"L·ªói khi x·ª≠ l√Ω MRSTY.RRF: {e}")
        conn.close()
        return

    logger.info("ƒêang t·∫°o index cho DB (b∆∞·ªõc n√†y c≈©ng c√≥ th·ªÉ m·∫•t m·ªôt l√∫c)...")
    cursor.execute("CREATE INDEX idx_atoms_str_lower ON atoms (str_lower);")
    cursor.execute("CREATE INDEX idx_sem_types_cui ON semantic_types (cui);")
    cursor.execute("CREATE INDEX idx_sem_types_sty ON semantic_types (sty);")
    conn.commit()

    conn.close()
    logger.info(f"‚úÖ‚úÖ‚úÖ X√¢y d·ª±ng DB UMLS th√†nh c√¥ng! ƒê√£ l∆∞u t·∫°i: {OUTPUT_DB_PATH}")

if __name__ == "__main__":
    build_db()

===== .\docker-compose.yml =====
version: "3.9"

services:
  neo4j:
    image: neo4j:5.26.18
    container_name: neo4j_primekg
    restart: always
    ports:
      - "7474:7474"
      - "7687:7687"
    environment:
      - NEO4J_AUTH=neo4j/12345678
      - NEO4J_PLUGINS=["apoc", "graph-data-science"]
      - NEO4J_apoc_import_file_enabled=true
      - NEO4J_apoc_export_file_enabled=true
      - NEO4J_dbms_security_procedures_unrestricted=apoc.*,gds.*
      - NEO4J_dbms_security_allow__csv__import__from__file__urls=true
      - NEO4J_server_directories_import=/import
      - NEO4J_server_memory_heap_initial__size=3G
      - NEO4J_server_memory_heap_max__size=3G
      - NEO4J_server_memory_pagecache_size=4G

    volumes:
      - neo4j_data:/data
      - ./logs:/logs
      - ./data/primekg/import:/import

    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider localhost:7474 || exit 1"]
      interval: 10s
      timeout: 10s
      retries: 10

volumes:
  neo4j_data:
    name: medcot_primekg_data
    external: true

===== .\main.py =====
# T·ªáp: main.py
import logging, sys, time, argparse
from concurrent.futures import ThreadPoolExecutor

for lib in ["PyRuSH", "presidio-analyzer", "medspacy", "urllib3", "sentence_transformers", "httpx", "httpcore", "hpack", "google.ai"]: logging.getLogger(lib).setLevel(logging.CRITICAL)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', handlers=[logging.StreamHandler(sys.stdout)], force=True)

from src.core.state import MedCOTState
from src.modules import step0_preprocess, step1_extraction, step2_linking, step4_retrieval, step5_reasoning, step6_path_generation, step7_verification, step8_synthesis, step9_safety, step10_logging
from src.utils.neo4j_connect import db_connector

logger = logging.getLogger("MED-COT_MAIN")

def run_pipeline(query: str, patient_context: str = None, config: dict = None):
    # ... (To√†n b·ªô n·ªôi dung c·ªßa file main.py gi·ªØ nguy√™n nh∆∞ phi√™n b·∫£n tr∆∞·ªõc) ...
    if not db_connector:
        logger.critical("‚ùå K·∫øt n·ªëi Neo4j th·∫•t b·∫°i. D·ª´ng pipeline.")
        return None
    cfg = config or {}
    use_gcot, use_safety = cfg.get("use_gcot", True), cfg.get("use_safety", True)
    logger.info(f"{'='*50}\nüöÄ RUNNING PIPELINE | GCoT: {use_gcot} | Safety: {use_safety}\nüöÄ QUERY: '{query}'\n{'='*50}")
    state = MedCOTState(raw_query=query, patient_context=patient_context)
    start_time = time.time()
    try:
        logger.info("\n--- üèÅ PHASE 1: DATA PREPARATION ---")
        state = step0_preprocess.run(state)
        state = step1_extraction.run(state)
        state = step2_linking.run(state)
        logger.info("\n--- ‚ö° PHASE 2: PARALLEL REASONING & SAFETY ---")
        safety_future = None
        with ThreadPoolExecutor(max_workers=2) as executor:
            if use_safety:
                logger.info("‚è© Triggering Safety Engine in Background...")
                safety_future = executor.submit(step9_safety.run, state.model_copy(deep=True))
            try:
                logger.info("--- Running Step 4: Subgraph Retrieval ---")
                state = step4_retrieval.run(state)
                if use_gcot:
                    logger.info("--- Running Step 5: GCoT Reasoning ---")
                    state = step5_reasoning.run(state)
                logger.info("--- Running Step 6: Path Generation ---")
                state = step6_path_generation.run(state)
                logger.info("--- Running Step 7: Verification ---")
                state = step7_verification.run(state)
                logger.info("--- Running Step 8: Answer Synthesis ---")
                state = step8_synthesis.run(state)
            except Exception as e:
                logger.exception(f"‚ùå Critical Error in Main Reasoning Thread: {e}")
                state.log("MAIN_FLOW", "FAILED", str(e))
            if safety_future:
                try:
                    logger.info("‚è≥ Waiting for Safety Check to finish...")
                    safety_result = safety_future.result()
                    if safety_result.safety_flags:
                        state.safety_flags.extend(safety_result.safety_flags)
                        logger.warning(f"‚ö†Ô∏è Safety Flags Detected: {len(state.safety_flags)}")
                        safety_msg = "\n".join([f"‚ö†Ô∏è {f['msg']}" for f in state.safety_flags])
                        original_ans = state.final_answer or "No answer generated."
                        state.final_answer = f"**üö® SAFETY ALERTS:**\n{safety_msg}\n\n---\n{original_ans}"
                    else:
                        logger.info("‚úÖ Safety Check Passed (No Flags).")
                except Exception as e:
                    logger.error(f"‚ùå Error in Safety Thread: {e}")
        logger.info("\n--- üìù PHASE 3: LOGGING ---")
        step10_logging.run(state)
    except Exception as e:
        logger.exception(f"Critical pipeline error: {e}")
    finally:
        total_time = time.time() - start_time
        logger.info(f"\n{'='*50}\nüèÅ PIPELINE FINISHED IN {total_time:.2f} SECONDS\n{'='*50}")
    return state

def inspect_and_display(state: MedCOTState):
    print(f"\n\033[1m\033[94m--- FINAL RESULT ---\033[0m\n‚ùì Query: {state.raw_query}\n\nüí° ANSWER:\n{state.final_answer or 'No answer generated.'}\n")
    if state.safety_flags:
        print(f"\033[91müö® Safety Flags: {len(state.safety_flags)}\033[0m")
        for f in state.safety_flags: print(f"  - {f['msg']}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Run MED-COT Pipeline")
    parser.add_argument("--query", type=str, required=True, help="Medical question.")
    parser.add_argument("--context", type=str, default=None, help="Patient context.")
    parser.add_argument("--no-gcot", action="store_true", help="Disable GCoT.")
    parser.add_argument("--no-safety", action="store_true", help="Disable Safety.")
    args = parser.parse_args()
    cfg = {"use_gcot": not args.no_gcot, "use_safety": not args.no_safety}
    final_state = run_pipeline(args.query, args.context, cfg)
    if final_state: inspect_and_display(final_state)
    if db_connector: db_connector.close()

===== .\run_all.sh =====
#!/bin/bash

# ==============================================================================
# MED-COT AUTOMATION PIPELINE (WINDOWS COMPATIBLE FIX)
# ==============================================================================

# D·ª´ng script n·∫øu c√≥ l·ªánh b·ªã l·ªói
set -e

# --- C·∫§U H√åNH PYTHON CHO WINDOWS ---
# Ki·ªÉm tra xem l·ªánh n√†o kh·∫£ d·ª•ng: python hay python.exe
if command -v python.exe &> /dev/null; then
    PYTHON_CMD="python.exe"
elif command -v python &> /dev/null; then
    PYTHON_CMD="python"
else
    echo "‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y Python! H√£y ƒë·∫£m b·∫£o b·∫°n ƒë√£ activate conda environment."
    echo "üëâ Th·ª≠ ch·∫°y: 'conda activate pythera-graph' tr∆∞·ªõc khi ch·∫°y script."
    exit 1
fi

echo "üöÄ KH·ªûI ƒê·ªòNG MED-COT PIPELINE TO√ÄN DI·ªÜN..."
echo "üêç Using Python: $PYTHON_CMD"

# --- 0. KI·ªÇM TRA M√îI TR∆Ø·ªúNG ---
if [ ! -f ".env" ]; then
    echo "‚ùå L·ªói: File .env kh√¥ng t·ªìn t·∫°i."
    exit 1
fi

# --- 1. INFRASTRUCTURE & DATA INGESTION ---
echo "========================================================"
echo "üì¶ STEP 1: INFRASTRUCTURE & KNOWLEDGE BASE SETUP"
echo "========================================================"

# 1.1 Kh·ªüi ƒë·ªông Neo4j
echo "üîπ ƒêang ki·ªÉm tra Neo4j..."
if ! docker ps | grep -q "neo4j_primekg"; then
    echo "   -> Neo4j ch∆∞a ch·∫°y. ƒêang kh·ªüi ƒë·ªông..."
    bash run/run_primekg.sh &
    echo "   -> ƒêang ƒë·ª£i Neo4j s·∫µn s√†ng (30s)..."
    sleep 30
else
    echo "   -> Neo4j ƒëang ch·∫°y."
fi

# 1.2 N·∫°p d·ªØ li·ªáu t√πy ch·ªânh
echo "üîπ Ch·∫°y n·∫°p d·ªØ li·ªáu t√πy ch·ªânh (Custom Ingest)..."
$PYTHON_CMD run/5_ingest_custom_data.py

# 1.3 T·∫°o FAISS Index
echo "üîπ X√¢y d·ª±ng FAISS Index..."
$PYTHON_CMD run/build_faiss_index.py


# --- 2. GENERATE DATASET ---
echo "========================================================"
echo "üß† STEP 2: GENERATE RICH MedCOT DATASET"
echo "========================================================"
echo "‚ö†Ô∏è  L∆∞u √Ω: B∆∞·ªõc n√†y t·ªën nhi·ªÅu th·ªùi gian..."

$PYTHON_CMD run/0_generate_rich_dataset.py

if [ ! -f "data/medcot_rich_training_data.jsonl" ]; then
    echo "‚ùå L·ªói: Kh√¥ng sinh ƒë∆∞·ª£c file data/medcot_rich_training_data.jsonl"
    exit 1
fi


# --- 3. TRAIN AUXILIARY MODELS ---
echo "========================================================"
echo "üîß STEP 3: TRAIN INTERNAL MODELS (GNN & VERIFIER)"
echo "========================================================"

echo "üîπ Chu·∫©n b·ªã d·ªØ li·ªáu cho GNN..."
$PYTHON_CMD run/prepare_gnn_dataset.py

echo "üîπ Train GNN..."
$PYTHON_CMD run/train_gnn_next_hop.py

echo "üîπ Train Verifier..."
$PYTHON_CMD run/train_verifier.py


# --- 4. TRAIN LLMs ---
echo "========================================================"
echo "üéì STEP 4: TRAIN GENERATIVE MODELS (LLM SFT)"
echo "========================================================"

echo "üî• Training Model 1: TRM..."
$PYTHON_CMD run/1_train_trm_generative_enhanced.py

echo "üî• Training Model 2: LoRA MedCOT..."
$PYTHON_CMD run/2_train_lora_medcot.py

echo "üî• Training Model 3: LoRA Default..."
$PYTHON_CMD run/3_train_lora_default.py


# --- 5. EVALUATION ---
echo "========================================================"
echo "‚öñÔ∏è STEP 5: EVALUATION"
echo "========================================================"
$PYTHON_CMD run/4_evaluate_models.py

echo "========================================================"
echo "üéâüéâüéâ TO√ÄN B·ªò PIPELINE ƒê√É HO√ÄN T·∫§T! üéâüéâüéâ"
echo "========================================================"

===== .\setup.py =====
from setuptools import setup, find_packages

setup(
    name="minimed",
    version="1.0",
    packages=find_packages(),
)

===== .\.cache\arax_queries\fac1f9a9c0747d6a9db0385077561ed6.json =====
[]

===== .\run\0_generate_rich_dataset.py =====
import os
import json
import logging
import pandas as pd
import gc  # Garbage collection ƒë·ªÉ qu·∫£n l√Ω RAM
from tqdm import tqdm
from src.core.state import MedCOTState
from src.modules import (
    step0_preprocess, step1_extraction, step2_linking,
    step4_retrieval, step5_reasoning, step6_path_generation,
    step7_verification
)
from src.utils.neo4j_connect import db_connector
# Import module Local LLM m·ªõi t·∫°o
from src.utils.local_llm import local_llm

# --- CONFIG ---
logging.basicConfig(level=logging.ERROR)
INPUT_PARQUET = "data/medical_o1_vi_translated_EVALUATED_GEMINI.parquet"
OUTPUT_JSONL = "data/medcot_rich_training_data.jsonl"

def generate_raw_trace(query: str):
    """
    Ch·∫°y pipeline MedCOT (Step 0 -> Step 7) ƒë·ªÉ l·∫•y d·ªØ li·ªáu th√¥ t·ª´ Knowledge Graph.
    """
    state = MedCOTState(raw_query=query)
    try:
        # Ch·∫°y c√°c b∆∞·ªõc logic ƒë·ªì th·ªã
        state = step0_preprocess.run(state)
        state = step1_extraction.run(state)
        state = step2_linking.run(state)
        state = step4_retrieval.run(state)
        state = step5_reasoning.run(state)
        state = step6_path_generation.run(state)
        state = step7_verification.run(state)
        
        path_text = ""
        # state.gcot['verified_path_text'] ƒë∆∞·ª£c g√°n trong step7
        if state.gcot.get('verified_path_text'):
            path_text = state.gcot['verified_path_text']
        elif state.candidate_paths:
            path_text = state.candidate_paths[0].get('text_repr', "")

        raw_info = {
            "query": query,
            "seed_nodes": [e.best_candidate.preferred_name for e in state.linked_entities if e.link_status == 'linked'],
            "graph_context": state.gcot.get("graph_tokens", ""),
            "verified_path_text": path_text, # T√™n c·ªôt quan tr·ªçng
            "confidence": state.global_confidence
        }
        return raw_info
    except Exception as e:
        return None

def normalize_cot_with_llm(raw_info):
    """
    S·ª≠ d·ª•ng Local LLM ƒë·ªÉ vi·∫øt l·∫°i suy lu·∫≠n.
    """
    if not raw_info or not raw_info["verified_path_text"]:
        return "Reasoning could not be generated due to lack of graph evidence."

    prompt = f"""
    Analyze the medical Knowledge Graph path below and explain the reasoning step-by-step to answer the question.
    
    **Question:** "{raw_info['query']}"
    **Entities:** {', '.join(raw_info['seed_nodes'])}
    **Graph Path:** "{raw_info['verified_path_text']}"
    
    **Requirement:** Write a concise, logical Chain-of-Thought based on this path.
    """
    
    try:
        return local_llm.generate_cot(prompt)
    except Exception as e:
        return f"Local LLM Error: {e}"

def main():
    if db_connector is None:
        print("‚ùå Neo4j connection failed. Please check docker container.")
        return
    
    if not os.path.exists(INPUT_PARQUET):
        print(f"‚ùå Input file not found: {INPUT_PARQUET}")
        return

    print(f"üöÄ Loading data from {INPUT_PARQUET}...")
    df = pd.read_parquet(INPUT_PARQUET)
    
    # Ch·∫°y test v·ªõi 10 d√≤ng ƒë·∫ßu
    # df = df.head(10) 
    
    print("‚è≥ Warming up Local LLM (DeepSeek-R1-1.5B)...")
    try:
        local_llm.load_model()
    except Exception as e:
        print(f"‚ùå Failed to load Local LLM: {e}")
        return

    results = []
    
    print("running...")
    for idx, row in tqdm(df.iterrows(), total=len(df), desc="Generating Rich Traces"):
        raw_trace_info = generate_raw_trace(row['Question'])
        
        if raw_trace_info:
            normalized_medcot = normalize_cot_with_llm(raw_trace_info)
        else:
            normalized_medcot = "Pipeline failed to generate trace."

        # --- S·ª¨A ƒê·ªîI ·ªû ƒê√ÇY ---
        # Th√™m c·ªôt 'verified_path_text' v√†o dictionary k·∫øt qu·∫£ ƒë·ªÉ l∆∞u xu·ªëng file
        results.append({
            "question": row['Question'],
            "answer": row['Response'],
            "default_cot": row['Complex_CoT'],
            "medcot_cot": normalized_medcot,
            "verified_path_text": raw_trace_info.get("verified_path_text", "") if raw_trace_info else ""
        })
        # ---------------------
        
        if idx % 10 == 0:
            gc.collect()

    print(f"üíæ Saving {len(results)} rows to {OUTPUT_JSONL}...")
    with open(OUTPUT_JSONL, 'w', encoding='utf-8') as f:
        for item in results:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
            
    local_llm.unload()
    
    if db_connector: 
        db_connector.close()
        
    print(f"‚úÖ DONE! Rich CoT data ready at: {OUTPUT_JSONL}")

if __name__ == "__main__":
    main()

===== .\run\1_train_trm_generative_enhanced.py =====
# T·ªáp: run/1_train_trm_generative_enhanced.py
import torch
from datasets import load_dataset
from peft import LoraConfig
from transformers import AutoTokenizer, AutoModelForCausalLM
# --- S·ª¨A ƒê·ªîI IMPORT: D√πng SFTConfig thay cho TrainingArguments ---
from trl import SFTTrainer, SFTConfig

# --- CONFIG ---
BASE_MODEL = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
DATA_FILE = "data/medcot_rich_training_data.jsonl"
OUTPUT_DIR = "models/trm_generative_enhanced_adapter"

def main():
    print(f"üöÄ B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán Model 1 (N√¢ng cao): TRM-Inspired with Self-Enhancement")
    
    dataset = load_dataset("json", data_files=DATA_FILE, split="train[:10]")

    def format_enhanced_trm_prompt(example):
        """
        Prompt n√†y d·∫°y model c√°ch "t·ª± n√¢ng cao" (Self-Enhance) suy lu·∫≠n.
        """
        prompt = (
            "<bos><start_of_turn>user\n"
            "You are a medical reasoning engine. Your task is to self-enhance your reasoning process before providing an answer.\n"
            "1. **Self-Enhanced Reasoning:** Based on the initial Chain-of-Thought, reformulate it into an even clearer, more structured, and logically rigorous thought process.\n"
            "2. **Final Answer:** Based on your new, enhanced reasoning, provide the final clinical answer.\n\n"
            f"**Question:** {example['question']}\n"
            f"**Initial Chain-of-Thought:**\n{example['medcot_cot']}<end_of_turn>\n"
            "<start_of_turn>model\n"
            f"**Self-Enhanced Reasoning:**\n{example['medcot_cot']}\n\n"
            f"**Final Answer:**\n{example['answer']}<end_of_turn><eos>"
        )
        # SFTTrainer m·ªõi s·∫Ω t·ª± ƒë·ªông t√¨m c·ªôt 'text'
        return {"text": prompt}

    dataset = dataset.map(format_enhanced_trm_prompt)

    model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL, device_map="auto", torch_dtype=torch.bfloat16, trust_remote_code=True
    )
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    peft_config = LoraConfig(r=16, lora_alpha=32, lora_dropout=0.05, task_type="CAUSAL_LM")
    
    # --- S·ª¨A ƒê·ªîI QUAN TR·ªåNG ---
    # S·ª≠ d·ª•ng SFTConfig v√† ƒë∆∞a t·∫•t c·∫£ c√°c tham s·ªë v√†o ƒë√¢y
    training_args = SFTConfig(
        output_dir=f"models/checkpoints/trm_enhanced_sft",
        num_train_epochs=1,
        per_device_train_batch_size=2,
        gradient_accumulation_steps=8,
        learning_rate=2e-4,
        logging_steps=10,
        fp16=True,
        save_strategy="epoch",
        report_to="none",
        # C√°c tham s·ªë ƒë·∫∑c th√π c·ªßa SFT
        max_length=2048,
        dataset_text_field="text", # Ch·ªâ ƒë·ªãnh t√™n c·ªôt text ·ªü ƒë√¢y
    )

    # Kh·ªüi t·∫°o SFTTrainer v·ªõi c·∫•u tr√∫c m·ªõi, g·ªçn g√†ng h∆°n
    trainer = SFTTrainer(
        model=model,
        args=training_args,
        train_dataset=dataset,
        peft_config=peft_config,
        # tokenizer=tokenizer,
    )
    # ---------------------------
    
    print("üî• Training is starting...")
    trainer.train()
    
    trainer.save_model(OUTPUT_DIR)
    print(f"‚úÖ Model 1 (N√¢ng cao) ƒë√£ hu·∫•n luy·ªán xong. Adapter ƒë∆∞·ª£c l∆∞u t·∫°i: {OUTPUT_DIR}")

if __name__ == "__main__":
    main()

===== .\run\2_train_lora_medcot.py =====
# T·ªáp: run/2_train_lora_medcot.py
import torch
from datasets import load_dataset
from peft import LoraConfig, prepare_model_for_kbit_training
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
# --- S·ª¨A ƒê·ªîI IMPORT: D√πng SFTConfig thay cho TrainingArguments ---
from trl import SFTTrainer, SFTConfig

# --- CONFIG ---
BASE_MODEL = "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B" 
DATA_FILE = "data/medcot_rich_training_data.jsonl"
OUTPUT_DIR = "models/medcot_lora_adapter"

def main():
    print(f"üöÄ B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán Model 2 (Optimized for 4GB VRAM)")

    dataset = load_dataset("json", data_files=DATA_FILE, split="train[:10]")

    def format_medcot_prompt(example):
        prompt = (
            f"Question: {example['question']}\n"
            f"Reasoning: {example['medcot_cot']}\n"
            f"Answer: {example['answer']}"
        )
        return {"text": prompt}
    
    dataset = dataset.map(format_medcot_prompt)

    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_use_double_quant=True,
    )

    model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL, 
        torch_dtype=torch.bfloat16,
        trust_remote_code=True
    ).to("cuda")
    # ----------------------------------------

    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # --- THAY ƒê·ªîI PEFT CONFIG CHO GI·ªêNG C√ÅC SCRIPT KH√ÅC ---
    peft_config = LoraConfig(
        r=16, 
        lora_alpha=32, 
        lora_dropout=0.05, 
        task_type="CAUSAL_LM"
    )
    
    # --- ƒê·∫¢M B·∫¢O TRAINING ARGS D√ôNG fp16 ---
    training_args = SFTConfig(
        output_dir=f"models/checkpoints/medcot_sft",
        num_train_epochs=1,
        per_device_train_batch_size=1, # Gi·ªØ batch size nh·ªè
        gradient_accumulation_steps=16,
        learning_rate=2e-4,
        logging_steps=5,
        fp16=True, # ƒê√£ ƒë√∫ng
        save_strategy="epoch",
        # optim="paged_adamw_32bit", # X√≥a optim n√†y v√¨ kh√¥ng d√πng 4-bit n·ªØa
        report_to="none",
        max_length=1024,
        dataset_text_field="text",
    )

    trainer = SFTTrainer(
        model=model,
        args=training_args,
        train_dataset=dataset,
        peft_config=peft_config,
        # tokenizer=tokenizer,
    )
    # ---------------------------

    print("üî• Training is starting...")
    trainer.train()
    
    trainer.save_model(OUTPUT_DIR)
    print(f"‚úÖ Training Done! Saved to: {OUTPUT_DIR}")

if __name__ == "__main__":
    main()

===== .\run\3_train_lora_default.py =====
# T·ªáp: run/3_train_lora_default.py
import torch
from datasets import load_dataset
from peft import LoraConfig
from transformers import AutoTokenizer, AutoModelForCausalLM
# --- S·ª¨A ƒê·ªîI IMPORT: D√πng SFTConfig thay cho TrainingArguments ---
from trl import SFTTrainer, SFTConfig

# --- CONFIG ---
BASE_MODEL = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
DATA_FILE = "data/medcot_rich_training_data.jsonl"
OUTPUT_DIR = "models/default_lora_adapter"

def main():
    print(f"üöÄ B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán Model 3: LoRA fine-tuned on Default reasoning (Baseline)")

    dataset = load_dataset("json", data_files=DATA_FILE, split="train[:10]")

    def format_default_prompt(example):
        """
        H√†m t·∫°o prompt cho SFT ti√™u chu·∫©n, s·ª≠ d·ª•ng CoT m·∫∑c ƒë·ªãnh.
        """
        prompt = (
            "<bos><start_of_turn>user\n"
            "Based on the provided reasoning trace, answer the medical question.\n\n"
            f"**Question:** {example['question']}\n"
            f"**Reasoning Trace:**\n{example['default_cot']}<end_of_turn>\n"
            "<start_of_turn>model\n"
            f"**Answer:**\n{example['answer']}<end_of_turn><eos>"
        )
        return {"text": prompt}
    
    dataset = dataset.map(format_default_prompt)

    model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL, 
        device_map="auto", 
        torch_dtype=torch.bfloat16,
        trust_remote_code=True
    )
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    peft_config = LoraConfig(
        r=16, 
        lora_alpha=32, 
        lora_dropout=0.05, 
        task_type="CAUSAL_LM"
    )
    
    # --- S·ª¨A ƒê·ªîI QUAN TR·ªåNG ---
    # S·ª≠ d·ª•ng SFTConfig
    training_args = SFTConfig(
        output_dir=f"models/checkpoints/default_sft",
        num_train_epochs=1,
        per_device_train_batch_size=2,
        gradient_accumulation_steps=8,
        learning_rate=2e-4,
        logging_steps=10,
        bf16=True,
        save_strategy="epoch",
        report_to="none",
        # C√°c tham s·ªë ƒë·∫∑c th√π c·ªßa SFT
        max_length=2048,
        dataset_text_field="text",
    )

    trainer = SFTTrainer(
        model=model,
        args=training_args,
        train_dataset=dataset,
        peft_config=peft_config,
        # tokenizer=tokenizer,
    )
    # ---------------------------

    print("üî• Training is starting...")
    trainer.train()
    
    trainer.save_model(OUTPUT_DIR)
    print(f"‚úÖ Model 3 (Default) ƒë√£ hu·∫•n luy·ªán xong. Adapter ƒë∆∞·ª£c l∆∞u t·∫°i: {OUTPUT_DIR}")

if __name__ == "__main__":
    main()

===== .\run\4_evaluate_models.py =====
# run/4_evaluate_models.py
import os
import json
import torch
from tqdm import tqdm
from peft import PeftModel
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from src.utils.local_llm import local_llm # D√πng l·∫°i utility wrapper

# --- CONFIG ---
BASE_MODEL = "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B" # ƒê·ªìng b·ªô model
TEST_DATA_FILE = "data/medcot_rich_training_data.jsonl" 
ADAPTER_PATHS = {
    # "TRM_Enhanced": "models/trm_generative_enhanced_adapter", # Uncomment n·∫øu ƒë√£ train
    "LoRA_MedCOT": "models/medcot_lora_adapter",
    # "LoRA_Default": "models/default_lora_adapter"
}

def load_adapter_model(adapter_path):
    print(f"\nüß† Loading adapter: {adapter_path}")
    # Load base model 4-bit
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16,
    )
    model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL, quantization_config=bnb_config, device_map="auto"
    )
    # Load adapter
    model = PeftModel.from_pretrained(model, adapter_path)
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)
    return model, tokenizer

def generate(model, tokenizer, prompt):
    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
    with torch.no_grad():
        outputs = model.generate(**inputs, max_new_tokens=256)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

def main():
    if not os.path.exists(TEST_DATA_FILE):
        print("Data file not found.")
        return

    # L·∫•y 5 m·∫´u test
    test_data = []
    with open(TEST_DATA_FILE, 'r', encoding='utf-8') as f:
        for i, line in enumerate(f):
            if i >= 5: break
            test_data.append(json.loads(line))

    results = {}

    for name, path in ADAPTER_PATHS.items():
        if not os.path.exists(path):
            print(f"‚ö†Ô∏è Skipping {name}: path not found.")
            continue
            
        # 1. Load Model Adapter c·∫ßn test
        # L∆∞u √Ω: V√¨ VRAM √≠t, c·∫ßn unload model c≈© tr∆∞·ªõc khi load model m·ªõi
        # ·ªû ƒë√¢y script ch·∫°y tu·∫ßn t·ª± ƒë∆°n gi·∫£n, ta gi·∫£ ƒë·ªãnh load ƒë√® ho·∫∑c restart script
        model, tokenizer = load_adapter_model(path)
        
        model_outputs = []
        for item in tqdm(test_data, desc=f"Testing {name}"):
            prompt = f"Question: {item['question']}\nReasoning: {item['medcot_cot']}\nAnswer:"
            output = generate(model, tokenizer, prompt)
            
            # Ch·ªâ l∆∞u text k·∫øt qu·∫£, b·ªè qua ph·∫ßn ch·∫•m ƒëi·ªÉm t·ª± ƒë·ªông (Judge)
            # V√¨ load th√™m 1 model n·ªØa ƒë·ªÉ ch·∫•m ƒëi·ªÉm s·∫Ω s·∫≠p VRAM
            model_outputs.append({
                "question": item['question'],
                "target": item['answer'],
                "generated": output.split("Answer:")[-1].strip()
            })
            
        results[name] = model_outputs
        
        # Gi·∫£i ph√≥ng VRAM th·ªß c√¥ng
        del model
        del tokenizer
        torch.cuda.empty_cache()

    # L∆∞u k·∫øt qu·∫£
    with open("evaluation_results.json", "w", encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    print("\n‚úÖ Evaluation Done. Check evaluation_results.json")

if __name__ == "__main__":
    main()

===== .\run\5_ingest_custom_data.py =====
# run/5_ingest_custom_data.py
import os
import json
import logging
import glob
import re
from pathlib import Path
from src.utils.neo4j_connect import db_connector
from src.utils.local_llm import local_llm

# --- C·∫§U H√åNH ---
DATA_DIR = Path("data/custom_knowledge")
DATA_DIR.mkdir(parents=True, exist_ok=True)

# Logging ra m√†n h√¨nh
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)
formatter = logging.Formatter('%(asctime)s - %(message)s', datefmt='%H:%M:%S')
console_handler.setFormatter(formatter)

logger = logging.getLogger("KG_BUILDER")
logger.setLevel(logging.INFO)
if not logger.handlers:
    logger.addHandler(console_handler)

class KnowledgeExtractor:
    def __init__(self):
        try:
            local_llm.load_model()
        except Exception as e:
            logger.error(f"‚ùå Kh√¥ng th·ªÉ load Local LLM: {e}")
            raise e

    def clean_json_response(self, text):
        """L√†m s·∫°ch chu·ªói JSON t·ª´ output c·ªßa LLM (Robust Version)"""
        # 1. Lo·∫°i b·ªè th·∫ª <think> v√† markdown
        text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)
        text = re.sub(r'```json', '', text)
        text = re.sub(r'```', '', text)
        
        # 2. T√¨m kh·ªëi JSON th√¥ t·ª´ d·∫•u { ƒë·∫ßu ti√™n ƒë·∫øn d·∫•u } cu·ªëi c√πng
        start_idx = text.find('{')
        end_idx = text.rfind('}')
        
        if start_idx == -1 or end_idx == -1 or end_idx <= start_idx:
            return "{}"
        
        json_str = text[start_idx : end_idx + 1]
        
        # 3. Fix l·ªói ph·ªï bi·∫øn: D·∫•u ph·∫©y th·ª´a ·ªü cu·ªëi list/dict (VD: {"a": 1,})
        # Regex n√†y t√¨m d·∫•u ph·∫©y ƒë·ª©ng tr∆∞·ªõc d·∫•u ƒë√≥ng ngo·∫∑c v√† x√≥a n√≥
        json_str = re.sub(r',\s*([\]}])', r'\1', json_str)
        
        return json_str.strip()

    def extract_graph_from_text(self, text_chunk):
        # One-shot Prompt: Cung c·∫•p v√≠ d·ª• c·ª• th·ªÉ ƒë·ªÉ ƒë·ªãnh h∆∞·ªõng model
        prompt = f"""
        You are a medical data extractor. Convert the text into a JSON Knowledge Graph.
        
        ### EXAMPLE:
        Text: "Metformin treats Type 2 Diabetes but may cause Nausea."
        JSON Output:
        {{
            "nodes": [
                {{"id": "Metformin", "label": "Drug"}},
                {{"id": "Type 2 Diabetes", "label": "Disease"}},
                {{"id": "Nausea", "label": "Symptom"}}
            ],
            "edges": [
                {{"source": "Metformin", "target": "Type 2 Diabetes", "type": "TREATS"}},
                {{"source": "Metformin", "target": "Nausea", "type": "CAUSES"}}
            ]
        }}
        
        ### TASK:
        Text: "{text_chunk}"
        
        Required: Output VALID JSON only. No explanations.
        """
        
        try:
            # --- T·ªêI ∆ØU G·ªåI H√ÄM ---
            messages = [
                {"role": "system", "content": "You are a JSON extractor. Output valid JSON only."},
                {"role": "user", "content": prompt}
            ]
            
            input_ids = local_llm.tokenizer.apply_chat_template(
                messages, add_generation_prompt=True, return_tensors="pt"
            ).to(local_llm.model.device)
            
            attention_mask = import_torch().ones_like(input_ids)
            
            print("   ‚Ü≥ ü§ñ AI ƒëang suy nghƒ©...", end="\r")
            
            with import_torch().no_grad():
                outputs = local_llm.model.generate(
                    input_ids,
                    attention_mask=attention_mask,
                    pad_token_id=local_llm.tokenizer.pad_token_id,
                    max_new_tokens=1024, # ƒê·ªß d√†i cho JSON
                    temperature=0.1,     # Th·∫•p ƒë·ªÉ ·ªïn ƒë·ªãnh
                    do_sample=False      # Greedy search
                )
            
            raw_response = local_llm.tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)
            print("   ‚Ü≥ ‚úÖ AI ƒë√£ tr·∫£ l·ªùi!       ") 

            clean_json = self.clean_json_response(raw_response)
            
            # --- PARSE JSON ---
            try:
                graph_data = json.loads(clean_json)
            except json.JSONDecodeError as e:
                print(f"   ‚Ü≥ ‚ö†Ô∏è JSON Parse Error: {str(e)[:50]}")
                # print(f"DEBUG: {clean_json}") # Uncomment ƒë·ªÉ debug
                return None
            
            # Chu·∫©n h√≥a keys
            if "nodes" not in graph_data: graph_data["nodes"] = []
            if "edges" not in graph_data: graph_data["edges"] = []
            
            return graph_data
            
        except Exception as e:
            print(f"   ‚Ü≥ ‚ùå L·ªói h·ªá th·ªëng: {str(e)[:50]}...")
            return None

    def ingest_to_neo4j(self, graph_data):
        if not graph_data or not db_connector: return

        # ƒê·∫£m b·∫£o source/target trong edges ƒë·ªÅu t·ªìn t·∫°i trong nodes ƒë·ªÉ tr√°nh l·ªói orphan edges
        # Trong th·ª±c t·∫ø, c√≥ th·ªÉ c·∫ßn merge nodes tr∆∞·ªõc
        
        node_query = """
        UNWIND $nodes AS n MERGE (node {name: n.id}) 
        ON CREATE SET node.id = n.id, node.source='User_Upload' 
        WITH node, n CALL apoc.create.addLabels(node, [n.label]) YIELD node as l RETURN count(l)
        """
        edge_query = """
        UNWIND $edges AS e MATCH (s {name: e.source}), (t {name: e.target}) 
        MERGE (s)-[r:RELATED {type: e.type, provenance:'User_Upload'}]->(t) RETURN count(r)
        """

        try:
            if graph_data.get("nodes"):
                db_connector.run_query(node_query, {"nodes": graph_data["nodes"]})
            if graph_data.get("edges"):
                db_connector.run_query(edge_query, {"edges": graph_data["edges"]})
            logger.info(f"   + DB: Saved {len(graph_data.get('nodes', []))} nodes, {len(graph_data.get('edges', []))} edges.")
        except Exception as e:
            logger.error(f"‚ùå DB Error: {e}")

def import_torch():
    import torch
    return torch

def main():
    print("üöÄ B·∫Øt ƒë·∫ßu n·∫°p d·ªØ li·ªáu (Robust Mode)")
    if db_connector is None: 
        print("‚ùå Kh√¥ng c√≥ k·∫øt n·ªëi DB.")
        return

    files = glob.glob(str(DATA_DIR / "*.txt"))
    if not files:
        print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file .txt n√†o trong data/custom_knowledge")
        print("   -> T·∫°o file sample...")
        sample_file = DATA_DIR / "sample_vn.txt"
        with open(sample_file, "w", encoding="utf-8") as f:
            f.write("C√¢y ch√≥ ƒë·∫ª (Di·ªáp h·∫° ch√¢u) h·ªó tr·ª£ tr·ªã vi√™m gan B nh∆∞ng g√¢y h·∫° huy·∫øt √°p.")
        files = [str(sample_file)]

    extractor = KnowledgeExtractor()
    
    for file_path in files:
        logger.info(f"üìÇ File: {os.path.basename(file_path)}")
        with open(file_path, "r", encoding="utf-8") as f: text = f.read()
        
        # Chia nh·ªè text
        chunk_size = 800 
        chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]
        
        for idx, chunk in enumerate(chunks):
            logger.info(f"   Processing chunk {idx+1}/{len(chunks)}...")
            graph_data = extractor.extract_graph_from_text(chunk)
            if graph_data: extractor.ingest_to_neo4j(graph_data)

    local_llm.unload()
    if db_connector: db_connector.close()
    print("\nüéâ Ho√†n t·∫•t!")

if __name__ == "__main__":
    main()

===== .\run\build_faiss_index.py =====
# run/build_faiss_index.py
import json
import logging
import numpy as np
import faiss
import os
from pathlib import Path
from tqdm import tqdm
from sentence_transformers import SentenceTransformer
from src.utils.neo4j_connect import db_connector

# --- CONFIG ---
MODEL_NAME = "BAAI/bge-small-en-v1.5" # Model nh·ªè, nhanh, hi·ªáu qu·∫£
OUTPUT_DIR = Path("data/kg_index")
INDEX_PATH = OUTPUT_DIR / "kg_faiss.index"
META_PATH = OUTPUT_DIR / "kg_nodes_meta.json"
BATCH_SIZE = 5000  # X·ª≠ l√Ω 5000 node m·ªói l·∫ßn ƒë·ªÉ ti·∫øt ki·ªám RAM

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger("FAISS_BUILDER")

def main():
    # 1. Ki·ªÉm tra n·∫øu Index ƒë√£ t·ªìn t·∫°i th√¨ Skip
    if INDEX_PATH.exists() and META_PATH.exists():
        print(f"\n‚è© [SKIP] FAISS Index ƒë√£ t·ªìn t·∫°i t·∫°i: {OUTPUT_DIR}")
        print("üëâ N·∫øu b·∫°n v·ª´a n·∫°p d·ªØ li·ªáu m·ªõi v√† mu·ªën build l·∫°i, h√£y x√≥a th∆∞ m·ª•c 'data/kg_index' r·ªìi ch·∫°y l·∫°i script n√†y.")
        return

    # 2. Ki·ªÉm tra k·∫øt n·ªëi DB
    if db_connector is None:
        logger.error("‚ùå Kh√¥ng c√≥ k·∫øt n·ªëi Neo4j. Vui l√≤ng ki·ªÉm tra Docker.")
        return

    # T·∫°o th∆∞ m·ª•c output
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

    # 3. ƒê·∫øm t·ªïng s·ªë node ƒë·ªÉ hi·ªÉn th·ªã thanh ti·∫øn tr√¨nh
    logger.info("üìä ƒêang ƒë·∫øm t·ªïng s·ªë node c·∫ßn index...")
    count_query = "MATCH (n) WHERE n.name IS NOT NULL RETURN count(n) as total"
    try:
        res = db_connector.run_query(count_query)
        total_nodes = res[0]['total']
        logger.info(f"   -> T·ªïng s·ªë node: {total_nodes}")
    except Exception as e:
        logger.error(f"‚ùå L·ªói khi ƒë·∫øm node: {e}")
        return

    # 4. Kh·ªüi t·∫°o Model & Index
    logger.info(f"üß† Loading SentenceTransformer: {MODEL_NAME}")
    encoder = SentenceTransformer(MODEL_NAME)
    
    # S·ª≠ d·ª•ng IndexFlatIP (Inner Product) cho cosine similarity (khi vectors ƒë√£ normalize)
    # Lo·∫°i n√†y ti·∫øt ki·ªám RAM h∆°n HNSW v√† v·∫´n ƒë·ªß nhanh cho v√†i tri·ªáu node.
    embedding_dim = 384
    index = faiss.IndexFlatIP(embedding_dim) 

    all_meta = []
    
    # 5. V√≤ng l·∫∑p Batch Processing (Ti·∫øt ki·ªám RAM)
    logger.info("üöÄ B·∫Øt ƒë·∫ßu qu√° tr√¨nh Indexing theo batch...")
    
    query = """
    MATCH (n)
    WHERE n.name IS NOT NULL
    RETURN elementId(n) AS node_id, labels(n) AS labels, n.name AS name
    ORDER BY elementId(n)
    SKIP $skip LIMIT $limit
    """
    
    skip = 0
    pbar = tqdm(total=total_nodes, desc="Indexing Nodes", unit="node")

    while skip < total_nodes:
        # A. Fetch Batch t·ª´ Neo4j
        rows = db_connector.run_query(query, {"skip": skip, "limit": BATCH_SIZE})
        if not rows:
            break
            
        batch_meta = []
        batch_texts = []
        
        # B. Prepare Data
        for r in rows:
            # X·ª≠ l√Ω an to√†n d·ªØ li·ªáu
            lbls = r.get("labels", [])
            lbl = lbls[0] if lbls else "Unknown"
            name = r.get("name", "Unknown")
            nid = str(r.get("node_id"))
            
            # L∆∞u metadata g·ªçn nh·∫π
            meta_item = {
                "node_id": nid,
                "labels": lbls,
                "name": name
            }
            batch_meta.append(meta_item)
            
            # Text ƒë·ªÉ embed: "Name (Label)"
            batch_texts.append(f"{name} ({lbl})")
        
        # C. Encode Batch (GPU/CPU)
        if batch_texts:
            embeddings = encoder.encode(
                batch_texts,
                batch_size=256,
                show_progress_bar=False,
                normalize_embeddings=True # Quan tr·ªçng cho FlatIP/Cosine
            )
            
            # D. Add to FAISS Index
            index.add(np.asarray(embeddings, dtype="float32"))
            
            # E. Append Meta
            all_meta.extend(batch_meta)
        
        skip += BATCH_SIZE
        pbar.update(len(rows))

    pbar.close()

    # 6. L∆∞u xu·ªëng ƒëƒ©a
    logger.info(f"üíæ ƒêang l∆∞u FAISS index v√†o {INDEX_PATH}...")
    faiss.write_index(index, str(INDEX_PATH))

    logger.info(f"üíæ ƒêang l∆∞u Metadata v√†o {META_PATH}...")
    with open(META_PATH, "w", encoding="utf-8") as f:
        json.dump(all_meta, f, ensure_ascii=False, indent=None) # indent=None cho file nh·ªè g·ªçn

    logger.info("üéâ Ho√†n t·∫•t build FAISS index!")
    if db_connector:
        db_connector.close()

if __name__ == "__main__":
    main()

===== .\run\generate_training_data.py =====
# run/generate_training_data.py
import pandas as pd
import json
import logging
from tqdm import tqdm
from src.core.state import MedCOTState
from src.modules import (
    step0_preprocess, step1_extraction, step2_linking,
    step4_retrieval, step5_reasoning, step6_path_generation,
    step7_verification, step8_synthesis # Kh√¥ng c·∫ßn step9, step10 cho training data
)
from src.utils.neo4j_connect import db_connector

# Config logging g·ªçn nh·∫π
logging.getLogger("medspacy").setLevel(logging.CRITICAL)
logging.basicConfig(level=logging.INFO)

INPUT_FILE = "data/medical_o1_vi_translated_EVALUATED_GEMINI.parquet"
OUTPUT_FILE = "data/medcot_training_data.jsonl"

def run_partial_pipeline(query):
    """Ch·∫°y pipeline MedCOT ƒë·ªÉ l·∫•y Reasoning Trace (z)"""
    state = MedCOTState(raw_query=query)
    try:
        # Ch·∫°y c√°c b∆∞·ªõc ƒë·ªÉ l·∫•y context v√† reasoning
        state = step0_preprocess.run(state)
        state = step1_extraction.run(state)
        state = step2_linking.run(state)
        state = step4_retrieval.run(state)
        state = step5_reasoning.run(state) # L·∫•y thought vectors
        state = step6_path_generation.run(state)
        state = step7_verification.run(state) # L·∫•y verified path
        
        # T·ªïng h·ª£p Reasoning Trace t·ª´ pipeline (ƒê√¢y ch√≠nh l√† 'z')
        reasoning_trace = ""
        
        # 1. Th√™m KG Path
        if state.verified_path:
            path_str = " -> ".join([f"{s['source']}--[{s['edge']}]-->{s['target']}" for s in state.verified_path])
            reasoning_trace += f"KG Evidence: {path_str}. "
        
        # 2. Th√™m GCoT thoughts (Semantic reasoning)
        if state.gcot.get("graph_tokens"):
             reasoning_trace += f" Semantic Context: {state.gcot['graph_tokens']}"
             
        return reasoning_trace
    except Exception as e:
        print(f"Error processing query: {e}")
        return ""

def main():
    if db_connector is None:
        print("‚ùå No Database Connection")
        return

    print(f"‚è≥ Reading {INPUT_FILE}...")
    df = pd.read_parquet(INPUT_FILE)
    
    # Ch·ªçn subset n·∫øu c·∫ßn ƒë·ªÉ test nhanh
    # df = df.head(100) 
    
    results = []
    
    print("üöÄ Generating MedCOT Reasoning Traces...")
    for idx, row in tqdm(df.iterrows(), total=len(df)):
        question = row['Question'] # Ho·∫∑c Question_Vi
        target_response = row['Response'] # Ho·∫∑c Response_Vi
        default_cot = row['Complex_CoT']
        
        # Sinh Custom CoT t·ª´ Pipeline
        medcot_reasoning = run_partial_pipeline(question)
        
        results.append({
            "question": question,
            "target": target_response,
            "default_cot": default_cot,       # Cho Model 3
            "medcot_reasoning": medcot_reasoning # Cho Model 1 & 2
        })
        
        # Save checkpoint m·ªói 100 d√≤ng
        if len(results) % 100 == 0:
            pd.DataFrame(results).to_json(OUTPUT_FILE, orient='records', lines=True)

    # Save final
    pd.DataFrame(results).to_json(OUTPUT_FILE, orient='records', lines=True)
    print(f"‚úÖ Data saved to {OUTPUT_FILE}")

if __name__ == "__main__":
    main()



===== .\run\import_primekg.sh =====
#!/bin/bash

VOLUME_NAME="medcot_primekg_data"

echo "üõë Stopping Neo4j container if running..."
docker-compose down

echo "üóëÔ∏è  Deleting old Neo4j data volume: $VOLUME_NAME..."
docker volume rm $VOLUME_NAME || true

echo "üöÄ Starting PrimeKG data import into Neo4j (WITH GDS PLUGIN on 5.26.18)..."

MSYS_NO_PATHCONV=1 docker run --interactive --tty --rm \
    --volume "$(pwd)/data/primekg/import":/import \
    --volume $VOLUME_NAME:/data \
    --env NEO4J_PLUGINS='["apoc", "graph-data-science"]' \
    neo4j:5.26.18 \
    neo4j-admin database import full \
    --nodes=/import/nodes.csv \
    --relationships=/import/edges.csv \
    --overwrite-destination \
    neo4j
# -----------------------------------------------------------------------------

if [ $? -eq 0 ]; then
    echo "‚úÖ IMPORT SUCCESSFUL!"
    echo "üëç Data has been imported into the '$VOLUME_NAME' volume."
    echo "üëâ You can now run 'bash run/run_primekg.sh' to start the server."
else
    echo "‚ùå IMPORT FAILED. Please check the errors above."
fi



===== .\run\prepare_gnn_dataset.py =====
import os
import torch
import pandas as pd
from tqdm import tqdm
from pathlib import Path
from src.core.state import MedCOTState
from src.modules import step0_preprocess, step1_extraction, step2_linking, step4_retrieval
from src.modules.step5_reasoning import _prepare_hetero_data_robust, load_encoder
from src.utils.neo4j_connect import db_connector

DATA_FILE = "data/medcot_rich_training_data.jsonl"
OUTPUT_DIR = Path("data/processed_gnn_data")
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

def main():
    if not db_connector: return
    encoder = load_encoder()
    
    if not os.path.exists(DATA_FILE):
        print(f"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y file d·ªØ li·ªáu ƒë·∫ßu v√†o {DATA_FILE}")
        return
        
    df = pd.read_json(DATA_FILE, lines=True)
    
    # --- S·ª¨A LOGIC L·ªåC ƒê·ªÇ CH·∫∂T CH·∫º H∆†N ---
    if 'verified_path_text' not in df.columns:
        print(f"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y c·ªôt 'verified_path_text' trong {DATA_FILE}.")
        return

    # L·ªçc c√°c d√≤ng c√≥ verified_path_text kh√¥ng r·ªóng v√† l√† m·ªôt chu·ªói
    df_filtered = df[df['verified_path_text'].apply(lambda x: isinstance(x, str) and len(x) > 5)].copy()
    
    if df_filtered.empty:
        print(f"‚ö†Ô∏è Kh√¥ng c√≥ m·∫´u h·ª£p l·ªá n√†o trong {DATA_FILE} ƒë·ªÉ t·∫°o d·ªØ li·ªáu GNN.")
        return
    # ----------------------------------------
    
    print(f"Processing {len(df_filtered)} samples for GNN dataset...")
    for idx, row in tqdm(df_filtered.iterrows(), total=len(df_filtered)):
        try:
            state = MedCOTState(raw_query=row['question'])
            state = step0_preprocess.run(state)
            state = step1_extraction.run(state)
            state = step2_linking.run(state)
            state = step4_retrieval.run(state, top_k_nodes=100)
            
            ug = state.graph_refs.get("ckg_subgraph")
            if not ug or not ug.get("nodes"): continue

            ckg_n = [n for n in ug["nodes"] if n.get("label") not in ["Patient", "Observation"]]
            psg_n = [n for n in ug["nodes"] if n.get("label") in ["Patient", "Observation"]]
            ckg_e = [e for e in ug["edges"] if e.get("provenance") != "PSG"]
            psg_e = [e for e in ug["edges"] if e.get("provenance") == "PSG"]

            ckg_d, _ = _prepare_hetero_data_robust(ckg_n, ckg_e, encoder)
            psg_d, _ = _prepare_hetero_data_robust(psg_n, psg_e, encoder)
            
            if not ckg_d.edge_types: continue

            # D√πng index g·ªëc ƒë·ªÉ ƒë·∫∑t t√™n file cho nh·∫•t qu√°n
            original_index = row.name 
            torch.save({
                "ckg_data": ckg_d,
                "psg_data": psg_d,
                "query_text": state.normalized_query,
                "target_path": row['verified_path_text']
            }, OUTPUT_DIR / f"sample_{original_index}.pt")
            
        except Exception as e:
            # print(f"Skipping sample {idx} due to error: {e}")
            pass

if __name__ == "__main__":
    main()

===== .\run\run_primekg.sh =====
#!/bin/bash

echo "üõë Stopping and removing any existing Neo4j containers..."
docker-compose down

echo "‚ñ∂Ô∏è  Starting the Neo4j container..."

docker-compose up -d

if [ $? -ne 0 ]; then
    echo "‚ùå Failed to start the Neo4j container. Please check the errors above."
    exit 1
fi

echo "üí§ Waiting for the server to initialize..."
sleep 5

echo "‚úÖ Container has been started."
echo "‚è≥ Tailing logs... (Press Ctrl+C to exit logs, the server will keep running in the background)"

docker-compose logs -f



===== .\run\train_gnn_next_hop.py =====
# run/train_gnn_next_hop.py
import logging
import torch
import torch.optim as optim
import os
import glob
from tqdm import tqdm
from sentence_transformers import SentenceTransformer
from src.models.dual_tower_gnn import CoGCoT_DualTower_GNN
import numpy as np

logging.basicConfig(level=logging.CRITICAL)

# --- CONFIG ---
DATA_DIR = "data/processed_gnn_data"
OUTPUT_PATH = "models/gnn_dual_tower_weights.pth"
EPOCHS = 10
LEARNING_RATE = 1e-4

def generate_training_samples_from_tensor(ckg_data, gold_path_text):
    # Logic t·∫°o sample t·ª´ data ƒë√£ pre-process
    # ƒê√¢y l√† logic gi·∫£ l·∫≠p: Map text path -> node index trong ckg_data
    # Th·ª±c t·∫ø c·∫ßn mapping ch√≠nh x√°c h∆°n t·ª´ ID node sang index
    # ·ªû ƒë√¢y ta d√πng random negative sampling ƒë∆°n gi·∫£n ƒë·ªÉ demo
    try:
        # L·∫•y danh s√°ch node types
        ntypes = ckg_data.node_types
        if not ntypes: return []
        
        # Ch·ªçn ƒë·∫°i di·ªán 1 node type ch√≠nh (VD: Disease)
        target_ntype = ntypes[0]
        num_nodes = ckg_data[target_ntype].x.shape[0]
        
        if num_nodes < 2: return []
        
        # Random Positive pair (Gi·∫£ l·∫≠p Next Hop)
        src_idx = np.random.randint(0, num_nodes)
        pos_idx = np.random.randint(0, num_nodes)
        
        # Negative samples
        neg_indices = np.random.choice(num_nodes, 5, replace=True).tolist()
        
        return [(target_ntype, src_idx, pos_idx, neg_indices)]
    except:
        return []

def main():
    print("üöÄ Starting Optimized GNN Training...")
    
    files = glob.glob(os.path.join(DATA_DIR, "*.pt"))
    if not files:
        print(f"‚ùå No .pt files found in {DATA_DIR}. Run prepare_gnn_dataset.py first.")
        return

    encoder = SentenceTransformer("all-MiniLM-L6-v2")
    
    # Load 1 sample ƒë·ªÉ init model
    sample_0 = torch.load(files[0])
    model = CoGCoT_DualTower_GNN(
        sample_0['ckg_data'].metadata(), 
        sample_0['psg_data'].metadata(), 
        hidden_channels=128, query_dim=384
    )
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
    loss_fn = torch.nn.BCEWithLogitsLoss()
    
    model.train()

    for epoch in range(EPOCHS):
        total_loss = 0
        valid_batches = 0
        
        # Shuffle files
        np.random.shuffle(files)
        
        pbar = tqdm(files, desc=f"Epoch {epoch+1}/{EPOCHS}")
        for file_path in pbar:
            try:
                data = torch.load(file_path)
                ckg_data = data['ckg_data']
                psg_data = data['psg_data']
                query_text = data['query_text']
                
                query_emb = encoder.encode(query_text, convert_to_tensor=True)
                
                # Forward Pass
                node_embs, _ = model(ckg_data, psg_data, query_emb)
                
                # Generate Samples & Compute Loss
                samples = generate_training_samples_from_tensor(ckg_data, data['target_path'])
                
                batch_loss = 0
                for ntype, src_idx, pos_idx, neg_idxs in samples:
                    if ntype not in node_embs: continue
                    
                    emb_matrix = node_embs[ntype]
                    src_emb = emb_matrix[src_idx]
                    pos_emb = emb_matrix[pos_idx]
                    
                    pos_score = torch.dot(src_emb, pos_emb)
                    
                    for neg_idx in neg_idxs:
                        neg_emb = emb_matrix[neg_idx]
                        neg_score = torch.dot(src_emb, neg_emb)
                        batch_loss += loss_fn(pos_score - neg_score, torch.tensor(1.0))
                
                if batch_loss != 0:
                    optimizer.zero_grad()
                    batch_loss.backward()
                    optimizer.step()
                    total_loss += batch_loss.item()
                    valid_batches += 1
                    
                pbar.set_postfix({'loss': total_loss / (valid_batches + 1e-9)})
                
            except Exception:
                continue

        print(f"Epoch {epoch+1} Avg Loss: {total_loss / (valid_batches + 1e-9):.4f}")

    torch.save(model.state_dict(), OUTPUT_PATH)
    print(f"‚úÖ GNN Model saved to {OUTPUT_PATH}")

if __name__ == "__main__":
    main()

===== .\run\train_verifier.py =====
# run/train_verifier.py
import torch
import torch.nn as nn
from torch.utils.data import TensorDataset, DataLoader
import numpy as np
from pathlib import Path
from src.models.verifier import MultiSignalVerifier

# --- CONFIG ---
# --- N√ÇNG C·∫§P: TƒÉng INPUT_DIM t·ª´ 7 l√™n 8 ---
INPUT_DIM = 8 # Features: [nli, gcot, in_kg, causality, len, src_deg, tgt_deg, provenance]
MODEL_PATH = Path("models/verifier_weights.pth")
MODEL_PATH.parent.mkdir(exist_ok=True)

def create_dummy_data(num_samples=1000):
    """T·∫°o d·ªØ li·ªáu gi·∫£ l·∫≠p ƒë·ªÉ hu·∫•n luy·ªán."""
    print("... Creating dummy training data ...")
    
    # Positive samples (good paths)
    pos_features = np.random.rand(num_samples // 2, INPUT_DIM)
    pos_features[:, 0] = np.random.uniform(0.7, 1.0, num_samples // 2) # High NLI
    pos_features[:, 2] = 1.0 # In KG
    pos_features[:, 7] = np.random.uniform(0.8, 1.0, num_samples // 2) # High Provenance (PrimeKG, PSG)
    pos_labels = np.ones(num_samples // 2)

    # Negative samples (bad paths)
    neg_features = np.random.rand(num_samples // 2, INPUT_DIM)
    neg_features[:, 0] = np.random.uniform(0.0, 0.4, num_samples // 2) # Low NLI
    neg_features[:, 7] = np.random.uniform(0.3, 0.5, num_samples // 2) # Low Provenance (RTX-KG2, Default)
    neg_labels = np.zeros(num_samples // 2)

    X = np.vstack([pos_features, neg_features])
    y = np.concatenate([pos_labels, neg_labels])

    # Shuffle
    idx = np.random.permutation(len(X))
    X, y = X[idx], y[idx]

    return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)

def main():
    # ... (Logic hu·∫•n luy·ªán gi·ªØ nguy√™n)
    print("üöÄ Starting Verifier Model Training...")
    X_train, y_train = create_dummy_data()
    dataset = TensorDataset(X_train, y_train.unsqueeze(1))
    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)
    
    model = MultiSignalVerifier(input_dim=INPUT_DIM)
    criterion = nn.BCEWithLogitsLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    
    epochs = 20
    for epoch in range(epochs):
        for batch_X, batch_y in dataloader:
            optimizer.zero_grad()
            outputs = model(batch_X)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
        print(f"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}")

    torch.save(model.state_dict(), MODEL_PATH)
    print(f"‚úÖ Model weights saved to {MODEL_PATH}")

if __name__ == "__main__":
    main()

===== .\run\__init__.py =====


===== .\src\__init__.py =====


===== .\src\core\config.py =====
# T·ªáp: src/core/config.py
LINKING_THRESHOLD = 0.7 
# (C√°c h·∫±ng s·ªë kh√°c gi·ªØ nguy√™n nh∆∞ tr∆∞·ªõc)
SPACY_MODEL_NAME = "xx_sent_ud_sm"
EXTRACTION_MODEL_NAME = "urchade/gliner_multi-v2.1"
ENTITY_LABELS = ["medical condition or disease", "symptom or sign", "drug or medication", "laboratory test", "anatomy or body part", "medical procedure", "gene or protein"]
GLINER_TO_INTERNAL_LABEL_MAP = {"medical condition or disease": "disease", "symptom or sign": "symptom", "drug or medication": "drug", "laboratory test": "lab_test", "medical procedure": "procedure", "anatomy or body part": "anatomy", "gene or protein": "gene"}
INTERNAL_LABEL_TO_KG_TYPE_MAP = {"disease": "Disease", "drug": "Drug", "symptom": "Effect/Phenotype", "anatomy": "Anatomy", "procedure": "Procedure", "lab_test": "Gene/Protein", "gene": "Gene/Protein"}
KG_TYPE_TO_INTERNAL_MAP = {v: k for k, v in INTERNAL_LABEL_TO_KG_TYPE_MAP.items()}
DEFAULT_EXTRACTION_THRESHOLD = 0.35
KNOWN_ENTITIES = {"diabetes": ("disease", "Disease"), "metformin": ("drug", "Drug"), "hypertension": ("disease", "Disease"), "warfarin": ("drug", "Drug"), "aspirin": ("drug", "Drug"), "kidney disease": ("disease", "Disease")}
DENSE_RETRIEVAL_MODEL = "BAAI/bge-small-en-v1.5"
RERANKER_MODEL = "cross-encoder/ms-marco-MiniLM-L-6-v2"
KG_TYPE_TO_UMLS_STY_MAP = {"disease": ["Disease or Syndrome", "Neoplastic Process", "Pathologic Function", "Congenital Abnormality", "Mental or Behavioral Dysfunction", "Injury or Poisoning"], "symptom": ["Sign or Symptom", "Finding", "Laboratory or Test Result"], "drug": ["Pharmacologic Substance", "Clinical Drug", "Antibiotic", "Biologically Active Substance"], "procedure": ["Therapeutic or Preventive Procedure", "Diagnostic Procedure", "Health Care Activity"], "anatomy": ["Body Part, Organ, or Organ Component", "Anatomical Structure", "Body Location or Region", "Tissue"], "gene": ["Gene or Genome", "Amino Acid, Peptide, or Protein", "Enzyme"], "lab_test": ["Laboratory Procedure", "Diagnostic Procedure"]}
NODE_EMBEDDING_DIM = 384  
HGT_HIDDEN_CHANNELS = 128
HGT_NUM_HEADS = 4
NLI_MODEL_NAME = "cross-encoder/nli-distilroberta-base"
WEIGHTS = {"in_kg": 0.35, "link_pred": 0.05, "nli": 0.15, "causality": 0.15, "gcot": 0.10, "trust": 0.20}

===== .\src\core\state.py =====
# src/core/state.py
from typing import List, Optional, Any, Dict, Tuple
from pydantic import BaseModel, Field
from datetime import datetime
import uuid

class Mention(BaseModel):
    text: str
    label: str
    span: Tuple[int, int]
    score: float
    source: str
    kg_type: Optional[str] = None
    attributes: Dict[str, Any] = Field(default_factory=dict)

class LinkedCandidate(BaseModel):
    node_id: str
    node_label: str
    preferred_name: str
    score: float
    source: str

class LinkedEntity(BaseModel):
    source_mention: Mention
    candidates: List[LinkedCandidate] = Field(default_factory=list)
    link_status: str = "unlinked"
    best_candidate: Optional[LinkedCandidate] = None

class MedCOTState(BaseModel):
    query_id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    created_at: datetime = Field(default_factory=datetime.now)

    raw_query: str
    patient_context: Optional[str] = None
    normalized_query: Optional[str] = None
    normalized_patient_context: Optional[str] = None 
    sentences: List[Dict[str, Any]] = Field(default_factory=list)
    mentions: List[Mention] = Field(default_factory=list)
    linked_entities: List[LinkedEntity] = Field(default_factory=list)
    seed_nodes: List[str] = Field(default_factory=list)
    unlinked_mentions: List[Mention] = Field(default_factory=list)
    graph_refs: Dict[str, Any] = Field(default_factory=dict)
    gcot: Dict[str, Any] = Field(default_factory=dict)
    candidate_paths: List[Dict[str, Any]] = Field(default_factory=list)
    verified_path: List[Dict[str, Any]] = Field(default_factory=list)
    global_confidence: float = 0.0
    reasoning_mode: str = "Abstain"
    final_answer: Optional[str] = None
    safety_flags: List[Dict[str, Any]] = Field(default_factory=list)
    logs: List[Dict[str, Any]] = Field(default_factory=list)

    def log(self, step: str, status: str, message: Any = "", metadata: dict = None):
        if metadata is None: metadata = {}
        
        # --- FIX P0-4: Auto-detect metadata ---
        if isinstance(message, dict) and not metadata:
            metadata = message
            message_str = ""
        else:
            message_str = str(message)
            
        self.logs.append({
            "step": step,
            "status": status,
            "message": message_str,
            "timestamp": datetime.now().isoformat(),
            **metadata
        })

===== .\src\core\__init__.py =====


===== .\src\modules\step0_preprocess.py =====
# src/modules/step0_preprocess.py
import re
import unicodedata
import json
from pathlib import Path
from typing import Optional, Dict
import logging

from presidio_analyzer import AnalyzerEngine
from presidio_anonymizer import AnonymizerEngine
from presidio_anonymizer.entities import OperatorConfig
import spacy

from src.core.state import MedCOTState

# Gi·∫£m log th·ª´a t·ª´ c√°c th∆∞ vi·ªán
logging.getLogger("PyRuSH").setLevel(logging.WARNING)
logging.getLogger("presidio-analyzer").setLevel(logging.WARNING)

logger = logging.getLogger("step0_preprocess")

_resources = {}

def load_resources():
    """T·∫£i c√°c t√†i nguy√™n c·∫ßn thi·∫øt cho preprocessing."""
    global _resources
    if _resources:
        return _resources
        
    logger.info("‚è≥ Loading Preprocessing resources...")
    try:
        # T·∫£i t·ª´ ƒëi·ªÉn vi·∫øt t·∫Øt (n·∫øu c√≥)
        dict_path = Path("data/dictionaries/abbreviations.json")
        _resources["abbreviations"] = json.load(open(dict_path, "r", encoding="utf-8")) if dict_path.is_file() else {}
        
        # Kh·ªüi t·∫°o Presidio ƒë·ªÉ ·∫©n th√¥ng tin nh·∫°y c·∫£m (PHI)
        _resources["analyzer"] = AnalyzerEngine()
        _resources["anonymizer"] = AnonymizerEngine()
        _resources["phi_operators"] = {
            "DEFAULT": OperatorConfig("replace", {"new_value": "<PHI>"}),
        }
        
        # T·∫£i model SpaCy ƒë·ªÉ t√°ch c√¢u
        _resources["nlp"] = spacy.load("en_core_web_sm") # D√πng model ti·∫øng Anh
        
    except Exception as e:
        logger.critical(f"‚ùå Failed to load preprocessing resources: {e}")
        # Th·ª≠ t·∫£i l·∫°i model spacy n·∫øu l·ªói
        try:
            import subprocess
            subprocess.run(["python", "-m", "spacy", "download", "en_core_web_sm"])
            _resources["nlp"] = spacy.load("en_core_web_sm")
        except Exception as spacy_e:
            logger.critical(f"‚ùå Spacy model download/load failed again: {spacy_e}")
            return {}
            
    return _resources

def _normalize_text(text: Optional[str]) -> str:
    """Chu·∫©n h√≥a text: b·ªè kho·∫£ng tr·∫Øng th·ª´a, normalize unicode."""
    if not text: return ""
    text = unicodedata.normalize('NFC', text)
    text = re.sub(r"[ \t]+", " ", text)
    text = re.sub(r"\n{3,}", "\n\n", text).strip()
    return text

def _expand_abbreviations(text: str, abbr_dict: Dict[str, str]) -> str:
    """Thay th·∫ø c√°c t·ª´ vi·∫øt t·∫Øt y khoa b·∫±ng d·∫°ng ƒë·∫ßy ƒë·ªß."""
    if not abbr_dict: return text
    for abbr, full_text in abbr_dict.items():
        pattern = r'\b' + re.escape(abbr) + r'\b'
        text = re.sub(pattern, full_text, text, flags=re.IGNORECASE)
    return text

def run(state: MedCOTState, enable_phi_redaction: bool = True) -> MedCOTState:
    """
    H√†m ch√≠nh c·ªßa pipeline Step 0.
    Th·ª±c hi·ªán: Chu·∫©n h√≥a, m·ªü r·ªông vi·∫øt t·∫Øt, ·∫©n th√¥ng tin nh·∫°y c·∫£m, t√°ch c√¢u.
    """
    resources = load_resources()
    if not resources.get("nlp") or not resources.get("analyzer"):
        state.log("0_PREPROCESS", "FAILED", "Resource loading failed.")
        raise RuntimeError("Preprocessing resources failed to load.")

    try:
        # 1. X·ª≠ l√Ω Query
        normalized_query = _normalize_text(state.raw_query)
        expanded_query = _expand_abbreviations(normalized_query, resources.get("abbreviations", {}))
        
        redacted_query = expanded_query
        if enable_phi_redaction:
            analyzer_results = resources["analyzer"].analyze(text=expanded_query, language='en')
            redacted_query = resources["anonymizer"].anonymize(
                text=expanded_query, analyzer_results=analyzer_results, operators=resources["phi_operators"]
            ).text
        state.normalized_query = redacted_query

        # 2. X·ª≠ l√Ω Patient Context
        if state.patient_context:
            normalized_context = _normalize_text(state.patient_context)
            expanded_context = _expand_abbreviations(normalized_context, resources.get("abbreviations", {}))
            
            redacted_context = expanded_context
            if enable_phi_redaction:
                analyzer_results = resources["analyzer"].analyze(text=expanded_context, language='en')
                redacted_context = resources["anonymizer"].anonymize(
                    text=expanded_context, analyzer_results=analyzer_results, operators=resources["phi_operators"]
                ).text
            state.normalized_patient_context = redacted_context
        
        # 3. T√°ch c√¢u (ch·ªâ t√°ch c√¢u c·ªßa query ch√≠nh)
        doc = resources["nlp"](state.normalized_query)
        state.sentences = [{"text": sent.text, "span": (sent.start_char, sent.end_char)} for sent in doc.sents]
        
        state.log("0_PREPROCESS", "SUCCESS", "Text normalized and sentences segmented.")
        
    except Exception as e:
        logger.exception("Error during preprocessing")
        state.log("0_PREPROCESS", "FAILED", message=str(e))
        raise e
        
    return state

===== .\src\modules\step10_logging.py =====
# src/modules/step10_logging.py
import logging
import json
from pathlib import Path
from datetime import datetime, date
from src.core.state import MedCOTState
import numpy as np

logging.basicConfig(level=logging.INFO, format='%(levelname)s:%(name)s:%(message)s')
logger = logging.getLogger("step10_logging")

def clean_for_json(obj):
    """
    H√†m ƒë·ªá quy ƒë·ªÉ l√†m s·∫°ch d·ªØ li·ªáu tr∆∞·ªõc khi l∆∞u v√†o JSON.
    X·ª≠ l√Ω:
    1. C√°c ki·ªÉu s·ªë c·ªßa Numpy (int64, float32, etc.) -> int, float c·ªßa Python
    2. Numpy Arrays -> List
    3. Datetime/Date -> ISO String
    4. C√°c ki·ªÉu d·ªØ li·ªáu c∆° b·∫£n -> Gi·ªØ nguy√™n
    """
    if isinstance(obj, (np.integer, np.int64)):
        return int(obj)
    if isinstance(obj, (np.floating, np.float32, np.float64)):
        return float(obj)
    if isinstance(obj, np.ndarray):
        return obj.tolist()
    if isinstance(obj, (datetime, date)):
        return obj.isoformat()
    if isinstance(obj, dict):
        return {k: clean_for_json(v) for k, v in obj.items()}
    if isinstance(obj, list):
        return [clean_for_json(i) for i in obj]
    return obj

def run(state: MedCOTState, output_dir: str = "output/audit_logs") -> MedCOTState:
    logger.info("üöÄ B·∫Øt ƒë·∫ßu ghi log v√† audit trail...")
    
    try:
        Path(output_dir).mkdir(parents=True, exist_ok=True)
        log_file = Path(output_dir) / f"{state.query_id}.json"
        
        # 1. L·∫•y dictionary thu·∫ßn t·ª´ Pydantic
        state_dict = state.model_dump(mode='python')

        # 2. L√†m s·∫°ch d·ªØ li·ªáu (Numpy + Datetime + c√°c ki·ªÉu s·ªë ƒë·∫∑c bi·ªát)
        clean_state_dict = clean_for_json(state_dict)

        # 3. Ghi file
        with open(log_file, 'w', encoding='utf-8') as f:
            json.dump(clean_state_dict, f, indent=2, ensure_ascii=False)
            
        logger.info(f"‚úÖ ƒê√£ l∆∞u audit trail v√†o {log_file}")
        state.log("10_LOGGING", "SUCCESS", metadata={"log_file": str(log_file)})

    except Exception as e:
        logger.exception("L·ªói trong qu√° tr√¨nh Logging")
        state.log("10_LOGGING", "FAILED", message=str(e))
        
    return state



===== .\src\modules\step1_extraction.py =====
# src/modules/step1_extraction.py
import logging
import re
from typing import List, Dict, Any

from gliner import GLiNER
import medspacy
from spacy.tokens import Span
from spacy.util import filter_spans
from spacy.matcher import PhraseMatcher  # <--- NEW IMPORT

from src.core.state import MedCOTState, Mention
from src.core import config

logging.getLogger("root").setLevel(logging.ERROR)
logging.basicConfig(level=logging.INFO, format='%(levelname)s:%(name)s:%(message)s')
logger = logging.getLogger("step1_hybrid_extraction")

_models = {}

def load_models():
    """T·∫£i model GLiNER v√† MedSpaCy + Kh·ªüi t·∫°o PhraseMatcher."""
    global _models
    if _models: return _models
    
    logger.info("‚è≥ ƒêang t·∫£i c√°c model cho Hybrid Extraction...")
    try:
        # 1. Load GLiNER
        _models["gliner"] = GLiNER.from_pretrained(config.EXTRACTION_MODEL_NAME)
        
        # 2. Load MedSpaCy
        try:
            nlp = medspacy.load(enable=["medspacy_pyrush", "medspacy_target_matcher", "medspacy_context"])
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Fallback to basic MedSpaCy: {e}")
            nlp = medspacy.load(enable=[]) 
            
        if not Span.has_extension("source_mention"):
            Span.set_extension("source_mention", default=None)
            
        _models["medspacy"] = nlp
        
        # 3. INIT PHRASE MATCHER (T·ªëi ∆∞u h√≥a t·ªëc ƒë·ªô)
        # Thay v√¨ regex loop, ta d√πng PhraseMatcher c·ªßa SpaCy
        logger.info("   ‚Ü≥ Building PhraseMatcher for Dictionary entities...")
        matcher = PhraseMatcher(nlp.vocab, attr="LOWER")
        patterns = []
        
        for term, (label, kg_type) in config.KNOWN_ENTITIES.items():
            # M√£ h√≥a metadata v√†o ID: "LABEL||KG_TYPE"
            pattern_id = f"{label}||{kg_type}"
            matcher.add(pattern_id, [nlp.make_doc(term)])
            
        _models["matcher"] = matcher
        
    except Exception as e:
        logger.error(f"‚ùå L·ªói t·∫£i model: {e}", exc_info=True)
        return {}
        
    return _models

def _run_ner_on_text(text: str) -> List[Dict[str, Any]]:
    """Helper ch·∫°y NER tr√™n m·ªôt ƒëo·∫°n text"""
    if not text: return []
    models = _models # ƒê·∫£m b·∫£o load_models ƒë√£ ch·∫°y
    if not models: return []
    
    # A. GLiNER Prediction (Deep Learning)
    gliner_ents = []
    try:
        raw_preds = models["gliner"].predict_entities(text, config.ENTITY_LABELS, threshold=config.DEFAULT_EXTRACTION_THRESHOLD)
        for e in raw_preds:
            final_label = config.GLINER_TO_INTERNAL_LABEL_MAP.get(e["label"], "unknown")
            gliner_ents.append({
                "text": e["text"], 
                "label": final_label, 
                "span": (e["start"], e["end"]), 
                "score": e["score"], 
                "source": "gliner"
            })
    except Exception:
        pass

    # B. Dictionary Match (PhraseMatcher - High Speed)
    dict_ents = []
    nlp = models["medspacy"]
    matcher = models["matcher"]
    
    doc = nlp(text) # Tokenize text
    matches = matcher(doc) # T√¨m ki·∫øm c·ª±c nhanh
    
    for match_id, start, end in matches:
        string_id = nlp.vocab.strings[match_id] # L·∫•y l·∫°i ID: "label||kg_type"
        label, kg_type = string_id.split("||")
        span_doc = doc[start:end]
        
        dict_ents.append({
            "text": span_doc.text,
            "label": label,
            "kg_type": kg_type,
            "span": (span_doc.start_char, span_doc.end_char), # Char offset chu·∫©n
            "score": 1.0,
            "source": "expert_dictionary"
        })
    
    # C. Merge: ∆Øu ti√™n Dictionary ƒë√® l√™n GLiNER
    if not dict_ents: return sorted(gliner_ents, key=lambda x: x['span'][0])
    
    final_entities = []
    # T·∫°o set c√°c kho·∫£ng span c·ªßa dictionary ƒë·ªÉ check overlap
    dict_ranges = set()
    for d in dict_ents:
        # Add range index v√†o set (ƒë∆°n gi·∫£n h√≥a overlap check)
        for i in range(d['span'][0], d['span'][1]):
            dict_ranges.add(i)
            
    for g_ent in gliner_ents:
        # Ki·ªÉm tra xem span c·ªßa GLiNER c√≥ ƒë√® l√™n Dictionary kh√¥ng
        is_overlapped = False
        for i in range(g_ent['span'][0], g_ent['span'][1]):
            if i in dict_ranges:
                is_overlapped = True
                break
        
        if not is_overlapped: 
            final_entities.append(g_ent)
            
    final_entities.extend(dict_ents)
    return sorted(final_entities, key=lambda x: x['span'][0])

def run(state: MedCOTState) -> MedCOTState:
    models = load_models()
    if not models:
        state.log("1_EXTRACTION", "FAILED", "Model load error.")
        raise RuntimeError("Extraction models failed to load.")
    
    all_raw_mentions = []
    
    # 1. Extract t·ª´ Query
    q_ments = _run_ner_on_text(state.normalized_query)
    for m in q_ments: m['source_doc'] = 'query'
    all_raw_mentions.extend(q_ments)

    # 2. Extract t·ª´ Patient Context
    if state.normalized_patient_context:
        c_ments = _run_ner_on_text(state.normalized_patient_context)
        for m in c_ments: m['source_doc'] = 'patient_context'
        all_raw_mentions.extend(c_ments)
    
    if not all_raw_mentions:
        state.log("1_EXTRACTION", "SKIPPED", "No entities found.")
        return state

    try:
        # G√°n kg_type chu·∫©n n·∫øu thi·∫øu
        for m in all_raw_mentions:
            if "kg_type" not in m:
                m["kg_type"] = config.INTERNAL_LABEL_TO_KG_TYPE_MAP.get(m["label"], "Phenotype")
        
        # 3. Context Analysis (Negation, Temporality)
        full_text = (state.normalized_query or "") + "\n" + (state.normalized_patient_context or "")
        nlp = models["medspacy"]
        doc = nlp(full_text)
        
        valid_spans = []
        offset_context = len(state.normalized_query or "") + 1
        
        for m in all_raw_mentions:
            start, end = m["span"]
            # ƒêi·ªÅu ch·ªânh offset n·∫øu mention n·∫±m ·ªü patient_context
            if m["source_doc"] == 'patient_context':
                start += offset_context
                end += offset_context
            
            # T·∫°o span an to√†n
            if start < len(full_text) and end <= len(full_text):
                span = doc.char_span(start, end, label=m["label"])
                if span is not None:
                    span._.set("source_mention", m)
                    valid_spans.append(span)

        # L·ªçc span tr√πng l·∫∑p b·∫±ng thu·∫≠t to√°n c·ªßa SpaCy
        doc.ents = filter_spans(valid_spans)
        
        final_list = []
        for ent in doc.ents:
            src = ent._.get("source_mention")
            # L·∫•y attributes t·ª´ Context Rules
            attrs = {}
            if ent._.is_negated: attrs['negated'] = True
            if ent._.is_historical: attrs['historical'] = True
            if ent._.is_hypothetical: attrs['hypothetical'] = True
            
            final_list.append(Mention(
                text=src["text"], 
                label=src["label"],
                span=src["span"], 
                score=src["score"],
                source=src["source_doc"], 
                kg_type=src["kg_type"],
                attributes=attrs
            ))

        state.mentions = final_list
        state.log("1_EXTRACTION", "SUCCESS", metadata={"count": len(final_list)})
        
    except Exception as e:
        logger.exception("Error in extraction run")
        state.log("1_EXTRACTION", "FAILED", str(e))
        
    return state

===== .\src\modules\step2_linking.py =====
# src/modules/step2_linking.py
import logging
import re
import numpy as np
from typing import List
from pathlib import Path

from sentence_transformers import CrossEncoder

from src.core.state import MedCOTState, Mention, LinkedEntity, LinkedCandidate
from src.utils.neo4j_connect import db_connector
from src.core import config
from utils.kg_faiss_index import load_resources as load_faiss_resources
from utils.umls_normalizer import umls_service, build_umls_db_from_rrf

logging.basicConfig(level=logging.INFO, format='%(levelname)s:%(name)s:%(message)s')
logger = logging.getLogger("step2_linking")

_resources = {}

def initialize_umls():
    db_path = Path("data/umls/umls_lookup.db")
    if not db_path.exists():
        logger.warning(f"Kh√¥ng t√¨m th·∫•y DB UMLS t·∫°i '{db_path}'.")
        logger.warning("S·∫Ω th·ª≠ x√¢y d·ª±ng t·ª´ file RRF. Vi·ªác n√†y c√≥ th·ªÉ m·∫•t r·∫•t nhi·ªÅu th·ªùi gian.")
        mrconso_path = Path("data/umls/MRCONSO.RRF")
        mrsty_path = Path("data/umls/MRSTY.RRF")
        if not mrconso_path.exists() or not mrsty_path.exists():
            raise FileNotFoundError(
                "Kh√¥ng t√¨m th·∫•y file MRCONSO.RRF ho·∫∑c MRSTY.RRF trong 'data/umls/'. "
                "Vui l√≤ng ƒë·∫∑t file UMLS v√†o th∆∞ m·ª•c n√†y."
            )
        build_umls_db_from_rrf(mrconso_path, mrsty_path, db_path)
    
    umls_service.connect()


def load_resources():
    global _resources
    if _resources: return _resources
    logger.info("‚è≥ Loading Linking Resources...")
    try:
        initialize_umls()

        index, meta, encoder = load_faiss_resources()
        _resources["faiss_index"] = index
        _resources["faiss_meta"] = meta
        _resources["encoder"] = encoder
        _resources["reranker"] = CrossEncoder(config.RERANKER_MODEL)
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Linking resources partial load failure: {e}")
    return _resources

def _find_candidates_in_primekg(search_text: str, resources: dict) -> List[LinkedCandidate]:
    candidates = []
    if "faiss_index" in resources:
        vec = resources["encoder"].encode([search_text], normalize_embeddings=True)
        D, I = resources["faiss_index"].search(np.asarray(vec, dtype="float32"), 5)
        for i, idx in enumerate(I[0]):
            if idx == -1: continue
            meta = resources["faiss_meta"][int(idx)]
            candidates.append(LinkedCandidate(
                node_id=meta["node_id"], 
                node_label=meta.get("labels", ["Unknown"])[0], 
                preferred_name=meta["name"], 
                score=float(D[0][i]), 
                source="dense"
            ))
    return candidates

def run(state: MedCOTState) -> MedCOTState:
    resources = load_resources()
    if not state.mentions:
        return state
    if not umls_service.conn:
        state.log("2_LINKING", "FAILED", "UMLS Service not connected.")
        return state
    
    final_linked = []
    
    for mention in state.mentions:
        le = LinkedEntity(source_mention=mention)
        
        target_stys_list = config.KG_TYPE_TO_UMLS_STY_MAP.get(mention.label)
        target_stys_tuple = tuple(target_stys_list) if target_stys_list else None
        
        norm_candidates = umls_service.normalize(
            text=mention.text,
            target_stys=target_stys_tuple,
            top_k=3
        )
        mention.attributes['umls_candidates'] = norm_candidates

        primekg_candidates = []
        linked_from_cui = None
        
        if norm_candidates:
            for norm_cand in norm_candidates:
                candidates_for_norm = _find_candidates_in_primekg(norm_cand['pref_name'], resources)
                if candidates_for_norm:
                    primekg_candidates.extend(candidates_for_norm)
                    linked_from_cui = norm_cand['cui']
                    break

        if not primekg_candidates:
            logger.info(f"Fallback: Searching for original text '{mention.text}'")
            primekg_candidates = _find_candidates_in_primekg(mention.text, resources)

        # --- N√ÇNG C·∫§P: L·ªåC ·ª®NG VI√äN THEO LO·∫†I TH·ª∞C TH·ªÇ (TYPE-AWARE FILTERING) ---
        if primekg_candidates and mention.kg_type:
            filtered_candidates = [
                cand for cand in primekg_candidates 
                if mention.kg_type.lower() in cand.node_label.lower()
            ]
            if filtered_candidates:
                primekg_candidates = filtered_candidates
            else:
                logger.warning(f"Type filtering for '{mention.text}' ({mention.kg_type}) removed all candidates. Keeping original list.")
        # --- K·∫æT TH√öC N√ÇNG C·∫§P ---

        if primekg_candidates:
            pairs = [[mention.text, c.preferred_name] for c in primekg_candidates]
            scores = resources["reranker"].predict(pairs)
            
            for i, c in enumerate(primekg_candidates):
                reranker_prob = float(1 / (1 + np.exp(-scores[i])))
                c.score = 0.4 * c.score + 0.6 * reranker_prob
            
            primekg_candidates.sort(key=lambda x: x.score, reverse=True)

            if primekg_candidates and primekg_candidates[0].score >= config.LINKING_THRESHOLD:
                le.link_status = "linked"
                le.best_candidate = primekg_candidates[0]
                le.candidates = primekg_candidates[:3]
                if linked_from_cui:
                    if not hasattr(le.best_candidate, 'attributes'):
                        le.best_candidate.attributes = {}
                    le.best_candidate.attributes['cui'] = linked_from_cui

        final_linked.append(le)

    state.linked_entities = final_linked
    state.seed_nodes = list(set([le.best_candidate.node_id for le in final_linked if le.link_status == "linked"]))
    state.log("2_LINKING", "SUCCESS", {"linked_count": len(state.seed_nodes)})
    
    return state

===== .\src\modules\step4_retrieval.py =====
# T·ªáp: src/modules/step4_retrieval.py
import logging
import uuid
from typing import Dict, Any, List
from src.core.state import MedCOTState
from src.utils.neo4j_connect import db_connector
# --- IMPORT M·ªöI ƒê·ªÇ S·ª¨ D·ª§NG ARAX ---
from src.utils.arax_client import arax_client

logging.basicConfig(level=logging.INFO, format="%(levelname)s:%(name)s:%(message)s")
logger = logging.getLogger("step4_retrieval")

def _parse_and_flatten_results(results: List[Dict]) -> Dict[str, Any]:
    """H√†m helper ƒë·ªÉ x·ª≠ l√Ω v√† l√†m ph·∫≥ng k·∫øt qu·∫£ tr·∫£ v·ªÅ t·ª´ Neo4j."""
    if not results: return {"nodes": [], "edges": []}
    unique_nodes, unique_edges = {}, {}

    for record in results:
        for node in record.get("nodes_data", []):
            if node and node.get('id'):
                unique_nodes[node['id']] = {
                    "id": node['id'], "label": node['labels'][0] if node['labels'] else "Unknown",
                    "name": node.get('name', 'Unknown'), "provenance": "PrimeKG"
                }
        for rel in record.get("rels_data", []):
            if rel and rel.get('id'):
                unique_edges[rel['id']] = {
                    "source": rel['source'], "target": rel['target'],
                    "type": rel['type'], "provenance": "PrimeKG"
                }

    valid_ids = set(unique_nodes.keys())
    clean_edges = [e for e in unique_edges.values() if e['source'] in valid_ids and e['target'] in valid_ids]
    return {"nodes": list(unique_nodes.values()), "edges": clean_edges}

def _run_bfs_expansion(seed_ids: List[str], limit: int = 150) -> Dict[str, Any]:
    """Truy v·∫•n ƒë·ªì th·ªã con t·ª´ c∆° s·ªü d·ªØ li·ªáu Neo4j (PrimeKG local)."""
    if not seed_ids or db_connector is None: return {"nodes": [], "edges": []}
    logger.info(f"   üï∏ [BFS - PrimeKG] Expanding subgraph from local Neo4j...")
    
    query = """
    MATCH (seed) WHERE elementId(seed) IN $seeds
    CALL apoc.path.subgraphAll(seed, {
        maxLevel: 2
    })
    YIELD nodes, relationships

    WITH 
        [n IN nodes WHERE n IS NOT NULL | {id: elementId(n), labels: labels(n), name: n.name}] AS nodes_data,
        [r IN relationships WHERE r IS NOT NULL | {id: elementId(r), source: elementId(startNode(r)), target: elementId(endNode(r)), type: type(r)}] AS rels_data
    RETURN nodes_data, rels_data
    LIMIT 1
    """
    try:
        results = db_connector.run_query(query, {"seeds": seed_ids})
        graph = _parse_and_flatten_results(results)
        
        if len(graph["nodes"]) > limit:
            graph["nodes"] = graph["nodes"][:limit]
            valid = set(n['id'] for n in graph["nodes"])
            graph["edges"] = [e for e in graph["edges"] if e['source'] in valid and e['target'] in valid]
        return graph
    except Exception as e:
        logger.error(f"BFS Query on PrimeKG failed: {e}")
        return {"nodes": [], "edges": []}

def _build_patient_state_graph(state: MedCOTState) -> Dict[str, Any]:
    """X√¢y d·ª±ng ƒë·ªì th·ªã ng·ªØ c·∫£nh b·ªánh nh√¢n (Patient State Graph - PSG)."""
    psg_nodes, psg_edges = [], []
    pid = f"PATIENT_{state.query_id[:8]}"
    psg_nodes.append({"id": pid, "label": "Patient", "name": "The Patient", "provenance": "PSG"})
    for le in state.linked_entities:
        if le.link_status == "linked" and le.source_mention.source == "patient_context":
            evt_id = f"EVT_{uuid.uuid4().hex[:6]}"
            psg_nodes.append({"id": evt_id, "label": "Observation", "name": le.source_mention.text, "provenance": "PSG"})
            psg_edges.append({"source": evt_id, "target": str(le.best_candidate.node_id), "type": "GROUNDED_IN", "provenance": "PSG"})
            psg_edges.append({"source": pid, "target": evt_id, "type": "PRESENTS_WITH", "provenance": "PSG"})
    return {"nodes": psg_nodes, "edges": psg_edges}

def run(state: MedCOTState, use_arax_fallback: bool = True) -> MedCOTState:
    """
    H√†m ch√≠nh c·ªßa Step 4, gi·ªù ƒë√¢y k·∫øt h·ª£p c·∫£ PrimeKG v√† ARAX.
    """
    logger.info("üöÄ Step 4: Hybrid Retrieval (PrimeKG + ARAX Fallback)")
    
    # 1. L·∫•y d·ªØ li·ªáu t·ª´ PrimeKG local (lu√¥n ch·∫°y)
    local_graph = _run_bfs_expansion(state.seed_nodes)
    
    # --- 2. L·∫§Y TH√äM D·ªÆ LI·ªÜU T·ª™ ARAX (N·∫æU ƒê∆Ø·ª¢C B·∫¨T) ---
    arax_graph = {"nodes": [], "edges": []}
    if use_arax_fallback:
        entity_names_to_query = [
            le.best_candidate.preferred_name 
            for le in state.linked_entities if le.link_status == "linked"
        ]
        
        if entity_names_to_query:
            logger.info(f"üåç [Fallback - ARAX] Querying external KG for: {entity_names_to_query}")
            # ARAX client tr·∫£ v·ªÅ m·ªôt danh s√°ch c√°c C·∫†NH
            arax_edges = arax_client.query_kg2(entity_names_to_query, max_results=5)
            
            if arax_edges:
                arax_nodes_map = {}
                # T·ª´ danh s√°ch c·∫°nh, ch√∫ng ta t·ª± x√¢y d·ª±ng danh s√°ch node t∆∞∆°ng ·ª©ng
                for edge in arax_edges:
                    s_id, s_name = edge.get('source_id', edge['source']), edge['source']
                    t_id, t_name = edge.get('target_id', edge['target']), edge['target']
                    
                    # Th√™m node v√†o map ƒë·ªÉ t·ª± ƒë·ªông lo·∫°i b·ªè tr√πng l·∫∑p
                    arax_nodes_map[s_id] = {"id": s_id, "label": "ExternalEntity", "name": s_name, "provenance": "ARAX/KG2"}
                    arax_nodes_map[t_id] = {"id": t_id, "label": "ExternalEntity", "name": t_name, "provenance": "ARAX/KG2"}
                    
                    # C·∫≠p nh·∫≠t l·∫°i c·∫°nh ƒë·ªÉ d√πng ID thay v√¨ t√™n, cho nh·∫•t qu√°n
                    edge['source'] = s_id
                    edge['target'] = t_id
                
                arax_graph["nodes"] = list(arax_nodes_map.values())
                arax_graph["edges"] = arax_edges
                logger.info(f"‚úÖ ARAX returned {len(arax_graph['nodes'])} nodes and {len(arax_graph['edges'])} edges.")

    # 3. X√¢y d·ª±ng ƒë·ªì th·ªã ng·ªØ c·∫£nh b·ªánh nh√¢n
    psg = _build_patient_state_graph(state)
    
    # 4. G·ªôp k·∫øt qu·∫£ t·ª´ c·∫£ 3 ngu·ªìn: PrimeKG, ARAX, v√† PSG
    logger.info("   - Merging graphs from all sources...")
    merged_nodes = local_graph.get("nodes", []) + arax_graph.get("nodes", []) + psg.get("nodes", [])
    merged_edges = local_graph.get("edges", []) + arax_graph.get("edges", []) + psg.get("edges", [])
    
    # D√πng dictionary ƒë·ªÉ lo·∫°i b·ªè c√°c node tr√πng l·∫∑p theo ID
    final_node_map = {n['id']: n for n in merged_nodes}
    valid_node_ids = set(final_node_map.keys())
    
    # L·ªçc l·∫°i c√°c c·∫°nh ƒë·ªÉ ƒë·∫£m b·∫£o c·∫£ node ngu·ªìn v√† ƒë√≠ch ƒë·ªÅu t·ªìn t·∫°i
    final_edges = [e for e in merged_edges if e['source'] in valid_node_ids and e['target'] in valid_node_ids]
    
    state.graph_refs["ckg_subgraph"] = {"nodes": list(final_node_map.values()), "edges": final_edges}
    
    arax_was_used = use_arax_fallback and bool(arax_graph.get("edges"))
    state.log(
        "4_RETRIEVAL", 
        "SUCCESS", 
        metadata={
            "nodes": len(final_node_map), 
            "edges": len(final_edges),
            "arax_used": arax_was_used
        }
    )
    return state

===== .\src\modules\step5_reasoning.py =====
import logging
import torch
import numpy as np
from sentence_transformers import SentenceTransformer
from torch_geometric.data import HeteroData
from pathlib import Path
from src.core.state import MedCOTState
from src.models.dual_tower_gnn import CoGCoT_DualTower_GNN

# --- C·∫§U H√åNH LOGGING ƒê·ªÇ T·∫ÆT R√ÅC ---
# T·∫Øt log DEBUG c·ªßa PyRuSH v√† c√°c th∆∞ vi·ªán kh√°c ƒë·ªÉ log g·ªçn g√†ng
logging.getLogger("PyRuSH").setLevel(logging.ERROR)
logging.basicConfig(level=logging.INFO, format='%(levelname)s:%(name)s:%(message)s')
logger = logging.getLogger("step5_dual_tower")
# ------------------------------------

_encoder = None
GNN_MODEL_PATH = Path("models/gnn_dual_tower_weights.pth")

def load_encoder():
    global _encoder
    if _encoder is None: 
        logger.info("loading sentence transformer...")
        _encoder = SentenceTransformer("all-MiniLM-L6-v2") 
    return _encoder

def _prepare_hetero_data_robust(nodes, edges, encoder):
    """
    H√†m n√†y t·∫°o data tr√™n CPU, ta s·∫Ω chuy·ªÉn l√™n GPU sau.
    """
    data = HeteroData()
    if not nodes: return data, {}

    # Group nodes and create embeddings (tr√™n CPU)
    grouped_nodes = {}
    node_id_to_idx = {} 
    
    for n in nodes:
        lbl = n.get("label", "Unknown").replace("/", "_").replace(" ", "_")
        if lbl not in grouped_nodes: grouped_nodes[lbl] = []
        current_idx = len(grouped_nodes[lbl])
        node_id_to_idx[n['id']] = (lbl, current_idx)
        grouped_nodes[lbl].append(n)

    for lbl, nlist in grouped_nodes.items():
        texts = [n.get("name", "Unknown") for n in nlist]
        embs = encoder.encode(texts, show_progress_bar=False)
        data[lbl].x = torch.tensor(embs, dtype=torch.float32)

    # Process Edges
    edge_index_map = {}
    for e in edges:
        s_id, t_id = e["source"], e["target"]
        if s_id not in node_id_to_idx or t_id not in node_id_to_idx:
            continue
        s_lbl, s_idx = node_id_to_idx[s_id]
        t_lbl, t_idx = node_id_to_idx[t_id]
        e_type = e.get("type", "RELATED").upper()
        triplet = (s_lbl, e_type, t_lbl)
        if triplet not in edge_index_map:
            edge_index_map[triplet] = [[], []]
        edge_index_map[triplet][0].append(s_idx)
        edge_index_map[triplet][1].append(t_idx)

    for triplet, indices in edge_index_map.items():
        if len(indices[0]) > 0:
            data[triplet].edge_index = torch.tensor(indices, dtype=torch.long)
    
    legacy_node_map = {}
    for lbl, nlist in grouped_nodes.items():
        legacy_node_map[lbl] = {n['id']: i for i, n in enumerate(nlist)}

    return data, legacy_node_map

def run(state: MedCOTState, num_think_steps: int = 2) -> MedCOTState:
    ug = state.graph_refs.get("ckg_subgraph")
    if not ug or not ug.get("nodes"):
        state.log("5_REASONING", "SKIPPED", "No subgraph")
        return state

    encoder = load_encoder()
    
    # --- S·ª¨A L·ªñI DEVICE ---
    # 1. X√°c ƒë·ªãnh thi·∫øt b·ªã ƒë√≠ch (GPU n·∫øu c√≥, kh√¥ng th√¨ CPU)
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"GNN running on device: {device}")
    
    # 2. T·∫°o Tensors (m·∫∑c ƒë·ªãnh tr√™n CPU ho·∫∑c GPU)
    q_emb = encoder.encode(state.normalized_query, convert_to_tensor=True) if state.normalized_query else torch.zeros(384)

    ckg_nodes = [n for n in ug["nodes"] if n.get("provenance") != "PSG"]
    psg_nodes = [n for n in ug["nodes"] if n.get("provenance") == "PSG"]
    ckg_edges = [e for e in ug["edges"] if e.get("provenance") != "PSG"]
    psg_edges = [e for e in ug["edges"] if e.get("provenance") == "PSG"]

    ckg_d, ckg_m = _prepare_hetero_data_robust(ckg_nodes, ckg_edges, encoder)
    psg_d, psg_m = _prepare_hetero_data_robust(psg_nodes, psg_edges, encoder)

    # 3. Chuy·ªÉn t·∫•t c·∫£ m·ªçi th·ª© l√™n c√πng m·ªôt device
    q_emb = q_emb.to(device)
    ckg_d = ckg_d.to(device)
    psg_d = psg_d.to(device)
    # -----------------------

    state.graph_refs["node_map"] = ckg_m

    if not ckg_d.node_types: 
        state.log("5_REASONING", "SKIPPED", "Empty CKG Data")
        return state

    try:
        # Load model v√† chuy·ªÉn n√≥ l√™n device
        model = CoGCoT_DualTower_GNN(ckg_d.metadata(), psg_d.metadata(), 128, 384, num_think_steps)
        model.to(device) # <--- QUAN TR·ªåNG
        
        if GNN_MODEL_PATH.exists(): 
            try:
                # Load weights v√†o model ƒë√£ ·ªü tr√™n GPU
                model.load_state_dict(torch.load(GNN_MODEL_PATH, map_location=device), strict=False)
            except Exception as load_err:
                logger.warning(f"Could not load GNN weights: {load_err}")
        
        model.eval()
        with torch.no_grad():
            # B√¢y gi·ªù t·∫•t c·∫£ input v√† model ƒë·ªÅu ·ªü tr√™n GPU
            final_x, thoughts = model(ckg_d, psg_d, q_emb)

        # Chuy·ªÉn k·∫øt qu·∫£ v·ªÅ l·∫°i CPU ƒë·ªÉ l∆∞u tr·ªØ (numpy/json kh√¥ng ƒë·ªçc ƒë∆∞·ª£c tensor GPU)
        final_node_embeddings = {}
        for nt, feat in final_x.items():
            final_node_embeddings[nt] = feat.cpu().numpy()
        state.graph_refs["final_node_embeddings"] = final_node_embeddings
            
        state.gcot["thought_vectors"] = thoughts
        state.log("5_REASONING", "SUCCESS", {"thoughts": len(thoughts)})

    except Exception as e:
        logger.error(f"GNN Error handled gracefully: {e}", exc_info=True) # exc_info=True ƒë·ªÉ in traceback
        state.log("5_REASONING", "FAILED_BUT_CONTINUED", str(e))

    return state

===== .\src\modules\step6_path_generation.py =====
# src/modules/step6_path_generation.py
import logging
import numpy as np
from sentence_transformers import SentenceTransformer, util, CrossEncoder
from src.core.state import MedCOTState

logging.basicConfig(level=logging.INFO, format='%(levelname)s:%(name)s:%(message)s')
logger = logging.getLogger("step6_constrained_path_gen")
_models = {}

# --- N√ÇNG C·∫§P THEO CH·ªêT 6: R√ÄNG BU·ªòC T√åM KI·∫æM THEO NG·ªÆ NGHƒ®A ---
SEMANTIC_CONSTRAINTS = {
    "TREATMENT": ["indication", "treats", "prevents", "mitigates"],
    "SAFETY": ["contraindication", "side effect", "adverse reaction", "risk_of", "causes", "interacts_with"],
    "DIAGNOSIS": ["biomarker", "associated_with", "has_symptom", "presents_with"],
    "GENERIC": [] # Ch·∫ø ƒë·ªô Fallback kh√¥ng c√≥ r√†ng bu·ªôc
}

def load_models():
    if not _models:
        _models["embedder"] = SentenceTransformer("all-MiniLM-L6-v2")
        _models["reranker"] = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")
    return _models["embedder"], _models["reranker"]

def detect_query_intent(query: str) -> str:
    q = query.lower()
    if any(w in q for w in ["treat", "cure", "therapy", "manage", "medication", "drug for"]): return "TREATMENT"
    if any(w in q for w in ["safe", "risk", "contraindicat", "bad", "interaction", "warn", "avoid"]): return "SAFETY"
    if any(w in q for w in ["diagnos", "test", "check", "symptom", "sign", "cause"]): return "DIAGNOSIS"
    return "GENERIC"

class ConstrainedPathGenerator:
    def __init__(self, state: MedCOTState, embedder):
        self.state = state
        self.embedder = embedder
        self.query_emb = embedder.encode(state.normalized_query, convert_to_tensor=True) if state.normalized_query else None
        self.intent = detect_query_intent(state.normalized_query)
        self.meta = {n['id']: n for n in state.graph_refs.get("ckg_subgraph", {}).get("nodes", [])}
        self.adj = self._build_adj(strict_mode=True)
        self.used_fallback = False

    def _build_adj(self, strict_mode=True):
        adj, edges = {}, self.state.graph_refs.get("ckg_subgraph", {}).get("edges", [])
        allowed = SEMANTIC_CONSTRAINTS.get(self.intent, []) if strict_mode else []
        
        valid_count = 0
        for e in edges:
            s, t, raw_type = e["source"], e["target"], e["type"].lower()
            if strict_mode and self.intent != "GENERIC" and allowed and not any(valid in raw_type for valid in allowed): continue
            valid_count += 1
            info = {"node": t, "edge_raw": e["type"], "edge_text": raw_type.replace("_", " "), "provenance": e.get("provenance", "DEFAULT")}
            adj.setdefault(s, []).append(info)
        
        logger.info(f"üï∏ Adj built (Strict={strict_mode}, Intent={self.intent}). Valid edges: {valid_count}/{len(edges)}")
        return adj

    def enable_fallback(self):
        logger.warning("‚ö†Ô∏è No paths found with strict constraints. Switching to GENERIC mode.")
        self.intent = "GENERIC"
        self.adj = self._build_adj(strict_mode=False)
        self.used_fallback = True

    def search(self, width=50, depth=3):
        if self.query_emb is None or not self.state.seed_nodes: return []
        seeds = [s for s in self.state.seed_nodes if s in self.adj]
        if not seeds: return []
        
        beam = [(0.0, [{"node_id": s}]) for s in seeds]
        final = []
        
        for _ in range(depth): # Max path length = depth
            candidates = []
            for score, path in beam:
                curr_node_id = path[-1]['node_id']
                if len(path) > 1: final.append((score, path))
                
                neighbors = self.adj.get(curr_node_id, [])
                current_path_nodes = {step['node_id'] for step in path}
                valid_neighbors = [n for n in neighbors if n['node'] not in current_path_nodes] # Cycle Control
                if not valid_neighbors: continue
                
                texts = [f"{n['edge_text']} {self.meta.get(n['node'], {}).get('name', '')}" for n in valid_neighbors]
                sem_sims = util.cos_sim(self.query_emb, self.embedder.encode(texts, convert_to_tensor=True))[0].cpu().numpy()
                
                for i, nb in enumerate(valid_neighbors):
                    new_step = {"node_id": nb['node'], "edge_raw": nb["edge_raw"], "edge_text": nb["edge_text"], "provenance": nb["provenance"]}
                    candidates.append((score + float(sem_sims[i]), path + [new_step]))
            
            if not candidates: break
            candidates.sort(key=lambda x: x[0], reverse=True)
            beam = candidates[:width]
        
        results, seen_paths = [], set()
        for score, path in sorted(final + beam, key=lambda x: x[0], reverse=True):
            if len(path) < 2: continue
            clean_path, parts = [], []
            for i in range(len(path) - 1):
                s_name = self.meta.get(path[i]['node_id'], {}).get('name', 'Unknown')
                t_name = self.meta.get(path[i+1]['node_id'], {}).get('name', 'Unknown')
                step_info = {"source": path[i]['node_id'], "target": path[i+1]['node_id'], "edge": path[i+1]['edge_raw'], "edge_text": path[i+1]['edge_text'], "provenance": path[i+1]['provenance']}
                clean_path.append(step_info)
                parts.append(f"{s_name} --[{step_info['edge_text']}]--> {t_name}")
            
            text_repr = " ".join(parts)
            if text_repr not in seen_paths:
                seen_paths.add(text_repr)
                results.append({"path": clean_path, "text_repr": text_repr, "score": float(score)})
        return results[:width]

def run(state: MedCOTState, beam_width: int = 50, max_path_length: int = 3) -> MedCOTState: # Max hops = 3-1 = 2
    try:
        embedder, reranker = load_models()
        gen = ConstrainedPathGenerator(state, embedder)
        
        paths = gen.search(width=beam_width, depth=max_path_length)
        if not paths:
            gen.enable_fallback()
            paths = gen.search(width=beam_width, depth=max_path_length)
        
        if not paths: 
            state.log("6_PATH_GEN", "SKIPPED", {"msg": "No paths found even with fallback"})
            return state
        
        path_texts = [[state.normalized_query, p["text_repr"]] for p in paths]
        scores = reranker.predict(path_texts)
        for i, p in enumerate(paths): 
            p['final_score'] = 0.3 * p['score'] + 0.7 * (1 / (1 + np.exp(-scores[i])))
        
        state.candidate_paths = sorted(paths, key=lambda x: x['final_score'], reverse=True)[:10]
        state.log("6_PATH_GEN", "SUCCESS", {"count": len(state.candidate_paths), "intent": gen.intent, "fallback_used": gen.used_fallback})
        
    except Exception as e:
        logger.exception("Path Gen Error")
        state.log("6_PATH_GEN", "FAILED", str(e))
    return state

===== .\src\modules\step7_verification.py =====
# src/modules/step7_verification.py
import logging
import numpy as np
import torch
from sentence_transformers import CrossEncoder
from pathlib import Path

from src.core.state import MedCOTState
from src.core import config
from src.models.verifier import MultiSignalVerifier

logging.basicConfig(level=logging.INFO, format='%(levelname)s:%(name)s:%(message)s')
logger = logging.getLogger("step7_verification_provenance")
_resources = {}
VERIFIER_MODEL_PATH = Path("models/verifier_weights.pth")

# --- N√ÇNG C·∫§P THEO CH·ªêT 7: TH·ª® T·ª∞ ∆ØU TI√äN NGU·ªíN G·ªêC ---
PROVENANCE_SCORES = {
    "PSG": 1.0,           # Patient State Graph (S·ª± th·∫≠t c·ªßa ca b·ªánh)
    "PrimeKG": 0.85,      # KG Local, ƒë√£ ƒë∆∞·ª£c curate
    "ARAX/KG2": 0.6,      # KG t·ª´ xa, ngu·ªìn ƒëa d·∫°ng
    "User_Upload": 0.9,   # D·ªØ li·ªáu do ng∆∞·ªùi d√πng n·∫°p, ∆∞u ti√™n
    "DEFAULT": 0.3        # M·∫∑c ƒë·ªãnh n·∫øu kh√¥ng c√≥ ngu·ªìn
}

def load_resources():
    global _resources
    if _resources: return _resources
    _resources['nli_model'] = CrossEncoder(config.NLI_MODEL_NAME)
    # --- N√ÇNG C·∫§P: INPUT_DIM TƒÇNG T·ª™ 7 L√äN 8 ƒê·ªÇ TH√äM PROVENANCE ---
    _resources['verifier_model'] = MultiSignalVerifier(input_dim=8)
    if VERIFIER_MODEL_PATH.exists():
        _resources['verifier_model'].load_state_dict(torch.load(VERIFIER_MODEL_PATH))
    _resources['verifier_model'].eval()
    return _resources

def _get_node_meta(node_id, state):
    for node in state.graph_refs.get("ckg_subgraph", {}).get("nodes", []):
        if node["id"] == node_id: return node
    return None

def _extract_path_features(path, state, nli_model):
    path_features = []
    node_map = {n['id']: n for n in state.graph_refs.get("ckg_subgraph", {}).get("nodes", [])}
    
    for step in path:
        src_meta = node_map.get(step['source'])
        tgt_meta = node_map.get(step['target'])
        if not src_meta or not tgt_meta: continue

        step_text = f"{src_meta['name']} {step.get('edge_text', step['edge'])} {tgt_meta['name']}"
        try:
            scores = nli_model.predict([(state.normalized_query, step_text)])
            probs = torch.softmax(torch.tensor(scores), dim=-1)
            nli_score = float(probs[0][-1]) # L·∫•y ƒëi·ªÉm c·ªßa "entailment"
        except Exception: nli_score = 0.5

        # --- N√ÇNG C·∫§P: TH√äM T√çN HI·ªÜU PROVENANCE V√ÄO VECTOR ---
        provenance_score = PROVENANCE_SCORES.get(step.get("provenance", "DEFAULT"), 0.3)
        
        # [nli, gcot, in_kg, causality, len, src_deg, tgt_deg, provenance]
        features = [ nli_score, 0.5, 1.0, 0.5, len(path), 1, 1, provenance_score ]
        path_features.append(features)
        
    return np.mean(path_features, axis=0) if path_features else None

def run(state: MedCOTState) -> MedCOTState:
    if not state.candidate_paths:
        state.reasoning_mode = "Abstain"
        return state

    resources = load_resources()
    path_vectors, valid_candidates = [], []
    for cand in state.candidate_paths:
        feats = _extract_path_features(cand['path'], state, resources['nli_model'])
        if feats is not None:
            path_vectors.append(feats)
            valid_candidates.append(cand)
            
    if not path_vectors:
        state.reasoning_mode = "Abstain"
        return state

    with torch.no_grad():
        logits = resources['verifier_model'](torch.tensor(np.array(path_vectors), dtype=torch.float32))
        confidences = torch.sigmoid(logits).squeeze().cpu().numpy()
        if np.ndim(confidences) == 0: confidences = [float(confidences)]

    for i, cand in enumerate(valid_candidates):
        cand['verification_confidence'] = float(confidences[i])

    verified_results = sorted(valid_candidates, key=lambda x: x['verification_confidence'], reverse=True)
    
    if verified_results and verified_results[0]['verification_confidence'] > 0.5:
        best = verified_results[0]
        state.verified_path = best['path']
        state.global_confidence = best['verification_confidence']
        state.reasoning_mode = "Graph-Strict" if state.global_confidence > 0.8 else "Cautious"
        state.gcot['verified_path_text'] = best.get('text_repr', '')
    else:
        state.reasoning_mode = "Abstain"
        
    state.log("7_VERIFICATION", "SUCCESS", {"mode": state.reasoning_mode, "conf": state.global_confidence})
    return state

===== .\src\modules\step8_synthesis.py =====
# src/modules/step8_synthesis.py
import logging
import os
from src.core.state import MedCOTState

try:
    from google import genai
except ImportError: 
    genai = None

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("step8_synthesis")
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")

def run(state: MedCOTState) -> MedCOTState:
    # --- 1. T·ªïng h·ª£p b·∫±ng ch·ª©ng t·ª´ c√°c b∆∞·ªõc tr∆∞·ªõc ---
    evidence_lines = []
    
    # Map ID sang T√™n cho d·ªÖ ƒë·ªçc
    node_map = {}
    if state.graph_refs.get("ckg_subgraph"):
        for n in state.graph_refs["ckg_subgraph"]["nodes"]:
            node_map[n['id']] = n.get('name', n['id'])

    # ∆Øu ti√™n d√πng ƒë∆∞·ªùng ƒëi ƒë√£ ƒë∆∞·ª£c ki·ªÉm ch·ª©ng (Step 7)
    if state.verified_path:
        evidence_lines.append("‚úÖ **Verified Path (High Confidence):**")
        for step in state.verified_path:
            s_name = node_map.get(step['source'], step['source'])
            t_name = node_map.get(step['target'], step['target'])
            rel = step.get('edge_text', step.get('edge', 'related_to'))
            evidence_lines.append(f"- {s_name} --[{rel}]--> {t_name}")
            
    # N·∫øu kh√¥ng, d√πng c√°c ƒë∆∞·ªùng ƒëi ti·ªÅm nƒÉng (Step 6)
    elif state.candidate_paths:
        evidence_lines.append("‚öñÔ∏è **Candidate Paths (Unverified):**")
        for p in state.candidate_paths[:5]: # L·∫•y top 5
            evidence_lines.append(f"- {p.get('text_repr', '')} (Score: {p.get('final_score', 0):.2f})")

    evidence_text = "\n".join(evidence_lines)
    state.gcot['compiled_cot'] = evidence_text 

    # --- 2. Fallback n·∫øu kh√¥ng c√≥ b·∫±ng ch·ª©ng ---
    if not evidence_text:
        state.final_answer = "I could not find sufficient evidence in the Knowledge Graph to answer this question."
        return state

    # --- 3. G·ªçi LLM ƒë·ªÉ t·ªïng h·ª£p c√¢u tr·∫£ l·ªùi ---
    if not GOOGLE_API_KEY or not genai:
        # Ch·∫ø ƒë·ªô Offline
        state.final_answer = f"**Answer generated based on Graph Evidence:**\n\n{evidence_text}"
        return state

    try:
        client = genai.Client(api_key=GOOGLE_API_KEY)
        prompt = f"""
        You are a medical assistant. Answer the question based ONLY on the provided evidence.
        If the evidence suggests a contraindication (bad interaction), warn the user.
        If the evidence shows a treatment, list it.

        Question: {state.normalized_query}
        
        Evidence from Knowledge Graph:
        {evidence_text}
        
        Answer:
        """
        # ƒê·ªïi model sang 'gemini-1.5-flash' (·ªïn ƒë·ªãnh h∆°n b·∫£n exp)
        res = client.models.generate_content(model="gemini-1.5-flash", contents=prompt)
        state.final_answer = res.text.strip()
        state.log("8_SYNTHESIS", "SUCCESS")
        
    except Exception as e:
        # --- FIX: X·ª≠ l√Ω l·ªói quota gracefully ---
        logger.warning(f"LLM Generation failed: {e}")
        state.final_answer = (
            "‚ö†Ô∏è **Note:** The AI summary could not be generated due to API limits.\n"
            "**Here is the raw evidence found in the Knowledge Graph:**\n\n"
            f"{evidence_text}"
        )
        state.log("8_SYNTHESIS", "PARTIAL_SUCCESS", {"error": str(e)})
        
    return state

===== .\src\modules\step9_safety.py =====
# src/modules/step9_safety.py
import logging
import itertools
from src.core.state import MedCOTState
from src.utils.neo4j_connect import db_connector

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("step9_safety")

def run(state: MedCOTState) -> MedCOTState:
    if not db_connector: return state

    # --- FIX ISSUE 8: Use Linked Entities for Robustness ---
    drugs = set()
    diseases = set()
    
    for le in state.linked_entities:
        if le.link_status == "linked":
            # Map KG Type v·ªÅ internal label (Drug, Disease)
            # Gi·∫£ s·ª≠ node_label trong DB l√† chu·∫©n (Drug, Disease)
            lbl = le.best_candidate.node_label.lower()
            name = le.best_candidate.preferred_name
            
            if "drug" in lbl:
                drugs.add(name)
            elif "disease" in lbl or "phenotype" in lbl:
                diseases.add(name)
    
    drugs = list(drugs)
    diseases = list(diseases)
    all_alerts = []

    # 1. DDI Check
    for d1, d2 in itertools.combinations(drugs, 2):
        q = """
        MATCH (a:Drug), (b:Drug)
        WHERE toLower(a.name) = toLower($d1) AND toLower(b.name) = toLower($d2)
        MATCH (a)-[r]-(b) 
        WHERE type(r) CONTAINS 'INTERACT' 
        RETURN type(r) as rel LIMIT 1
        """
        res = db_connector.run_query(q, {"d1": d1, "d2": d2})
        if res: all_alerts.append(f"üíä DDI: {d1} <-> {d2}")

    # 2. Contraindication Check
    for d in drugs:
        for dis in diseases:
            q = """
            MATCH (a:Drug), (b:Disease)
            WHERE toLower(a.name) = toLower($d) AND toLower(b.name) = toLower($dis)
            MATCH (a)-[r]-(b)
            WHERE type(r) CONTAINS 'CONTRA' OR type(r) CONTAINS 'RISK'
            RETURN type(r) as rel LIMIT 1
            """
            res = db_connector.run_query(q, {"d": d, "dis": dis})
            if res: all_alerts.append(f"‚õî Contraindication: {d} vs {dis}")

    if all_alerts:
        state.reasoning_mode = "Safety-Alert"
        for msg in all_alerts:
            state.safety_flags.append({"type": "CLINICAL_RISK", "msg": msg})
        state.final_answer = "**SAFETY WARNING:**\n" + "\n".join(all_alerts) + "\n\n" + (state.final_answer or "")

    state.log("9_SAFETY", "SUCCESS", {"alerts": len(all_alerts)})
    return state

===== .\src\modules\__init__.py =====


===== .\src\utils\arax_client.py =====
# src/utils/arax_client.py
import requests
import logging
import json
import time
import hashlib
from pathlib import Path

logger = logging.getLogger("ARAX_CLIENT")
ARAX_API_URL = "https://arax.ncats.io/api/arax/v1.4/query"

CACHE_DIR = Path(".cache/arax_queries")
CACHE_DIR.mkdir(parents=True, exist_ok=True)
CACHE_EXPIRATION_SECONDS = 86400 * 7 

class ARAXClient:
    def __init__(self):
        self.headers = {"accept": "application/json", "Content-Type": "application/json"}

    def _get_cache_key(self, names):
        key_str = "_".join(sorted([str(n).lower() for n in names]))
        return hashlib.md5(key_str.encode()).hexdigest()

    def query_kg2(self, entity_names: list, max_results=10):
        if not entity_names: return []
        cache_key = self._get_cache_key(entity_names)
        cache_file = CACHE_DIR / f"{cache_key}.json"

        if cache_file.exists():
            if time.time() - cache_file.stat().st_mtime < CACHE_EXPIRATION_SECONDS:
                logger.info(f"‚ö° [Cache Hit] Loading ARAX result for {len(entity_names)} entities")
                try:
                    with open(cache_file, 'r', encoding='utf-8') as f: return json.load(f)
                except Exception: pass

        all_results = []
        # Query t·ª´ng entity ƒë·ªÉ tr√°nh timeout
        for name in entity_names:
            try:
                araxi_commands = [
                    f"create message",
                    f"add_qnode(name={name}, key=n0)",
                    f"expand(edge_key=e0, target=n0)",
                    f"resultify(ignore_edge_direction=true)",
                    f"filter_results(action=limit, max_results={max_results})"
                ]
                payload = {"operations": {"actions": araxi_commands}}
                logger.info(f"üåç Querying ARAX/KG2 for: {name}...")
                response = requests.post(ARAX_API_URL, headers=self.headers, json=payload, timeout=45)
                
                if response.status_code == 200:
                    data = response.json()
                    if "message" in data and "knowledge_graph" in data["message"]:
                        kg = data["message"]["knowledge_graph"]
                        parsed_graph = self._parse_trapi_to_medcot(kg.get("nodes", {}), kg.get("edges", {}))
                        all_results.extend(parsed_graph)
            except Exception as e:
                logger.error(f"‚ùå Exception querying ARAX for {name}: {e}")
        
        try:
            with open(cache_file, 'w', encoding='utf-8') as f: json.dump(all_results, f, ensure_ascii=False)
        except Exception: pass
        return all_results

    def _parse_trapi_to_medcot(self, nodes, edges):
        medcot_edges = []
        node_map = {}
        for nid, n_info in nodes.items():
            node_map[nid] = n_info.get("name", nid)

        for eid, e_info in edges.items():
            primary_source = "unknown"
            for attr in e_info.get("attributes", []):
                if attr.get("attribute_type_id") == "biolink:primary_knowledge_source":
                    primary_source = attr.get("value")
                    break
            
            subject_id = e_info.get("subject")
            object_id = e_info.get("object")
            predicate = e_info.get("predicate", "related_to")
            if "biolink:" in predicate: predicate = predicate.split(":")[-1].upper()

            s_name = node_map.get(subject_id, subject_id)
            o_name = node_map.get(object_id, object_id)

            medcot_edges.append({
                "source": s_name, "target": o_name, "type": predicate,
                "source_id": subject_id, "target_id": object_id,
                "provenance": "RTX-KG2", "remote_source": primary_source
            })
        return medcot_edges

arax_client = ARAXClient()

===== .\src\utils\local_llm.py =====
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import logging
import gc

logging.getLogger("PyRuSH").setLevel(logging.CRITICAL)
logging.getLogger("root").setLevel(logging.ERROR)

logger = logging.getLogger("LOCAL_LLM")

# Model 1.5B t·ªëi ∆∞u cho 3050 Ti
MODEL_ID = "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"

class LocalCoTGenerator:
    _instance = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(LocalCoTGenerator, cls).__new__(cls)
            cls._instance.model = None
            cls._instance.tokenizer = None
        return cls._instance

    def load_model(self):
        """Load model v·ªõi c·∫•u h√¨nh t·ªëi ∆∞u VRAM"""
        if self.model is not None:
            return

        logger.info(f"‚è≥ Loading Local CoT Model tr√™n RTX 3050 Ti: {MODEL_ID}...")
        try:
            # C·∫•u h√¨nh 4-bit
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type='nf4'
            )

            # --- S·ª¨A L·∫†I: B·ªé force_download=True N·∫æU ƒê√É T·∫¢I XONG ---
            self.tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
            
            # --- FIX C·∫¢NH B√ÅO PAD TOKEN ---
            # Qwen/DeepSeek th∆∞·ªùng kh√¥ng c√≥ pad token m·∫∑c ƒë·ªãnh, ta g√°n n√≥ b·∫±ng EOS token
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
                self.tokenizer.pad_token_id = self.tokenizer.eos_token_id

            self.model = AutoModelForCausalLM.from_pretrained(
                MODEL_ID,
                quantization_config=bnb_config,
                device_map="auto",
                trust_remote_code=True
            )
            logger.info(f"‚úÖ Model ƒë√£ load! VRAM d·ª± ki·∫øn: ~1.5GB")
            
        except Exception as e:
            logger.critical(f"‚ùå L·ªói load model: {e}")
            raise e

    def generate_cot(self, prompt: str) -> str:
        if self.model is None:
            self.load_model()

        messages = [
            {"role": "system", "content": "You are a helpful AI assistant."},
            {"role": "user", "content": prompt}
        ]
        
        # Tokenize inputs
        input_ids = self.tokenizer.apply_chat_template(
            messages, 
            add_generation_prompt=True, 
            return_tensors="pt"
        ).to(self.model.device)

        # --- FIX C·∫¢NH B√ÅO ATTENTION MASK ---
        # T·∫°o mask: 1 cho token th·∫≠t, 0 cho padding (·ªü ƒë√¢y to√†n b·ªô l√† th·∫≠t v√¨ batch=1)
        attention_mask = torch.ones_like(input_ids)

        with torch.no_grad():
            outputs = self.model.generate(
                input_ids,
                attention_mask=attention_mask,    # Truy·ªÅn mask v√†o
                pad_token_id=self.tokenizer.pad_token_id, # Truy·ªÅn pad_token_id
                max_new_tokens=1024,
                temperature=0.6,
                do_sample=True,
                top_p=0.9
            )

        response = self.tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)
        return response.strip()

    def unload(self):
        """Gi·∫£i ph√≥ng VRAM"""
        if self.model is not None:
            del self.model
            del self.tokenizer
            self.model = None
            torch.cuda.empty_cache()
            gc.collect()
            logger.info("üóëÔ∏è ƒê√£ gi·∫£i ph√≥ng Model kh·ªèi GPU")

# Singleton
local_llm = LocalCoTGenerator()

===== .\src\utils\node_normalizer.py =====
# src/utils/node_normalizer.py
import requests
import logging
from typing import List, Dict

logger = logging.getLogger("NODE_NORMALIZER")
SRI_NN_URL = "https://nodenormalization-sri.renci.org/get_normalized_nodes"

class NodeNormalizer:
    def __init__(self):
        self.cache = {}

    def normalize(self, curies: List[str]) -> Dict[str, str]:
        """
        Input: List c√°c ID l·ªôn x·ªôn (VD: ['MONDO:0005148', 'DOID:9352'])
        Output: Map {input_id -> canonical_id} (VD: {'DOID:9352': 'MONDO:0005148'})
        """
        # 1. L·ªçc cache
        to_query = [c for c in curies if c not in self.cache]
        
        if not to_query:
            return {c: self.cache[c] for c in curies}

        # 2. G·ªçi API SRI Node Normalizer (Batch request)
        try:
            # API n√†y gi·ªõi h·∫°n curies url length, n√™n c·∫Øt batch n·∫øu c·∫ßn (·ªü ƒë√¢y l√†m ƒë∆°n gi·∫£n)
            params = {"curie": to_query, "conflate": "true"}
            response = requests.get(SRI_NN_URL, params=params, timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                for curie, info in data.items():
                    if info and "id" in info and "identifier" in info["id"]:
                        # L·∫•y ID chu·∫©n nh·∫•t (Preferred ID)
                        self.cache[curie] = info["id"]["identifier"]
                    else:
                        self.cache[curie] = curie # Fallback: Gi·ªØ nguy√™n n·∫øu kh√¥ng map ƒë∆∞·ª£c
            else:
                logger.warning(f"Node Normalizer API Error: {response.status_code}")
                
        except Exception as e:
            logger.error(f"Node Normalizer Failed: {e}")
        
        # ƒêi·ªÅn n·ªët nh·ªØng c√°i l·ªói v√†o cache ƒë·ªÉ tr√°nh query l·∫°i
        for c in to_query:
            if c not in self.cache: self.cache[c] = c
            
        return {c: self.cache[c] for c in curies}

node_normalizer = NodeNormalizer()

===== .\src\utils\__init__.py =====


===== .\tests\test_full_pipeline.py =====
# tests/test_full_pipeline.py
import logging
import sys
import time
import os

# --- CRITICAL: CONFIGURE LOGGING FIRST ---
# Ph·∫£i ƒë·∫∑t tr∆∞·ªõc t·∫•t c·∫£ c√°c import kh√°c ƒë·ªÉ c√≥ hi·ªáu l·ª±c
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)],
    force=True  # Ghi ƒë√® m·ªçi c·∫•u h√¨nh logging ƒë√£ c√≥
)

# T·∫Øt ti·∫øng ·ªìn t·ª´ c√°c th∆∞ vi·ªán c·ª• th·ªÉ b·∫±ng c√°ch set level CRITICAL
for lib in ["PyRuSH", "presidio-analyzer", "medspacy", "urllib3", "sentence_transformers", "httpx", "httpcore", "hpack", "google.ai"]:
    logging.getLogger(lib).setLevel(logging.CRITICAL)
# ------------------------------------------

from src.core.state import MedCOTState
from src.modules import (
    step0_preprocess, step1_extraction, step2_linking,
    step4_retrieval, step5_reasoning, step6_path_generation,
    step7_verification, step8_synthesis, step9_safety, step10_logging
)
from src.utils.neo4j_connect import db_connector

logger = logging.getLogger("TEST_PIPELINE")

def print_section(title):
    print(f"\n{'='*60}\nüöÄ {title}\n{'='*60}")

def inspect_advanced_features(state: MedCOTState):
    print(f"\nüîç --- KI·ªÇM TRA T√çNH NƒÇNG N√ÇNG CAO ---")
    linked = [e for e in state.linked_entities if e.link_status == "linked"]
    if linked: 
        names = [c.best_candidate.preferred_name for c in linked[:3]]
        print(f"‚úÖ [Step 2] Linked: {names}")
    
    graph = state.graph_refs.get("ckg_subgraph", {})
    nodes = graph.get("nodes", [])
    edges = graph.get("edges", [])
    print(f"‚úÖ [Step 4] Subgraph: {len(nodes)} nodes, {len(edges)} edges")
    
    print(f"‚úÖ [Step 6] Candidate Paths Found: {len(state.candidate_paths)}")
    
    if state.verified_path:
        print(f"‚úÖ [Step 7] Verification Confidence: {state.global_confidence:.4f}")
    else:
        print("‚ö†Ô∏è [Step 7] No path passed verification threshold.")
        
    print(f"‚úÖ [Step 9] Safety Flags Triggered: {len(state.safety_flags)}")

def run_test_case(query, context=None):
    if db_connector is None: 
        logger.critical("Database connector not available. Aborting.")
        return
        
    start_t = time.time()
    state = MedCOTState(raw_query=query, patient_context=context)

    try:
        state = step0_preprocess.run(state)
        state = step1_extraction.run(state)
        state = step2_linking.run(state)
        state = step4_retrieval.run(state)
        state = step5_reasoning.run(state)
        state = step6_path_generation.run(state)
        state = step7_verification.run(state)
        state = step8_synthesis.run(state)
        state = step9_safety.run(state)
        state = step10_logging.run(state)
        
        print_section("K·∫æT QU·∫¢ CU·ªêI C√ôNG")
        print(f"‚ùì Query: {query}")
        print(f"\nüí° ANSWER:\n{state.final_answer}\n")
        
        inspect_advanced_features(state)
        
    except Exception as e:
        logger.exception(f"Pipeline crashed: {e}")
    
    print(f"\n‚è±Ô∏è Total Time: {time.time() - start_t:.2f}s")

if __name__ == "__main__":
    print_section("TEST CASE 1: SIMPLE QUERY")
    run_test_case("What are the treatments for hypertension?")

    print_section("TEST CASE 2: SAFETY & DDI CHECK")
    run_test_case("Is it safe to take Warfarin and Aspirin together?")



===== .\tests\test_step_0_preprocess.py =====
# tests/test_step_0_preprocess.py
from src.core.state import MedCOTState
from src.modules import step0_preprocess
from pprint import pprint

def main():
    print("="*50)
    print("üß™ B·∫ÆT ƒê·∫¶U TEST B∆Ø·ªöC 0: PREPROCESSING")
    print("="*50)

    test_query = "   B·ªánh nh√¢n John Doe, 50 tu·ªïi, c√≥ ti·ªÅn s·ª≠ ƒêTƒê type 2.   \n\n C·∫ßn t∆∞ v·∫•n th√™m.  "
    state = MedCOTState(raw_query=test_query)

    print(f"üîπ Query g·ªëc:\n'{state.raw_query}'")

    # Ch·∫°y b∆∞·ªõc 0
    state = step0_preprocess.run(state, enable_phi_redaction=True)

    print("\n‚úÖ K·∫æT QU·∫¢:")
    print(f"üî∏ Query ƒë√£ chu·∫©n h√≥a (·∫©n PHI):\n'{state.normalized_query}'")
    print("üî∏ C√°c c√¢u ƒë√£ t√°ch:")
    pprint(state.sentences)
    
    assert state.normalized_query == "B·ªánh nh√¢n <PERSON>, 50 tu·ªïi, c√≥ ti·ªÅn s·ª≠ ƒêTƒê type 2. \n\nC·∫ßn t∆∞ v·∫•n th√™m."
    assert len(state.sentences) > 1

    print("\nüéâ TEST B∆Ø·ªöC 0 TH√ÄNH C√îNG!")

if __name__ == "__main__":
    main()



===== .\tests\test_step_10_logging.py =====
# tests/test_step_10_logging.py
import os
from pathlib import Path
from src.core.state import MedCOTState
from src.modules import step10_logging

def main():
    print("="*50)
    print("üß™ B·∫ÆT ƒê·∫¶U TEST B∆Ø·ªöC 10: PROVENANCE LOGGING")
    print("="*50)
    
    state = MedCOTState(raw_query="final test")
    state.final_answer = "This is the final answer."
    state.global_confidence = 0.95

    print(f"üîπ Test v·ªõi query_id: {state.query_id}")

    # Ch·∫°y b∆∞·ªõc 10
    state = step10_logging.run(state)
    
    log_file_path = Path("output/audit_logs") / f"{state.query_id}.json"

    print("\n‚úÖ K·∫æT QU·∫¢:")
    print(f"üî∏ File log d·ª± ki·∫øn ƒë∆∞·ª£c t·∫°o t·∫°i: {log_file_path}")
    
    assert log_file_path.exists(), f"File log {log_file_path} kh√¥ng ƒë∆∞·ª£c t·∫°o!"
    
    # D·ªçn d·∫πp file test
    os.remove(log_file_path)
    print("üî∏ File log test ƒë√£ ƒë∆∞·ª£c x√≥a.")

    print("\nüéâ TEST B∆Ø·ªöC 10 TH√ÄNH C√îNG!")

if __name__ == "__main__":
    main()



===== .\tests\test_step_1_extraction.py =====
# tests/test_step_1_extraction.py
from src.core.state import MedCOTState
from src.modules import step1_extraction
from pprint import pprint

def main():
    print("="*50)
    print("üß™ B·∫ÆT ƒê·∫¶U TEST B∆Ø·ªöC 1: HYBRID EXTRACTION")
    print("="*50)

    # S·ª≠ d·ª•ng query ti·∫øng Anh ƒë·ªÉ ƒë·∫£m b·∫£o model ho·∫°t ƒë·ªông t·ªët nh·∫•t
    test_query = "The patient does not have fever, but has a history of hypertension and is taking metformin."
    state = MedCOTState(raw_query=test_query, normalized_query=test_query)

    print(f"üîπ Text ƒë·∫ßu v√†o:\n'{state.normalized_query}'")

    # Ch·∫°y b∆∞·ªõc 1
    state = step1_extraction.run(state)

    print("\n‚úÖ K·∫æT QU·∫¢:")
    print(f"üî∏ S·ªë th·ª±c th·ªÉ t√¨m th·∫•y: {len(state.mentions)}")
    for mention in state.mentions:
        print(f"  - Text: '{mention.text}', Label: {mention.label}, Attrs: {mention.attributes}")

    assert len(state.mentions) >= 2
    
    # Ki·ªÉm tra medspacy context (negation)
    fever_mention = next((m for m in state.mentions if "fever" in m.text.lower()), None)
    
    if fever_mention:
        # Code step 1 g√°n attrs['negated'] = True (thay v√¨ negated_existence)
        is_neg = fever_mention.attributes.get('negated')
        print(f"  > 'fever' attributes: {fever_mention.attributes}")
        assert is_neg is True, "Fever should be negated"

    print("\nüéâ TEST B∆Ø·ªöC 1 TH√ÄNH C√îNG!")

if __name__ == "__main__":
    main()



===== .\tests\test_step_2_linking.py =====
# tests/test_step_2_linking.py
from src.core.state import MedCOTState
from src.modules import step0_preprocess, step1_extraction, step2_linking
from src.utils.neo4j_connect import db_connector

def main():
    print("="*50)
    print("üß™ B·∫ÆT ƒê·∫¶U TEST B∆Ø·ªöC 2: ENTITY LINKING")
    print("="*50)

    if db_connector is None:
        print("‚ùå K·∫øt n·ªëi Neo4j th·∫•t b·∫°i. D·ª´ng test.")
        return

    # D√πng test case ti·∫øng Anh ƒë·ªÉ ƒë·∫£m b·∫£o c√≥ trong CKG dump
    test_query = "A patient with hypertension was treated with lisinopril."
    state = MedCOTState(raw_query=test_query)

    print(f"üîπ Query: '{test_query}'")

    # Ch·∫°y c√°c b∆∞·ªõc ph·ª• thu·ªôc
    state = step0_preprocess.run(state, enable_phi_redaction=False)
    state = step1_extraction.run(state)
    
    print(f"üî∏ ƒê√£ tr√≠ch xu·∫•t {len(state.mentions)} mentions.")

    # Ch·∫°y b∆∞·ªõc 2
    state = step2_linking.run(state)

    print("\n‚úÖ K·∫æT QU·∫¢:")
    for le in state.linked_entities:
        mention = le.source_mention
        if le.link_status == 'linked':
            best = le.best_candidate
            print(f"  [LINKED]   '{mention.text}' ({mention.kg_type}) -> {best.node_id} ('{best.preferred_name}')")
        else:
            print(f"  [UNLINKED] '{mention.text}' ({mention.kg_type})")

    print("\nüî∏ Seed Nodes cu·ªëi c√πng:")
    print(state.seed_nodes)

    assert len(state.seed_nodes) > 0, "Ph·∫£i link ƒë∆∞·ª£c √≠t nh·∫•t 1 node"
    
    if db_connector:
        db_connector.close()
    print("\nüéâ TEST B∆Ø·ªöC 2 TH√ÄNH C√îNG!")

if __name__ == "__main__":
    main()



===== .\tests\test_step_4_retrieval.py =====
# tests/test_step_4_retrieval.py
import json
from src.core.state import MedCOTState
from src.modules import step0_preprocess, step1_extraction, step2_linking, step4_retrieval
from src.utils.neo4j_connect import db_connector

def main():
    print("="*50)
    print("üß™ B·∫ÆT ƒê·∫¶U TEST B∆Ø·ªöC 4: SUBGRAPH RETRIEVAL")
    print("="*50)

    if db_connector is None:
        print("‚ùå K·∫øt n·ªëi Neo4j th·∫•t b·∫°i. D·ª´ng test.")
        return
        
    print("‚ÑπÔ∏è L∆ØU √ù: Test n√†y y√™u c·∫ßu b·∫°n ph·∫£i ch·∫°y 'python run/build_faiss_index.py' tr∆∞·ªõc.")

    test_query = "What are the treatments for hypertension?"
    state = MedCOTState(raw_query=test_query)

    print(f"üîπ Query: '{test_query}'")

    # Ch·∫°y c√°c b∆∞·ªõc ph·ª• thu·ªôc
    state = step0_preprocess.run(state)
    state = step1_extraction.run(state)
    state = step2_linking.run(state)
    
    if not state.seed_nodes:
        print("‚ùå Kh√¥ng t√¨m th·∫•y seed_nodes. D·ª´ng test.")
        return
    print(f"üî∏ Seed nodes t√¨m th·∫•y: {state.seed_nodes}")

    # Ch·∫°y b∆∞·ªõc 4
    state = step4_retrieval.run(state, top_k_nodes=100)

    subgraph = state.graph_refs.get("ckg_subgraph", {})
    nodes = subgraph.get('nodes', [])
    edges = subgraph.get('edges', [])

    print("\n‚úÖ K·∫æT QU·∫¢:")
    print(f"üî∏ Subgraph retrieved: {len(nodes)} nodes, {len(edges)} edges.")
    
    assert len(nodes) > 0, "Subgraph ph·∫£i c√≥ node"
    # assert len(edges) > 0, "Subgraph n√™n c√≥ c·∫°nh ƒë·ªÉ c√≥ √Ω nghƒ©a" # C√≥ th·ªÉ kh√¥ng c√≥ c·∫°nh n·∫øu c√°c node kh√¥ng li√™n quan tr·ª±c ti·∫øp

    if db_connector:
        db_connector.close()
        
    if len(nodes) > 0:
        print("\nüéâ TEST B∆Ø·ªöC 4 TH√ÄNH C√îNG!")
    else:
        print("\n‚ö†Ô∏è TEST B∆Ø·ªöC 4 HO√ÄN T·∫§T NH∆ØNG KH√îNG L·∫§Y ƒê∆Ø·ª¢C NODE N√ÄO. H√ÉY KI·ªÇM TRA L·∫†I INDEX V√Ä LOGIC.")


if __name__ == "__main__":
    main()



===== .\tests\test_step_5_reasoning.py =====
# tests/test_step_5_reasoning.py
import numpy as np
from src.core.state import MedCOTState
from src.modules import step0_preprocess, step1_extraction, step2_linking, step4_retrieval, step5_reasoning
from src.utils.neo4j_connect import db_connector

def main():
    print("="*50)
    print("üß™ B·∫ÆT ƒê·∫¶U TEST B∆Ø·ªöC 5: GCoT REASONING")
    print("="*50)

    if db_connector is None:
        print("‚ùå K·∫øt n·ªëi Neo4j th·∫•t b·∫°i. D·ª´ng test.")
        return
        
    test_query = "What are the treatments for pterygium?"
    state = MedCOTState(raw_query=test_query)

    state = step0_preprocess.run(state)
    state = step1_extraction.run(state)
    state = step2_linking.run(state)
    state = step4_retrieval.run(state, top_k_nodes=50)

    if not state.graph_refs.get("ckg_subgraph", {}).get("nodes"):
        print("‚ùå Subgraph r·ªóng. Kh√¥ng th·ªÉ ch·∫°y reasoning. D·ª´ng test.")
        return
        
    # Ch·∫°y b∆∞·ªõc 5 v·ªõi 2 b∆∞·ªõc suy lu·∫≠n
    state = step5_reasoning.run(state, num_think_steps=2)

    print("\n‚úÖ K·∫æT QU·∫¢:")
    thought_vectors = state.gcot.get('thought_vectors', [])
    print(f"üî∏ S·ªë l∆∞·ª£ng thought vectors ƒë√£ sinh: {len(thought_vectors)}")
    if thought_vectors:
        print(f"üî∏ Shape c·ªßa thought vector ƒë·∫ßu ti√™n: {np.array(thought_vectors[0]).shape}")

    final_embeddings = state.graph_refs.get('final_node_embeddings', {})
    print(f"üî∏ S·ªë lo·∫°i node c√≥ embedding cu·ªëi c√πng: {len(final_embeddings)}")

    # Test n√†y gi·ªù s·∫Ω PASS v√¨ code Step 5 ƒë√£ c√≥ v√≤ng l·∫∑p
    assert len(thought_vectors) == 2, "Ph·∫£i sinh ƒë·ªß s·ªë thought vectors"
    assert len(final_embeddings) > 0, "Ph·∫£i c√≥ final node embeddings"
    
    if db_connector:
        db_connector.close()

    print("\nüéâ TEST B∆Ø·ªöC 5 TH√ÄNH C√îNG!")

if __name__ == "__main__":
    main()



===== .\tests\test_step_6_path_generation.py =====
# tests/test_step_6_path_generation.py
from src.core.state import MedCOTState
from src.modules import (step0_preprocess, step1_extraction, step2_linking, 
                         step4_retrieval, step5_reasoning, step6_path_generation)
from src.utils.neo4j_connect import db_connector

def main():
    print("="*50)
    print("üß™ B·∫ÆT ƒê·∫¶U TEST B∆Ø·ªöC 6: PATH GENERATION")
    print("="*50)

    if db_connector is None:
        print("‚ùå K·∫øt n·ªëi Neo4j th·∫•t b·∫°i. D·ª´ng test.")
        return

    test_query = "What are the treatments for pterygium?"
    state = MedCOTState(raw_query=test_query)

    # Ch·∫°y c√°c b∆∞·ªõc ph·ª• thu·ªôc
    state = step0_preprocess.run(state)
    state = step1_extraction.run(state)
    state = step2_linking.run(state)
    state = step4_retrieval.run(state, top_k_nodes=50)
    state = step5_reasoning.run(state)

    if not state.graph_refs.get("final_node_embeddings"):
        print("‚ùå Kh√¥ng c√≥ embeddings. D·ª´ng test.")
        return

    # Ch·∫°y b∆∞·ªõc 6
    state = step6_path_generation.run(state, beam_width=3, max_path_length=3)

    print("\n‚úÖ K·∫æT QU·∫¢:")
    print(f"üî∏ S·ªë candidate paths t√¨m th·∫•y: {len(state.candidate_paths)}")

    if state.candidate_paths:
        print("üî∏ V√≠ d·ª• path ƒë·∫ßu ti√™n:")
        path_info = state.candidate_paths[0]
        path_str = " -> ".join([step['node_id'] for step in path_info['path']])
        print(f"  - Path: {path_str}")
        print(f"  - Score: {path_info['score']:.4f}")

    assert len(state.candidate_paths) >= 0 # C√≥ th·ªÉ kh√¥ng t√¨m th·∫•y path n√†o
    
    if db_connector:
        db_connector.close()
    
    print("\nüéâ TEST B∆Ø·ªöC 6 TH√ÄNH C√îNG!")

if __name__ == "__main__":
    main()



===== .\tests\test_step_7_verification.py =====
# tests/test_step_7_verification.py
from src.core.state import MedCOTState
from src.modules import (step0_preprocess, step1_extraction, step2_linking, 
                         step4_retrieval, step5_reasoning, step6_path_generation,
                         step7_verification)
from src.utils.neo4j_connect import db_connector

def main():
    print("="*50)
    print("üß™ B·∫ÆT ƒê·∫¶U TEST B∆Ø·ªöC 7: VERIFICATION")
    print("="*50)

    if db_connector is None: return

    test_query = "What are the treatments for hypertension?"
    state = MedCOTState(raw_query=test_query)

    # Ch·∫°y c√°c b∆∞·ªõc ph·ª• thu·ªôc
    state = step0_preprocess.run(state)
    state = step1_extraction.run(state)
    state = step2_linking.run(state)
    state = step4_retrieval.run(state, top_k_nodes=50)
    state = step5_reasoning.run(state)
    state = step6_path_generation.run(state)

    if not state.candidate_paths:
        print("‚ÑπÔ∏è Kh√¥ng c√≥ candidate path. D·ª´ng test.")
        return

    # Ch·∫°y b∆∞·ªõc 7
    state = step7_verification.run(state)

    print("\n‚úÖ K·∫æT QU·∫¢:")
    print(f"üî∏ Global Confidence: {state.global_confidence:.4f}")
    print(f"üî∏ Reasoning Mode: {state.reasoning_mode}")
    
    # S·ª≠a: Th√™m 'Cautious' v√† 'Safety-Alert' v√†o danh s√°ch h·ª£p l·ªá
    valid_modes = ["Graph-Strict", "Cautious", "Cautious-Generic", "Abstain", "Safety-Alert"]
    assert state.reasoning_mode in valid_modes, f"Mode {state.reasoning_mode} kh√¥ng h·ª£p l·ªá"
    
    if db_connector: db_connector.close()
    print("\nüéâ TEST B∆Ø·ªöC 7 TH√ÄNH C√îNG!")

if __name__ == "__main__":
    main()



===== .\tests\test_step_8_synthesis.py =====
# tests/test_step_8_synthesis.py
from src.core.state import MedCOTState
from src.modules import step8_synthesis

def main():
    print("="*50)
    print("üß™ B·∫ÆT ƒê·∫¶U TEST B∆Ø·ªöC 8: ANSWER SYNTHESIS")
    print("="*50)
    
    state = MedCOTState(raw_query="test", normalized_query="What causes Disease_A?")
    state.reasoning_mode = "Graph-Strict"
    state.verified_path = [
        {'source': 'Gene_X', 'edge': 'ASSOCIATED_WITH', 'target': 'Disease_A', 'step_confidence': 0.9}
    ]
    state.graph_refs["ckg_subgraph"] = {"nodes": [], "edges": []}

    state = step8_synthesis.run(state)

    print("\n‚úÖ K·∫æT QU·∫¢:")
    print(state.final_answer)
    
    # S·ª≠a: Assert l·ªèng h∆°n v√¨ format prompt thay ƒë·ªïi
    compiled = state.gcot.get('compiled_cot', '')
    assert "Verified Chain" in compiled or "Evidence" in compiled
    
    print("\nüéâ TEST B∆Ø·ªöC 8 TH√ÄNH C√îNG!")
    
if __name__ == "__main__":
    main()



===== .\tests\test_step_9_safety.py =====
# tests/test_step_9_safety.py
from src.core.state import MedCOTState, Mention
from src.modules import step9_safety

def main():
    print("="*50)
    print("üß™ B·∫ÆT ƒê·∫¶U TEST B∆Ø·ªöC 9: SAFETY ENGINE")
    print("="*50)
    
    state = MedCOTState(raw_query="test")
    state.final_answer = "Treat with Metformin and Warfarin."
    # Gi·∫£ l·∫≠p 2 thu·ªëc c√≥ t∆∞∆°ng t√°c
    state.mentions = [
        Mention(text="Metformin", label="drug", span=(0,0), score=1.0, source="dict"),
        Mention(text="Warfarin", label="drug", span=(0,0), score=1.0, source="dict"),
    ]

    state = step9_safety.run(state)

    print("\n‚úÖ K·∫æT QU·∫¢:")
    print(state.final_answer)
    print(state.safety_flags)
    
    if state.safety_flags:
        # S·ª≠a: Code safety m·ªõi g√°n type l√† 'CLINICAL_RISK'
        assert state.safety_flags[0]['type'] == 'CLINICAL_RISK'
        assert "SAFETY WARNINGS" in state.final_answer

    print("\nüéâ TEST B∆Ø·ªöC 9 TH√ÄNH C√îNG!")

if __name__ == "__main__":
    main()



===== .\tests\__init__.py =====


===== .\utils\kg_faiss_index.py =====
# T·ªáp: utils/kg_faiss_index.py
import logging
import json
from pathlib import Path
from sentence_transformers import SentenceTransformer
import faiss

# --- Configs ---
MODEL_NAME = "BAAI/bge-small-en-v1.5"
INDEX_DIR = Path("data/kg_index")
INDEX_PATH = INDEX_DIR / "kg_faiss.index"
META_PATH = INDEX_DIR / "kg_nodes_meta.json"

_resources = {}
logger = logging.getLogger("faiss_loader")

def load_resources():
    """T·∫£i FAISS index, metadata v√† sentence encoder m·ªôt l·∫ßn duy nh·∫•t."""
    global _resources
    if _resources:
        return _resources["index"], _resources["meta"], _resources["encoder"]

    logger.info("‚è≥ ƒêang t·∫£i t√†i nguy√™n FAISS...")
    if not INDEX_PATH.exists() or not META_PATH.exists():
        logger.error(f"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y file index '{INDEX_PATH}' ho·∫∑c metadata '{META_PATH}'.")
        logger.error("üëâ Vui l√≤ng ch·∫°y script 'python run/build_faiss_index.py' tr∆∞·ªõc.")
        raise FileNotFoundError("FAISS index files not found.")

    try:
        _resources["index"] = faiss.read_index(str(INDEX_PATH))
        with open(META_PATH, "r", encoding="utf-8") as f:
            _resources["meta"] = json.load(f)
        _resources["encoder"] = SentenceTransformer(MODEL_NAME)
        
        logger.info(f"‚úÖ ƒê√£ t·∫£i th√†nh c√¥ng FAISS index ({_resources['index'].ntotal} vectors) v√† encoder.")
        return _resources["index"], _resources["meta"], _resources["encoder"]
    except Exception as e:
        logger.exception("L·ªói nghi√™m tr·ªçng khi t·∫£i t√†i nguy√™n FAISS.")
        raise e



===== .\utils\neo4j_connect.py =====
# utils/neo4j_connect.py
import os
import time
from neo4j import GraphDatabase, Driver
from dotenv import load_dotenv

load_dotenv()

class Neo4jConnection:
    """
    Qu·∫£n l√Ω k·∫øt n·ªëi Neo4j v·ªõi c·∫•u h√¨nh Timeout cao h∆°n v√† Retry.
    """
    def __init__(self, uri, user, password):
        self._uri = uri
        self._user = user
        self._password = password
        self._driver: Driver = None
        self.connect()

    def connect(self):
        """Kh·ªüi t·∫°o driver v·ªõi c·∫•u h√¨nh m·∫°nh m·∫Ω h∆°n."""
        # N·∫øu driver ƒë√£ t·ªìn t·∫°i, kh√¥ng t·∫°o m·ªõi
        if self._driver is not None:
            return

        for i in range(3):
            try:
                self._driver = GraphDatabase.driver(
                    self._uri, 
                    auth=(self._user, self._password),
                    max_connection_lifetime=300,
                    keep_alive=True,
                    connection_acquisition_timeout=60,
                    connection_timeout=60
                )
                self._driver.verify_connectivity()
                print("‚úÖ K·∫øt n·ªëi Neo4j th√†nh c√¥ng!")
                return
            except Exception as e:
                print(f"‚ö†Ô∏è L·ªói k·∫øt n·ªëi l·∫ßn {i+1}: {e}. ƒêang th·ª≠ l·∫°i...")
                time.sleep(2)
        print("‚ùå Kh√¥ng th·ªÉ k·∫øt n·ªëi Neo4j sau 3 l·∫ßn th·ª≠.")

    def close(self):
        # --- FIX: Reset _driver v·ªÅ None sau khi ƒë√≥ng ---
        if self._driver is not None:
            self._driver.close()
            self._driver = None
            print("üîå K·∫øt n·ªëi Neo4j ƒë√£ ƒë√≥ng.")

    def run_query(self, query, parameters=None):
        if self._driver is None:
            self.connect()
            if self._driver is None: return []

        for attempt in range(3):
            try:
                with self._driver.session() as session:
                    result = session.run(query, parameters)
                    return list(result)
            except Exception as e:
                msg = str(e)
                if any(x in msg for x in ["ServiceUnavailable", "SessionExpired", "defunct", "Connection reset", "Closed"]):
                    print(f"‚ö†Ô∏è Connection drop detected ({msg}). Reconnecting ({attempt+1}/3)...")
                    self.close() # Reset driver
                    self.connect() # Re-init
                else:
                    print(f"‚ùå Query Error: {msg}")
                    raise e
        return []

# --- Singleton Instance ---
db_connector = None
try:
    db_uri = os.getenv("NEO4J_URI", "bolt://localhost:7687")
    db_user = os.getenv("NEO4J_USER", "neo4j")
    db_password = os.getenv("NEO4J_PASSWORD")

    if not db_password:
        print("‚ö†Ô∏è C·∫¢NH B√ÅO: Bi·∫øn m√¥i tr∆∞·ªùng NEO4J_PASSWORD ch∆∞a ƒë∆∞·ª£c thi·∫øt l·∫≠p.")
    
    db_connector = Neo4jConnection(uri=db_uri, user=db_user, password=db_password)
except Exception as e:
    print(f">> L·ªñI NGHI√äM TR·ªåNG: Kh√¥ng th·ªÉ kh·ªüi t·∫°o k·∫øt n·ªëi database. {e}")
    db_connector = None

===== .\utils\primekg_preprocessing.py =====
import pandas as pd
import os

# C·∫•u h√¨nh ƒë∆∞·ªùng d·∫´n
INPUT_FILE = "data/org/kg.csv"
OUTPUT_DIR = "data/primekg/import"

# T·∫°o th∆∞ m·ª•c output
os.makedirs(OUTPUT_DIR, exist_ok=True)

print(f"‚è≥ ƒêang ƒë·ªçc file g·ªëc: {INPUT_FILE} ...")
try:
    df = pd.read_csv(INPUT_FILE, low_memory=False)
except FileNotFoundError:
    print(f"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y file {INPUT_FILE}")
    print("üëâ H√£y ch·∫°y: wget -O data/org/kg.csv https://dataverse.harvard.edu/api/access/datafile/6180620")
    exit(1)

# --- 1. CLEANING & INSPECTION ---
print(f"üìä S·ªë d√≤ng d·ªØ li·ªáu: {len(df)}")
print(f"üîç C√°c c·ªôt trong file CSV: {list(df.columns)}")

# X√≥a kho·∫£ng tr·∫Øng th·ª´a trong t√™n c·ªôt (n·∫øu c√≥)
df.columns = df.columns.str.strip()

# Ki·ªÉm tra xem c√°c c·ªôt quan tr·ªçng c√≥ t·ªìn t·∫°i kh√¥ng
required_cols = ['x_id', 'x_type', 'x_name', 'y_id', 'y_type', 'y_name', 'relation']
missing_cols = [c for c in required_cols if c not in df.columns]
if missing_cols:
    print(f"‚ùå L·ªói: File CSV thi·∫øu c√°c c·ªôt quan tr·ªçng: {missing_cols}")
    exit(1)

# --- 2. X·ª¨ L√ù NODES (T·∫†O nodes.csv) ---
print("üî® ƒêang x·ª≠ l√Ω Nodes...")

# L·∫•y node t·ª´ ngu·ªìn (x)
nodes_x = df[['x_id', 'x_type', 'x_name', 'x_source']].rename(columns={
    'x_id': ':ID',
    'x_type': ':LABEL',
    'x_name': 'name',
    'x_source': 'source'
})

# L·∫•y node t·ª´ ƒë√≠ch (y)
nodes_y = df[['y_id', 'y_type', 'y_name', 'y_source']].rename(columns={
    'y_id': ':ID',
    'y_type': ':LABEL',
    'y_name': 'name',
    'y_source': 'source'
})

# G·ªôp v√† lo·∫°i b·ªè tr√πng l·∫∑p theo ID
all_nodes = pd.concat([nodes_x, nodes_y], ignore_index=True)
all_nodes.drop_duplicates(subset=[':ID'], inplace=True)

# Chu·∫©n h√≥a Label: Vi·∫øt hoa ch·ªØ c√°i ƒë·∫ßu ho·∫∑c to√†n b·ªô (VD: disease -> Disease)
all_nodes[':LABEL'] = all_nodes[':LABEL'].apply(lambda x: str(x).title())

# L∆∞u file nodes.csv
nodes_path = os.path.join(OUTPUT_DIR, "nodes.csv")
all_nodes.to_csv(nodes_path, index=False)
print(f"‚úÖ ƒê√£ l∆∞u {len(all_nodes)} nodes v√†o: {nodes_path}")

# --- 3. X·ª¨ L√ù EDGES (T·∫†O edges.csv) ---
print("üî® ƒêang x·ª≠ l√Ω Edges...")

# Ch·ªçn v√† ƒë·ªïi t√™n c·ªôt
edges = df.rename(columns={
    'x_id': ':START_ID',
    'y_id': ':END_ID',
    'relation': ':TYPE',
    'display_relation': 'display_relation'
})

# Ch·ªâ l·∫•y c√°c c·ªôt c·∫ßn thi·∫øt cho Neo4j
cols_to_keep = [':START_ID', ':END_ID', ':TYPE']
if 'display_relation' in df.columns:
    cols_to_keep.append('display_relation')

edges = edges[cols_to_keep]

# Chu·∫©n h√≥a Type quan h·ªá (Neo4j khuy√™n d√πng UPPER_CASE cho relationship)
edges[':TYPE'] = edges[':TYPE'].str.upper().str.replace(' ', '_')

# L∆∞u file edges.csv
edges_path = os.path.join(OUTPUT_DIR, "edges.csv")
edges.to_csv(edges_path, index=False)
print(f"‚úÖ ƒê√£ l∆∞u {len(edges)} edges v√†o: {edges_path}")

print("üéâ PREPROCESSING HO√ÄN T·∫§T!")



===== .\utils\test_connection.py =====
# test_connection.py
import pandas as pd
# M·ªõi: Import th·∫≥ng ƒë·ªëi t∆∞·ª£ng connector ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o s·∫µn
from src.utils.neo4j_connect import db_connector

def main_test():
    """
    H√†m test ch√≠nh, s·ª≠ d·ª•ng connector ƒë√£ ƒë∆∞·ª£c t√°i c·∫•u tr√∫c.
    """
    # M·ªõi: Ki·ªÉm tra xem connector c√≥ ƒë∆∞·ª£c t·∫°o th√†nh c√¥ng kh√¥ng
    if db_connector is None:
        print("‚ùå Kh√¥ng th·ªÉ ch·∫°y test v√¨ k·∫øt n·ªëi database th·∫•t b·∫°i.")
        return

    print("\n--- B·∫Øt ƒë·∫ßu ch·∫°y test query ---")
    try:
        query = """
        MATCH (d:Disease)
        RETURN d.id AS ID, d.name AS Name
        LIMIT 5
        """
        # M·ªõi: Ch·∫°y query c·ª±c k·ª≥ ƒë∆°n gi·∫£n
        data = db_connector.run_query(query)
        
        if data:
            print(f"üìä T√¨m th·∫•y d·ªØ li·ªáu m·∫´u ({len(data)} records):")
            df = pd.DataFrame(data)
            print(df)
        else:
            print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y node :Disease n√†o.")
            
    except Exception as e:
        print(f"‚ùå L·ªói khi ƒëang ch·∫°y query: {e}")
    finally:
        # M·ªõi: ƒê√≥ng k·∫øt n·ªëi (quan tr·ªçng khi ·ª©ng d·ª•ng k·∫øt th√∫c)
        if db_connector:
            db_connector.close()

if __name__ == "__main__":
    main_test()



===== .\utils\umls_normalizer.py =====
# T·ªáp: src/utils/umls_normalizer.py (PHI√äN B·∫¢N CHU·∫®N ƒê·ªÇ S·ª¨ D·ª§NG)

import logging
import sqlite3
from pathlib import Path
from tqdm import tqdm
from functools import lru_cache

# --- CONFIG ---
# C√°c b·ªô t·ª´ v·ª±ng (SAB) quan tr·ªçng v√† ƒë√°ng tin c·∫≠y nh·∫•t ƒë·ªÉ ∆∞u ti√™n
# B√¢y gi·ªù ch√∫ng ta s·∫Ω kh√¥ng d√πng b·ªô l·ªçc n√†y khi build n·ªØa ƒë·ªÉ ƒë·∫£m b·∫£o ƒë·ªß d·ªØ li·ªáu
# TARGET_SABS = {"SNOMEDCT_US", "RXNORM", "MSH", "NCI", "HGNC", "GO", "MDR"} 
SAB_RANKING = {
    "RXNORM": 1,
    "SNOMEDCT_US": 2,
    "NCI": 3,
    "MSH": 4,
    "HGNC": 5,
    "GO": 6,
    "MDR": 7
}

logger = logging.getLogger("UMLS_NORMALIZER")

class UMLSNormalizer:
    _instance = None
    
    def __new__(cls, db_path="data/umls/umls_lookup.db"):
        if cls._instance is None:
            cls._instance = super(UMLSNormalizer, cls).__new__(cls)
            cls._instance.db_path = Path(db_path)
            cls._instance.conn = None
        return cls._instance

    def connect(self):
        """K·∫øt n·ªëi t·ªõi DB SQLite. N·∫øu DB kh√¥ng t·ªìn t·∫°i, s·∫Ω kh√¥ng l√†m g√¨ c·∫£."""
        if self.conn is None and self.db_path.exists():
            try:
                self.conn = sqlite3.connect(self.db_path, check_same_thread=False)
                self.conn.row_factory = sqlite3.Row
                logger.info(f"‚úÖ ƒê√£ k·∫øt n·ªëi t·ªõi c∆° s·ªü d·ªØ li·ªáu UMLS t·∫°i: {self.db_path}")
            except sqlite3.Error as e:
                logger.error(f"‚ùå L·ªói k·∫øt n·ªëi SQLite: {e}")
                self.conn = None

    def disconnect(self):
        if self.conn:
            self.conn.close()
            self.conn = None
            logger.info("üîå ƒê√£ ng·∫Øt k·∫øt n·ªëi c∆° s·ªü d·ªØ li·ªáu UMLS.")

    @lru_cache(maxsize=1024)
    def normalize(self, text: str, target_stys: tuple[str] = None, top_k: int = 5) -> list[dict]:
        """
        T√¨m v√† x·∫øp h·∫°ng c√°c ·ª©ng vi√™n CUI cho m·ªôt text, c√≥ th·ªÉ l·ªçc theo Semantic Type.
        """
        if not self.conn:
            return []

        text_lower = text.lower()
        
        query = """
        SELECT
            a.cui, a.str, a.is_pref, a.sab, a.tty, s.sty
        FROM atoms a
        LEFT JOIN semantic_types s ON a.cui = s.cui
        WHERE a.str_lower = ?
        """
        params = [text_lower]
        
        if target_stys:
            placeholders = ','.join('?' for _ in target_stys)
            query += f" AND s.sty IN ({placeholders})"
            params.extend(target_stys)

        try:
            cursor = self.conn.cursor()
            cursor.execute(query, params)
            rows = cursor.fetchall()
        except sqlite3.Error as e:
            logger.error(f"L·ªói truy v·∫•n `normalize`: {e}")
            return []

        if not rows:
            return []

        cui_candidates = {}
        for row in rows:
            cui = row['cui']
            if cui not in cui_candidates:
                cui_candidates[cui] = {"cui": cui, "atoms": [], "stys": set()}
            cui_candidates[cui]["atoms"].append(dict(row))
            if row['sty']:
                cui_candidates[cui]["stys"].add(row['sty'])
        
        scored_results = []
        for cui, data in cui_candidates.items():
            best_atom = sorted(
                data['atoms'],
                key=lambda x: (-x['is_pref'], SAB_RANKING.get(x['sab'], 99)),
                reverse=False
            )[0]
            score = (best_atom['is_pref'] * 100) + (10 - SAB_RANKING.get(best_atom['sab'], 99))
            scored_results.append({
                "cui": cui, "pref_name": best_atom['str'],
                "stys": list(data['stys']), "sab": best_atom['sab'], "score": score
            })
            
        final_ranked_list = sorted(scored_results, key=lambda x: x['score'], reverse=True)
        return final_ranked_list[:top_k]

def build_umls_db_from_rrf(mrconso_path: Path, mrsty_path: Path, output_db_path: Path):
    """
    ƒê·ªçc c√°c file .RRF g·ªëc v√† x√¢y d·ª±ng m·ªôt DB SQLite ƒë·ªÉ tra c·ª©u hi·ªáu qu·∫£.
    Ch·ªâ ch·∫°y m·ªôt l·∫ßn duy nh·∫•t n·∫øu file DB ch∆∞a t·ªìn t·∫°i.
    """
    if output_db_path.exists():
        logger.info(f"‚è© File DB '{output_db_path}' ƒë√£ t·ªìn t·∫°i. B·ªè qua b∆∞·ªõc build.")
        return

    logger.info(f"üöß B·∫Øt ƒë·∫ßu x√¢y d·ª±ng c∆° s·ªü d·ªØ li·ªáu UMLS t·ª´ file RRF. Vi·ªác n√†y s·∫Ω m·∫•t nhi·ªÅu th·ªùi gian...")
    output_db_path.parent.mkdir(parents=True, exist_ok=True)
    
    conn = sqlite3.connect(output_db_path)
    cursor = conn.cursor()

    cursor.execute('''
        CREATE TABLE atoms (
            cui TEXT NOT NULL, str TEXT NOT NULL, str_lower TEXT NOT NULL,
            is_pref INTEGER NOT NULL, sab TEXT NOT NULL, tty TEXT
        )
    ''')
    cursor.execute('''
        CREATE TABLE semantic_types (
            cui TEXT NOT NULL, tui TEXT NOT NULL, sty TEXT NOT NULL
        )
    ''')

    logger.info("... ƒêang x·ª≠ l√Ω MRCONSO.RRF ...")
    with open(mrconso_path, 'r', encoding='utf-8') as f:
        batch = []
        batch_size = 100000
        for line in tqdm(f, desc="MRCONSO"):
            fields = line.strip().split('|')
            if len(fields) > 13 and fields[1] == 'ENG': # Ch·ªâ l·∫•y c√°c thu·∫≠t ng·ªØ ti·∫øng Anh
                cui, is_pref, sab, tty, str_val = fields[0], fields[4], fields[10], fields[11], fields[13]
                is_pref_int = 1 if is_pref == 'Y' else 0
                batch.append((cui, str_val, str_val.lower(), is_pref_int, sab, tty))
            if len(batch) >= batch_size:
                cursor.executemany("INSERT INTO atoms VALUES (?, ?, ?, ?, ?, ?)", batch)
                conn.commit()
                batch = []
        if batch:
            cursor.executemany("INSERT INTO atoms VALUES (?, ?, ?, ?, ?, ?)", batch)
            conn.commit()


    logger.info("... ƒêang x·ª≠ l√Ω MRSTY.RRF ...")
    with open(mrsty_path, 'r', encoding='utf-8') as f:
        batch = []
        batch_size = 100000
        for line in tqdm(f, desc="MRSTY"):
            fields = line.strip().split('|')
            if len(fields) > 3:
                cui, tui, sty = fields[0], fields[1], fields[3]
                batch.append((cui, tui, sty))
            if len(batch) >= batch_size:
                cursor.executemany("INSERT INTO semantic_types VALUES (?, ?, ?)", batch)
                conn.commit()
                batch = []
        if batch:
            cursor.executemany("INSERT INTO semantic_types VALUES (?, ?, ?)", batch)
            conn.commit()

    logger.info("... ƒêang t·∫°o index cho DB ...")
    cursor.execute("CREATE INDEX idx_atoms_str_lower ON atoms (str_lower);")
    cursor.execute("CREATE INDEX idx_sem_types_cui ON semantic_types (cui);")
    cursor.execute("CREATE INDEX idx_sem_types_sty ON semantic_types (sty);")

    conn.commit()
    conn.close()
    logger.info(f"‚úÖ X√¢y d·ª±ng DB UMLS th√†nh c√¥ng! ƒê√£ l∆∞u t·∫°i: {output_db_path}")

# Kh·ªüi t·∫°o singleton
umls_service = UMLSNormalizer()

===== .\utils\__init__.py =====


===== .\_utils\write_project_from_dump.py =====
import os
import re
from pathlib import Path

DUMP_FILE = "_utils/dump.txt"

def main():
    with open(DUMP_FILE, "r", encoding="utf-8") as f:
        content = f.read()

    # T√°ch theo pattern: ===== .\path\to\file =====
    pattern = re.compile(r"===== \.\\(.+?) =====\n", re.MULTILINE)
    matches = list(pattern.finditer(content))

    for i, match in enumerate(matches):
        rel_path = match.group(1).strip()
        start = match.end()
        end = matches[i + 1].start() if i + 1 < len(matches) else len(content)
        file_content = content[start:end].lstrip("\n")

        out_path = Path(rel_path)
        out_path.parent.mkdir(parents=True, exist_ok=True)

        with open(out_path, "w", encoding="utf-8") as f:
            f.write(file_content)

        print(f"‚úÖ Written: {out_path}")

if __name__ == "__main__":
    main()


===== DIRECTORY TREE =====
./
    .env.example
    build_umls.py
    docker-compose.yml
    main.py
    requirements.txt
    run_all.sh
    setup.py
    .cache/
        arax_queries/
            fac1f9a9c0747d6a9db0385077561ed6.json
    minimed.egg-info/
        dependency_links.txt
        PKG-INFO
        SOURCES.txt
        top_level.txt
    run/
        0_generate_rich_dataset.py
        1_train_trm_generative_enhanced.py
        2_train_lora_medcot.py
        3_train_lora_default.py
        4_evaluate_models.py
        5_ingest_custom_data.py
        build_faiss_index.py
        generate_training_data.py
        import_primekg.sh
        prepare_gnn_dataset.py
        run_primekg.sh
        train_gnn_next_hop.py
        train_verifier.py
        __init__.py
    src/
        __init__.py
        core/
            config.py
            state.py
            __init__.py
        modules/
            step0_preprocess.py
            step10_logging.py
            step1_extraction.py
            step2_linking.py
            step4_retrieval.py
            step5_reasoning.py
            step6_path_generation.py
            step7_verification.py
            step8_synthesis.py
            step9_safety.py
            __init__.py
        utils/
            arax_client.py
            local_llm.py
            node_normalizer.py
            __init__.py
    tests/
        test_full_pipeline.py
        test_step_0_preprocess.py
        test_step_10_logging.py
        test_step_1_extraction.py
        test_step_2_linking.py
        test_step_4_retrieval.py
        test_step_5_reasoning.py
        test_step_6_path_generation.py
        test_step_7_verification.py
        test_step_8_synthesis.py
        test_step_9_safety.py
        __init__.py
    utils/
        kg_faiss_index.py
        neo4j_connect.py
        primekg_preprocessing.py
        test_connection.py
        umls_normalizer.py
        __init__.py
    _utils/
        dump.txt
        write_project_from_dump.py
