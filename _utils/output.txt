
===== .\app_demo.py =====
# app_demo.py
import streamlit as st
import graphviz
from main import run_pipeline # <-- S·ª¨A ƒê·ªîI QUAN TR·ªåNG
from src.utils.neo4j_connect import db_connector

st.set_page_config(page_title="MedCOT Demo", layout="wide")
st.title("üß† MedCOT: Neuro-Symbolic Medical AI")

# Sidebar
with st.sidebar:
    st.header("Settings")
    # use_gnn v√† check_safety gi·ªù ƒë∆∞·ª£c ƒëi·ªÅu khi·ªÉn trong pipeline
    # B·∫°n c√≥ th·ªÉ th√™m c√°c config v√†o h√†m run_pipeline n·∫øu mu·ªën
    if db_connector:
        st.success("‚úÖ Neo4j Connected")
    else:
        st.error("‚ùå Neo4j Disconnected")

# Main Input
query = st.text_input("Enter Medical Question:", "Is it safe to take Warfarin and Aspirin together?")

if st.button("Run Analysis"):
    if not db_connector:
        st.error("Database connection required.")
        st.stop()
        
    with st.spinner("Running MedCOT Pipeline... This may take a moment."):
        try:
            # G·ªçi pipeline ƒë√£ ƒë∆∞·ª£c chu·∫©n h√≥a t·ª´ main.py
            state = run_pipeline(query=query)

            if state is None:
                st.error("Pipeline execution failed. Check logs for details.")
                st.stop()

            # Display Results
            col1, col2 = st.columns([1, 1])
            with col1:
                st.subheader("üí° Final Answer")
                st.markdown(state.final_answer)
                if state.safety_flags:
                    st.error(f"üö® {len(state.safety_flags)} Safety Alerts Detected!")
                    for flag in state.safety_flags:
                        st.warning(flag['msg'])
            
            with col2:
                st.subheader("üï∏ Reasoning Graph")
                if state.verified_path:
                    graph = graphviz.Digraph()
                    graph.attr(rankdir='LR')
                    
                    node_map = {n['id']: n['name'] for n in state.graph_refs.get("ckg_subgraph", {}).get("nodes", [])}

                    for step in state.verified_path:
                        s_name = node_map.get(step['source'], str(step['source']))[:25]
                        t_name = node_map.get(step['target'], str(step['target']))[:25]
                        edge_label = step.get('edge_text', step.get('edge', 'rel'))

                        graph.node(s_name, style='filled', fillcolor='lightblue')
                        graph.node(t_name, style='filled', fillcolor='lightgreen')
                        graph.edge(s_name, t_name, label=edge_label)
                    st.graphviz_chart(graph)
                else:
                    st.warning("No verified reasoning path was generated.")

        except Exception as e:
            st.error(f"An unexpected error occurred: {e}")

===== .\docker-compose.yml =====
version: "3.9"

services:
  neo4j:
    image: neo4j:5.26.18
    container_name: neo4j_primekg
    restart: always
    ports:
      - "7474:7474"
      - "7687:7687"
    environment:
      - NEO4J_AUTH=neo4j/12345678
      - NEO4J_PLUGINS=["apoc", "graph-data-science"]
      - NEO4J_apoc_import_file_enabled=true
      - NEO4J_apoc_export_file_enabled=true
      - NEO4J_dbms_security_procedures_unrestricted=apoc.*,gds.*
      - NEO4J_dbms_security_allow__csv__import__from__file__urls=true
      - NEO4J_server_directories_import=/import
      - NEO4J_server_memory_heap_initial__size=3G
      - NEO4J_server_memory_heap_max__size=3G
      - NEO4J_server_memory_pagecache_size=4G

    volumes:
      - neo4j_data:/data
      - ./logs:/logs
      - ./data/primekg/import:/import

    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider localhost:7474 || exit 1"]
      interval: 10s
      timeout: 10s
      retries: 10

volumes:
  neo4j_data:
    name: medcot_primekg_data
    external: true

===== .\main.py =====
# T·ªáp: main.py
import logging
import sys
import time
import argparse
import os
from concurrent.futures import ThreadPoolExecutor

# --- 1. ƒê·ªäNH NGHƒ®A B·ªò L·ªåC R√ÅC (Custom Filter) ---
class AntiNoiseFilter(logging.Filter):
    """B·ªô l·ªçc ƒë·∫∑c bi·ªát ƒë·ªÉ ch·∫∑n c√°c log r√°c c·ª©ng ƒë·∫ßu t·ª´ th∆∞ vi·ªán b√™n th·ª© 3."""
    def filter(self, record):
        msg = record.getMessage()
        if "eligible syntax" in msg:
            return False
        if "Loading faiss" in msg:
            return False
        return True

# --- 2. C·∫§U H√åNH LOGGING ---
NOISY_LIBS = [
    "PyRuSH", "presidio-analyzer", "medspacy", "urllib3", 
    "sentence_transformers", "httpx", "httpcore", "hpack", 
    "google.ai", "google.auth", "neo4j", "huggingface_hub", 
    "transformers", "faiss.loader", "faiss", "gliner", 
    "pdfminer", "charset_normalizer", "google_genai.models"
]
for lib in NOISY_LIBS:
    logging.getLogger(lib).setLevel(logging.CRITICAL)

os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["HUGGINGFACE_HUB_VERBOSITY"] = "error"
os.environ["TRANSFORMERS_VERBOSITY"] = "error"

logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    datefmt='%H:%M:%S', 
    handlers=[logging.StreamHandler(sys.stdout)],
    force=True
)

root_logger = logging.getLogger()
for handler in root_logger.handlers:
    handler.addFilter(AntiNoiseFilter())

# --- 3. IMPORT C√ÅC MODULE C·ª¶A PIPELINE ---
from src.core.state import MedCOTState
from src.modules import (
    step0_preprocess, step1_extraction, step2_linking, 
    step4_retrieval, step5_reasoning, step6_path_generation, 
    step7_verification, step8_synthesis, step9_safety, step10_logging
)
from src.utils.neo4j_connect import db_connector

logger = logging.getLogger("MED-COT_MAIN")

# --- 4. H√ÄM CH·∫†Y PIPELINE CH√çNH ---
def run_pipeline(query: str, patient_context: str = None, config: dict = None):
    if not db_connector:
        logger.critical("‚ùå K·∫øt n·ªëi Neo4j th·∫•t b·∫°i. D·ª´ng pipeline.")
        return None
        
    cfg = config or {}
    use_gcot = cfg.get("use_gcot", True)
    
    logger.info(f"{'='*50}\nüöÄ RUNNING PIPELINE (FINAL CLEAN)\nüöÄ QUERY: '{query}'\n{'='*50}")
    state = MedCOTState(raw_query=query, patient_context=patient_context)
    start_time = time.time()
    
    try:
        logger.info("\n--- üèÅ PHASE 1: DATA PREPARATION ---")
        state = step0_preprocess.run(state)
        state = step1_extraction.run(state)
        state = step2_linking.run(state)
        
        logger.info("\n--- ‚ö° PHASE 2: REASONING & RETRIEVAL ---")
        state = step4_retrieval.run(state)
        
        if use_gcot:
            state = step5_reasoning.run(state)
            
        state = step6_path_generation.run(state)
        state = step7_verification.run(state)
        
        # --- S·ª¨A ƒê·ªîI TH·ª® T·ª∞ TH·ª∞C THI ---
        logger.info("\n--- üî¨ PHASE 3: SYNTHESIS & SAFETY ---")
        # Ch·∫°y safety check l·∫ßn 1 ƒë·ªÉ t·∫°o `safety_flags` cho prompt c·ªßa LLM
        state = step9_safety.run(state)
        
        # T·ªïng h·ª£p c√¢u tr·∫£ l·ªùi d·ª±a tr√™n t·∫•t c·∫£ b·∫±ng ch·ª©ng, bao g·ªìm c·∫£ safety_flags
        state = step8_synthesis.run(state)
        
        # Ch·∫°y safety check l·∫ßn 2 ƒë·ªÉ ƒë·∫£m b·∫£o kh·ªëi c·∫£nh b√°o ƒë∆∞·ª£c ch√®n v√†o ƒë·∫ßu c√¢u tr·∫£ l·ªùi cu·ªëi c√πng
        state = step9_safety.run(state)
        # -------------------------------

        logger.info("\n--- üìù PHASE 4: LOGGING ---")
        step10_logging.run(state)

    except Exception as e:
        logger.exception(f"Critical pipeline error: {e}")
    finally:
        total_time = time.time() - start_time
        logger.info(f"\n{'='*50}\nüèÅ PIPELINE FINISHED IN {total_time:.2f} SECONDS\n{'='*50}")
    
    return state

# --- 5. H√ÄM HI·ªÇN TH·ªä K·∫æT QU·∫¢ ---
def inspect_and_display(state: MedCOTState):
    """In k·∫øt qu·∫£ cu·ªëi c√πng ra m√†n h√¨nh console m·ªôt c√°ch ƒë·∫πp m·∫Øt."""
    print(f"\n\033[1m\033[94m--- FINAL RESULT ---\033[0m\n")
    print(f"‚ùì Query: {state.raw_query}\n")
    print(f"üí° ANSWER:\n{state.final_answer or 'No answer generated.'}\n")
    if state.safety_flags:
        print(f"\033[91müö® Safety Flags Detected: {len(state.safety_flags)}\033[0m")
        for flag in state.safety_flags:
            print(f"  - {flag['msg']}")
    print("\n" + "="*50)

# ==============================================================================
#  6. ƒêI·ªÇM KH·ªûI CH·∫†Y CH√çNH (ENTRY POINT)
# ==============================================================================
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Run the full MedCOT Neuro-Symbolic Pipeline.")
    parser.add_argument("--query", type=str, required=True, help="The medical question to analyze.")
    parser.add_argument("--context", type=str, default=None, help="(Optional) Patient-specific context.")
    parser.add_argument("--no-gcot", action="store_true", help="(Optional) Disable the GNN reasoning step (Step 5).")
    args = parser.parse_args()
    
    final_state = run_pipeline(query=args.query, patient_context=args.context, config={"use_gcot": not args.no_gcot})
    
    if final_state:
        inspect_and_display(final_state)
        
    if db_connector:
        db_connector.close()

===== .\setup.py =====
from setuptools import setup, find_packages

setup(
    name="minimed",
    version="1.0",
    packages=find_packages(),
)

===== .\configs\dataset_config.yaml =====
# C·∫•u h√¨nh cho vi·ªác sinh dataset
input_parquet: "data/medical_o1_vi_translated_EVALUATED_GEMINI.parquet"
output_jsonl: "data/medcot_rich_training_data.jsonl"
llm_for_normalization: "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"

===== .\configs\evaluate_config.yaml =====
test_file: "data/pubmedqa/test.jsonl"
output_csv: "evaluation_results_pubmedqa.csv"
limit: 50 # Ch·∫°y th·ª≠ 50 m·∫´u (ƒë·∫∑t null ƒë·ªÉ ch·∫°y h·∫øt)
dataset_type: "pubmedqa" # Quan tr·ªçng: Ch·∫•m ƒëi·ªÉm theo Yes/No/Maybe

models_to_evaluate:
  # === Baseline: API Models (Zero-Shot) ===
  - id: "gpt-4o_zero-shot"
    type: "api"
    model_name: "gpt-4o"
    client: "openai"

  - id: "llama3-70b_zero-shot"
    type: "api"
    model_name: "llama3-70b-8192"
    client: "groq"
    
  # === Baseline: Standard RAG ===
  - id: "rag_gpt-4o"
    type: "rag"
    retriever_path: "data/kg_index" # N∆°i ch·ª©a FAISS index
    retriever_top_k: 3
    llm_config:
      type: "api"
      model_name: "gpt-4o"
      client: "openai"

  # === Our Models (Local Adapters) ===
  # L∆∞u √Ω: C√°c model n√†y ch·∫°y ·ªü ch·∫ø ƒë·ªô sinh text (Generation), kh√¥ng ch·∫°y full graph pipeline
  - id: "medcot_sft"
    type: "local"
    adapter_path: "models/sft_medcot_adapter"
    base_model: "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"

  - id: "medcot_dpo"
    type: "local"
    adapter_path: "models/dpo_medcot_adapter"
    base_model: "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"

  - id: "default_dpo"
    type: "local"
    adapter_path: "models/dpo_default_adapter"
    base_model: "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"

===== .\scripts\0_preprocess_primekg.py =====
import pandas as pd
import os

# C·∫•u h√¨nh ƒë∆∞·ªùng d·∫´n
INPUT_FILE = "data/org/kg.csv"
OUTPUT_DIR = "data/primekg/import"

# T·∫°o th∆∞ m·ª•c output
os.makedirs(OUTPUT_DIR, exist_ok=True)

print(f"‚è≥ ƒêang ƒë·ªçc file g·ªëc: {INPUT_FILE} ...")
try:
    df = pd.read_csv(INPUT_FILE, low_memory=False)
except FileNotFoundError:
    print(f"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y file {INPUT_FILE}")
    print("üëâ H√£y ch·∫°y: wget -O data/org/kg.csv https://dataverse.harvard.edu/api/access/datafile/6180620")
    exit(1)

# --- 1. CLEANING & INSPECTION ---
print(f"üìä S·ªë d√≤ng d·ªØ li·ªáu: {len(df)}")
print(f"üîç C√°c c·ªôt trong file CSV: {list(df.columns)}")

df.columns = df.columns.str.strip()
required_cols = ['x_id', 'x_type', 'x_name', 'y_id', 'y_type', 'y_name', 'relation']
missing_cols = [c for c in required_cols if c not in df.columns]
if missing_cols:
    print(f"‚ùå L·ªói: File CSV thi·∫øu c√°c c·ªôt quan tr·ªçng: {missing_cols}")
    exit(1)

# --- 2. X·ª¨ L√ù NODES (T·∫†O nodes.csv) ---
print("üî® ƒêang x·ª≠ l√Ω Nodes...")
nodes_x = df[['x_id', 'x_type', 'x_name', 'x_source']].rename(columns={
    'x_id': ':ID', 'x_type': ':LABEL', 'x_name': 'name', 'x_source': 'source'
})
nodes_y = df[['y_id', 'y_type', 'y_name', 'y_source']].rename(columns={
    'y_id': ':ID', 'y_type': ':LABEL', 'y_name': 'name', 'y_source': 'source'
})
all_nodes = pd.concat([nodes_x, nodes_y], ignore_index=True)
all_nodes.drop_duplicates(subset=[':ID'], inplace=True)
all_nodes[':LABEL'] = all_nodes[':LABEL'].apply(lambda x: str(x).title())
nodes_path = os.path.join(OUTPUT_DIR, "nodes.csv")
all_nodes.to_csv(nodes_path, index=False)
print(f"‚úÖ ƒê√£ l∆∞u {len(all_nodes)} nodes v√†o: {nodes_path}")


# --- 3. X·ª¨ L√ù EDGES (PHI√äN B·∫¢N ƒê·∫¶Y ƒê·ª¶) ---
print("üî® ƒêang x·ª≠ l√Ω Edges (Full Properties)...")

# Chu·∫©n b·ªã c·ªôt pubmed_id: Neo4j-admin c·∫ßn bi·∫øt ki·ªÉu d·ªØ li·ªáu l√† m·∫£ng
# Ta thay th·∫ø d·∫•u ph·∫©y c√°ch b·∫±ng d·∫•u ch·∫•m ph·∫©y ƒë·ªÉ neo4j-admin t·ª± t√°ch m·∫£ng
if 'pubmed_id' in df.columns:
    df['pubmed_id'] = df['pubmed_id'].astype(str).str.replace(',', ';')

# ƒê·ªïi t√™n c·ªôt, th√™m c√°c c·ªôt b·∫±ng ch·ª©ng khoa h·ªçc
edges = df.rename(columns={
    'x_id': ':START_ID',
    'y_id': ':END_ID',
    'relation': ':TYPE',
    'display_relation': 'display_relation',
    # === N√ÇNG C·∫§P ===
    'pubmed_id': 'pubmed_ids:string[]', # Ch·ªâ ƒë·ªãnh ƒë√¢y l√† m·∫£ng string cho neo4j-admin
    'evidence': 'evidence:string',
    'negation': 'negation:string'
})

# Ch·ªâ l·∫•y c√°c c·ªôt c·∫ßn thi·∫øt, bao g·ªìm c·∫£ c√°c c·ªôt m·ªõi
cols_to_keep = [
    ':START_ID', 
    ':END_ID', 
    ':TYPE', 
    'display_relation',
    'pubmed_ids:string[]', # T√™n c·ªôt m·ªõi
    'evidence:string',     # T√™n c·ªôt m·ªõi
    'negation:string'      # T√™n c·ªôt m·ªõi
]

# L·ªçc b·ªè c√°c c·ªôt kh√¥ng t·ªìn t·∫°i trong DataFrame ƒë·ªÉ tr√°nh l·ªói
existing_cols_to_keep = [col for col in cols_to_keep if col in edges.columns]
edges = edges[existing_cols_to_keep]

# Chu·∫©n h√≥a Type quan h·ªá
edges[':TYPE'] = edges[':TYPE'].str.upper().str.replace(' ', '_')

# L∆∞u file edges.csv
edges_path = os.path.join(OUTPUT_DIR, "edges.csv")
edges.to_csv(edges_path, index=False)
print(f"‚úÖ ƒê√£ l∆∞u {len(edges)} edges (v·ªõi ƒë·∫ßy ƒë·ªß thu·ªôc t√≠nh) v√†o: {edges_path}")

print("üéâ PREPROCESSING HO√ÄN T·∫§T!")

===== .\scripts\1_generate_dataset.py =====
import os
import json
import logging
import pandas as pd
import gc  # Garbage collection ƒë·ªÉ qu·∫£n l√Ω RAM
from tqdm import tqdm
from src.core.state import MedCOTState
from src.modules import (
    step0_preprocess, step1_extraction, step2_linking,
    step4_retrieval, step5_reasoning, step6_path_generation,
    step7_verification
)
from src.utils.neo4j_connect import db_connector
# Import module Local LLM m·ªõi t·∫°o
from src.utils.local_llm import local_llm

# --- CONFIG ---
logging.basicConfig(level=logging.ERROR)
INPUT_PARQUET = "data/medical_o1_vi_translated_EVALUATED_GEMINI.parquet"
OUTPUT_JSONL = "data/medcot_rich_training_data.jsonl"

def generate_raw_trace(query: str):
    """
    Ch·∫°y pipeline MedCOT (Step 0 -> Step 7) ƒë·ªÉ l·∫•y d·ªØ li·ªáu th√¥ t·ª´ Knowledge Graph.
    """
    state = MedCOTState(raw_query=query)
    try:
        # Ch·∫°y c√°c b∆∞·ªõc logic ƒë·ªì th·ªã
        state = step0_preprocess.run(state)
        state = step1_extraction.run(state)
        state = step2_linking.run(state)
        state = step4_retrieval.run(state)
        state = step5_reasoning.run(state)
        state = step6_path_generation.run(state)
        state = step7_verification.run(state)
        
        path_text = ""
        # state.gcot['verified_path_text'] ƒë∆∞·ª£c g√°n trong step7
        if state.gcot.get('verified_path_text'):
            path_text = state.gcot['verified_path_text']
        elif state.candidate_paths:
            path_text = state.candidate_paths[0].get('text_repr', "")

        raw_info = {
            "query": query,
            "seed_nodes": [e.best_candidate.preferred_name for e in state.linked_entities if e.link_status == 'linked'],
            "graph_context": state.gcot.get("graph_tokens", ""),
            "verified_path_text": path_text, # T√™n c·ªôt quan tr·ªçng
            "confidence": state.global_confidence
        }
        return raw_info
    except Exception as e:
        return None

def normalize_cot_with_llm(raw_info):
    """
    S·ª≠ d·ª•ng Local LLM ƒë·ªÉ vi·∫øt l·∫°i suy lu·∫≠n.
    """
    if not raw_info or not raw_info["verified_path_text"]:
        return "Reasoning could not be generated due to lack of graph evidence."

    prompt = f"""
    Analyze the medical Knowledge Graph path below and explain the reasoning step-by-step to answer the question.
    
    **Question:** "{raw_info['query']}"
    **Entities:** {', '.join(raw_info['seed_nodes'])}
    **Graph Path:** "{raw_info['verified_path_text']}"
    
    **Requirement:** Write a concise, logical Chain-of-Thought based on this path.
    """
    
    try:
        return local_llm.generate_cot(prompt)
    except Exception as e:
        return f"Local LLM Error: {e}"

def main():
    if db_connector is None:
        print("‚ùå Neo4j connection failed. Please check docker container.")
        return
    
    if not os.path.exists(INPUT_PARQUET):
        print(f"‚ùå Input file not found: {INPUT_PARQUET}")
        return

    print(f"üöÄ Loading data from {INPUT_PARQUET}...")
    df = pd.read_parquet(INPUT_PARQUET)
    
    # Ch·∫°y test v·ªõi 10 d√≤ng ƒë·∫ßu
    # df = df.head(10) 
    
    print("‚è≥ Warming up Local LLM (DeepSeek-R1-1.5B)...")
    try:
        local_llm.load_model()
    except Exception as e:
        print(f"‚ùå Failed to load Local LLM: {e}")
        return

    results = []
    
    print("running...")
    for idx, row in tqdm(df.iterrows(), total=len(df), desc="Generating Rich Traces"):
        raw_trace_info = generate_raw_trace(row['Question'])
        
        if raw_trace_info:
            normalized_medcot = normalize_cot_with_llm(raw_trace_info)
        else:
            normalized_medcot = "Pipeline failed to generate trace."

        # --- S·ª¨A ƒê·ªîI ·ªû ƒê√ÇY ---
        # Th√™m c·ªôt 'verified_path_text' v√†o dictionary k·∫øt qu·∫£ ƒë·ªÉ l∆∞u xu·ªëng file
        results.append({
            "question": row['Question'],
            "answer": row['Response'],
            "default_cot": row['Complex_CoT'],
            "medcot_cot": normalized_medcot,
            "verified_path_text": raw_trace_info.get("verified_path_text", "") if raw_trace_info else ""
        })
        # ---------------------
        
        if idx % 10 == 0:
            gc.collect()

    print(f"üíæ Saving {len(results)} rows to {OUTPUT_JSONL}...")
    with open(OUTPUT_JSONL, 'w', encoding='utf-8') as f:
        for item in results:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
            
    local_llm.unload()
    
    if db_connector: 
        db_connector.close()
        
    print(f"‚úÖ DONE! Rich CoT data ready at: {OUTPUT_JSONL}")

if __name__ == "__main__":
    main()

===== .\scripts\2_build_faiss.py =====
# run/build_faiss_index.py
import json
import logging
import numpy as np
import faiss
import os
from pathlib import Path
from tqdm import tqdm
from sentence_transformers import SentenceTransformer
from src.utils.neo4j_connect import db_connector

# --- CONFIG ---
MODEL_NAME = "BAAI/bge-small-en-v1.5" # Model nh·ªè, nhanh, hi·ªáu qu·∫£
OUTPUT_DIR = Path("data/kg_index")
INDEX_PATH = OUTPUT_DIR / "kg_faiss.index"
META_PATH = OUTPUT_DIR / "kg_nodes_meta.json"
BATCH_SIZE = 5000  # X·ª≠ l√Ω 5000 node m·ªói l·∫ßn ƒë·ªÉ ti·∫øt ki·ªám RAM

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger("FAISS_BUILDER")

def main():
    # 1. Ki·ªÉm tra n·∫øu Index ƒë√£ t·ªìn t·∫°i th√¨ Skip
    if INDEX_PATH.exists() and META_PATH.exists():
        print(f"\n‚è© [SKIP] FAISS Index ƒë√£ t·ªìn t·∫°i t·∫°i: {OUTPUT_DIR}")
        print("üëâ N·∫øu b·∫°n v·ª´a n·∫°p d·ªØ li·ªáu m·ªõi v√† mu·ªën build l·∫°i, h√£y x√≥a th∆∞ m·ª•c 'data/kg_index' r·ªìi ch·∫°y l·∫°i script n√†y.")
        return

    # 2. Ki·ªÉm tra k·∫øt n·ªëi DB
    if db_connector is None:
        logger.error("‚ùå Kh√¥ng c√≥ k·∫øt n·ªëi Neo4j. Vui l√≤ng ki·ªÉm tra Docker.")
        return

    # T·∫°o th∆∞ m·ª•c output
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

    # 3. ƒê·∫øm t·ªïng s·ªë node ƒë·ªÉ hi·ªÉn th·ªã thanh ti·∫øn tr√¨nh
    logger.info("üìä ƒêang ƒë·∫øm t·ªïng s·ªë node c·∫ßn index...")
    count_query = "MATCH (n) WHERE n.name IS NOT NULL RETURN count(n) as total"
    try:
        res = db_connector.run_query(count_query)
        total_nodes = res[0]['total']
        logger.info(f"   -> T·ªïng s·ªë node: {total_nodes}")
    except Exception as e:
        logger.error(f"‚ùå L·ªói khi ƒë·∫øm node: {e}")
        return

    # 4. Kh·ªüi t·∫°o Model & Index
    logger.info(f"üß† Loading SentenceTransformer: {MODEL_NAME}")
    encoder = SentenceTransformer(MODEL_NAME)
    
    # S·ª≠ d·ª•ng IndexFlatIP (Inner Product) cho cosine similarity (khi vectors ƒë√£ normalize)
    # Lo·∫°i n√†y ti·∫øt ki·ªám RAM h∆°n HNSW v√† v·∫´n ƒë·ªß nhanh cho v√†i tri·ªáu node.
    embedding_dim = 384
    index = faiss.IndexFlatIP(embedding_dim) 

    all_meta = []
    
    # 5. V√≤ng l·∫∑p Batch Processing (Ti·∫øt ki·ªám RAM)
    logger.info("üöÄ B·∫Øt ƒë·∫ßu qu√° tr√¨nh Indexing theo batch...")
    
    query = """
    MATCH (n)
    WHERE n.name IS NOT NULL
    RETURN elementId(n) AS node_id, labels(n) AS labels, n.name AS name
    ORDER BY elementId(n)
    SKIP $skip LIMIT $limit
    """
    
    skip = 0
    pbar = tqdm(total=total_nodes, desc="Indexing Nodes", unit="node")

    while skip < total_nodes:
        # A. Fetch Batch t·ª´ Neo4j
        rows = db_connector.run_query(query, {"skip": skip, "limit": BATCH_SIZE})
        if not rows:
            break
            
        batch_meta = []
        batch_texts = []
        
        # B. Prepare Data
        for r in rows:
            # X·ª≠ l√Ω an to√†n d·ªØ li·ªáu
            lbls = r.get("labels", [])
            lbl = lbls[0] if lbls else "Unknown"
            name = r.get("name", "Unknown")
            nid = str(r.get("node_id"))
            
            # L∆∞u metadata g·ªçn nh·∫π
            meta_item = {
                "node_id": nid,
                "labels": lbls,
                "name": name
            }
            batch_meta.append(meta_item)
            
            # Text ƒë·ªÉ embed: "Name (Label)"
            batch_texts.append(f"{name} ({lbl})")
        
        # C. Encode Batch (GPU/CPU)
        if batch_texts:
            embeddings = encoder.encode(
                batch_texts,
                batch_size=256,
                show_progress_bar=False,
                normalize_embeddings=True # Quan tr·ªçng cho FlatIP/Cosine
            )
            
            # D. Add to FAISS Index
            index.add(np.asarray(embeddings, dtype="float32"))
            
            # E. Append Meta
            all_meta.extend(batch_meta)
        
        skip += BATCH_SIZE
        pbar.update(len(rows))

    pbar.close()

    # 6. L∆∞u xu·ªëng ƒëƒ©a
    logger.info(f"üíæ ƒêang l∆∞u FAISS index v√†o {INDEX_PATH}...")
    faiss.write_index(index, str(INDEX_PATH))

    logger.info(f"üíæ ƒêang l∆∞u Metadata v√†o {META_PATH}...")
    with open(META_PATH, "w", encoding="utf-8") as f:
        json.dump(all_meta, f, ensure_ascii=False, indent=None) # indent=None cho file nh·ªè g·ªçn

    logger.info("üéâ Ho√†n t·∫•t build FAISS index!")
    if db_connector:
        db_connector.close()

if __name__ == "__main__":
    main()

===== .\scripts\3_prepare_gnn.py =====
import os
import torch
import pandas as pd
from tqdm import tqdm
from pathlib import Path
from src.core.state import MedCOTState
from src.modules import step0_preprocess, step1_extraction, step2_linking, step4_retrieval
from src.modules.step5_reasoning import _prepare_hetero_data_robust, load_encoder
from src.utils.neo4j_connect import db_connector

DATA_FILE = "data/medcot_rich_training_data.jsonl"
OUTPUT_DIR = Path("data/processed_gnn_data")
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

def main():
    if not db_connector: return
    encoder = load_encoder()
    
    if not os.path.exists(DATA_FILE):
        print(f"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y file d·ªØ li·ªáu ƒë·∫ßu v√†o {DATA_FILE}")
        return
        
    df = pd.read_json(DATA_FILE, lines=True)
    
    # --- S·ª¨A LOGIC L·ªåC ƒê·ªÇ CH·∫∂T CH·∫º H∆†N ---
    if 'verified_path_text' not in df.columns:
        print(f"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y c·ªôt 'verified_path_text' trong {DATA_FILE}.")
        return

    # L·ªçc c√°c d√≤ng c√≥ verified_path_text kh√¥ng r·ªóng v√† l√† m·ªôt chu·ªói
    df_filtered = df[df['verified_path_text'].apply(lambda x: isinstance(x, str) and len(x) > 5)].copy()
    
    if df_filtered.empty:
        print(f"‚ö†Ô∏è Kh√¥ng c√≥ m·∫´u h·ª£p l·ªá n√†o trong {DATA_FILE} ƒë·ªÉ t·∫°o d·ªØ li·ªáu GNN.")
        return
    # ----------------------------------------
    
    print(f"Processing {len(df_filtered)} samples for GNN dataset...")
    for idx, row in tqdm(df_filtered.iterrows(), total=len(df_filtered)):
        try:
            state = MedCOTState(raw_query=row['question'])
            state = step0_preprocess.run(state)
            state = step1_extraction.run(state)
            state = step2_linking.run(state)
            state = step4_retrieval.run(state, top_k_nodes=100)
            
            ug = state.graph_refs.get("ckg_subgraph")
            if not ug or not ug.get("nodes"): continue

            ckg_n = [n for n in ug["nodes"] if n.get("label") not in ["Patient", "Observation"]]
            psg_n = [n for n in ug["nodes"] if n.get("label") in ["Patient", "Observation"]]
            ckg_e = [e for e in ug["edges"] if e.get("provenance") != "PSG"]
            psg_e = [e for e in ug["edges"] if e.get("provenance") == "PSG"]

            ckg_d, _ = _prepare_hetero_data_robust(ckg_n, ckg_e, encoder)
            psg_d, _ = _prepare_hetero_data_robust(psg_n, psg_e, encoder)
            
            if not ckg_d.edge_types: continue

            # D√πng index g·ªëc ƒë·ªÉ ƒë·∫∑t t√™n file cho nh·∫•t qu√°n
            original_index = row.name 
            torch.save({
                "ckg_data": ckg_d,
                "psg_data": psg_d,
                "query_text": state.normalized_query,
                "target_path": row['verified_path_text']
            }, OUTPUT_DIR / f"sample_{original_index}.pt")
            
        except Exception as e:
            # print(f"Skipping sample {idx} due to error: {e}")
            pass

if __name__ == "__main__":
    main()

===== .\scripts\4_train_gnn.py =====
# run/train_gnn_next_hop.py
import logging
import torch
import torch.optim as optim
import os
import glob
from tqdm import tqdm
from sentence_transformers import SentenceTransformer
from src.models.dual_tower_gnn import CoGCoT_DualTower_GNN
import numpy as np

logging.basicConfig(level=logging.CRITICAL)

# --- CONFIG ---
DATA_DIR = "data/processed_gnn_data"
OUTPUT_PATH = "models/gnn_dual_tower_weights.pth"
EPOCHS = 10
LEARNING_RATE = 1e-4

def generate_training_samples_from_tensor(ckg_data, gold_path_text):
    # Logic t·∫°o sample t·ª´ data ƒë√£ pre-process
    # ƒê√¢y l√† logic gi·∫£ l·∫≠p: Map text path -> node index trong ckg_data
    # Th·ª±c t·∫ø c·∫ßn mapping ch√≠nh x√°c h∆°n t·ª´ ID node sang index
    # ·ªû ƒë√¢y ta d√πng random negative sampling ƒë∆°n gi·∫£n ƒë·ªÉ demo
    try:
        # L·∫•y danh s√°ch node types
        ntypes = ckg_data.node_types
        if not ntypes: return []
        
        # Ch·ªçn ƒë·∫°i di·ªán 1 node type ch√≠nh (VD: Disease)
        target_ntype = ntypes[0]
        num_nodes = ckg_data[target_ntype].x.shape[0]
        
        if num_nodes < 2: return []
        
        # Random Positive pair (Gi·∫£ l·∫≠p Next Hop)
        src_idx = np.random.randint(0, num_nodes)
        pos_idx = np.random.randint(0, num_nodes)
        
        # Negative samples
        neg_indices = np.random.choice(num_nodes, 5, replace=True).tolist()
        
        return [(target_ntype, src_idx, pos_idx, neg_indices)]
    except:
        return []

def main():
    print("üöÄ Starting Optimized GNN Training...")
    
    files = glob.glob(os.path.join(DATA_DIR, "*.pt"))
    if not files:
        print(f"‚ùå No .pt files found in {DATA_DIR}. Run prepare_gnn_dataset.py first.")
        return

    encoder = SentenceTransformer("all-MiniLM-L6-v2")
    
    # Load 1 sample ƒë·ªÉ init model
    sample_0 = torch.load(files[0])
    model = CoGCoT_DualTower_GNN(
        sample_0['ckg_data'].metadata(), 
        sample_0['psg_data'].metadata(), 
        hidden_channels=128, query_dim=384
    )
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
    loss_fn = torch.nn.BCEWithLogitsLoss()
    
    model.train()

    for epoch in range(EPOCHS):
        total_loss = 0
        valid_batches = 0
        
        # Shuffle files
        np.random.shuffle(files)
        
        pbar = tqdm(files, desc=f"Epoch {epoch+1}/{EPOCHS}")
        for file_path in pbar:
            try:
                data = torch.load(file_path)
                ckg_data = data['ckg_data']
                psg_data = data['psg_data']
                query_text = data['query_text']
                
                query_emb = encoder.encode(query_text, convert_to_tensor=True)
                
                # Forward Pass
                node_embs, _ = model(ckg_data, psg_data, query_emb)
                
                # Generate Samples & Compute Loss
                samples = generate_training_samples_from_tensor(ckg_data, data['target_path'])
                
                batch_loss = 0
                for ntype, src_idx, pos_idx, neg_idxs in samples:
                    if ntype not in node_embs: continue
                    
                    emb_matrix = node_embs[ntype]
                    src_emb = emb_matrix[src_idx]
                    pos_emb = emb_matrix[pos_idx]
                    
                    pos_score = torch.dot(src_emb, pos_emb)
                    
                    for neg_idx in neg_idxs:
                        neg_emb = emb_matrix[neg_idx]
                        neg_score = torch.dot(src_emb, neg_emb)
                        batch_loss += loss_fn(pos_score - neg_score, torch.tensor(1.0))
                
                if batch_loss != 0:
                    optimizer.zero_grad()
                    batch_loss.backward()
                    optimizer.step()
                    total_loss += batch_loss.item()
                    valid_batches += 1
                    
                pbar.set_postfix({'loss': total_loss / (valid_batches + 1e-9)})
                
            except Exception:
                continue

        print(f"Epoch {epoch+1} Avg Loss: {total_loss / (valid_batches + 1e-9):.4f}")

    torch.save(model.state_dict(), OUTPUT_PATH)
    print(f"‚úÖ GNN Model saved to {OUTPUT_PATH}")

if __name__ == "__main__":
    main()

===== .\scripts\5_train_llm.py =====
# scripts/5_train_llm.py
import yaml
import argparse
import torch
from datasets import load_dataset
from peft import LoraConfig
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from trl import SFTTrainer, DPOTrainer, SFTConfig

def get_prompt_formatter(template_type):
    """Factory ƒë·ªÉ l·∫•y h√†m format prompt d·ª±a tr√™n config."""
    if template_type == "medcot":
        return lambda ex: {"text": f"Question: {ex['question']}\nReasoning: {ex['medcot_cot']}\nAnswer: {ex['answer']}"}
    elif template_type == "default_cot":
        return lambda ex: {"text": f"Question: {ex['question']}\nReasoning: {ex['default_cot']}\nAnswer: {ex['answer']}"}
    elif template_type == "trm":
        return lambda ex: {
            "text": f"<bos><start_of_turn>user\nYou are a medical reasoning engine. Your task is to self-enhance your reasoning process before providing an answer.\n1. **Self-Enhanced Reasoning:** Based on the initial Chain-of-Thought, reformulate it into an even clearer, more structured, and logically rigorous thought process.\n2. **Final Answer:** Based on your new, enhanced reasoning, provide the final clinical answer.\n\n**Question:** {ex['question']}\n**Initial Chain-of-Thought:**\n{ex['medcot_cot']}<end_of_turn>\n<start_of_turn>model\n**Self-Enhanced Reasoning:**\n{ex['medcot_cot']}\n\n**Final Answer:**\n{ex['answer']}<end_of_turn><eos>"
        }
    else:
        raise ValueError(f"Unknown prompt template type: {template_type}")

def prepare_dpo_dataset(dataset):
    """
    Chu·∫©n b·ªã dataset cho DPO.
    - prompt: C√¢u h·ªèi
    - chosen: C√¢u tr·∫£ l·ªùi ƒë√∫ng (d·ª±a tr√™n MedCOT)
    - rejected: C√¢u tr·∫£ l·ªùi sai (d·ª±a tr√™n Default CoT)
    """
    def format_dpo(example):
        return {
            "prompt": f"Question: {example['question']}\nReasoning:",
            "chosen": f"{example['medcot_cot']}\nAnswer: {example['answer']}",
            "rejected": f"{example['default_cot']}\nAnswer: {example['answer']}" # Gi·∫£ ƒë·ªãnh c√¢u tr·∫£ l·ªùi gi·ªëng nhau, ch·ªâ CoT kh√°c nhau
        }
    return dataset.map(format_dpo)


def main(config_path: str):
    # 1. Load Config
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
    print(f"üöÄ Starting run: {config['run_name']} with mode: {config['train_mode'].upper()}")

    # 2. Load Dataset
    dataset = load_dataset("json", data_files=config['dataset_path'], split="train")

    # 3. Load Model & Tokenizer
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True, bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16, bnb_4bit_use_double_quant=True
    )
    model = AutoModelForCausalLM.from_pretrained(
        config['base_model'], quantization_config=bnb_config, device_map="auto", trust_remote_code=True
    )
    tokenizer = AutoTokenizer.from_pretrained(config['base_model'], trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    peft_config = LoraConfig(**config['peft_config'], task_type="CAUSAL_LM") if config.get('peft_config') else None

    # 4. Initialize Trainer based on mode
    trainer = None
    training_args = SFTConfig(output_dir=f"models/checkpoints/{config['run_name']}", **config['training_args'])

    if config['train_mode'] == 'sft':
        formatter = get_prompt_formatter(config['prompt_template']['type'])
        formatted_dataset = dataset.map(formatter)
        
        trainer = SFTTrainer(
            model=model,
            args=training_args,
            train_dataset=formatted_dataset,
            peft_config=peft_config,
            dataset_text_field="text",
            tokenizer=tokenizer,
        )
    elif config['train_mode'] == 'dpo':
        dpo_dataset = prepare_dpo_dataset(dataset)
        
        trainer = DPOTrainer(
            model,
            ref_model=None, # TRL s·∫Ω t·ª± t·∫°o ref_model
            args=training_args,
            beta=config['dpo_config']['beta'],
            train_dataset=dpo_dataset,
            tokenizer=tokenizer,
            peft_config=peft_config,
        )
    else:
        raise ValueError(f"Invalid train_mode: {config['train_mode']}")

    # 5. Train
    print("üî• Training is starting...")
    trainer.train()
    trainer.save_model(config['output_dir'])
    print(f"‚úÖ Training Done! Model saved to: {config['output_dir']}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--config", type=str, required=True, help="Path to the model config YAML file.")
    args = parser.parse_args()
    main(args.config)

===== .\scripts\6_evaluate_models.py =====
import yaml
import argparse
import pandas as pd
import json
import torch
import gc
import os
import time
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel
from openai import OpenAI
from pathlib import Path

# Import Retriever cho RAG
from src.utils.faiss_search import faiss_retriever

# L·∫•y API Key
OPENAI_KEY = os.getenv("OPENAI_API_KEY")
GROQ_KEY = os.getenv("GROQ_API_KEY")

# ==============================================================================
# 1. H√ÄM T·∫¢I D·ªÆ LI·ªÜU
# ==============================================================================
def load_test_data(file_path, limit=None):
    print(f"üìñ Reading {file_path}...")
    records = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            records.append(json.loads(line))
    if limit:
        return records[:limit]
    return records

# ==============================================================================
# 2. C√ÅC ENGINE INFERENCE (API, RAG, LOCAL)
# ==============================================================================

def run_api_inference(model_cfg, test_data):
    """Ch·∫°y model qua API (OpenAI ho·∫∑c Groq)."""
    print(f"\n‚òÅÔ∏è  Running API Model: {model_cfg['id']}")
    
    # C·∫•u h√¨nh Client
    if model_cfg['client'] == 'openai':
        client = OpenAI(api_key=OPENAI_KEY)
    elif model_cfg['client'] == 'groq':
        client = OpenAI(base_url="https://api.groq.com/openai/v1", api_key=GROQ_KEY)
    else:
        return ["Config Error: Unknown Client"] * len(test_data)

    outputs = []
    for item in tqdm(test_data, desc=f"Evaluating {model_cfg['id']}"):
        # Prompt PubMedQA: Context + Question -> Answer
        prompt = f"""
        Context: {item.get('Context', '')}
        Question: {item['Question']}
        
        Answer with 'yes', 'no', or 'maybe' followed by a brief explanation.
        Answer:
        """
        try:
            resp = client.chat.completions.create(
                model=model_cfg['model_name'],
                messages=[{"role": "user", "content": prompt}],
                temperature=0,
                max_tokens=256
            )
            outputs.append(resp.choices[0].message.content.strip())
        except Exception as e:
            outputs.append(f"API Error: {e}")
            time.sleep(1) # Tr√°nh rate limit
            
    return outputs

def run_rag_inference(model_cfg, test_data):
    """Ch·∫°y Standard RAG: Retrieve Graph Nodes -> GPT-4o Answer."""
    print(f"\nüìö  Running RAG Model: {model_cfg['id']}")
    
    if faiss_retriever is None:
        return ["FAISS Error: Retriever not loaded"] * len(test_data)
        
    client = OpenAI(api_key=OPENAI_KEY)
    outputs = []
    
    for item in tqdm(test_data, desc=f"Evaluating {model_cfg['id']}"):
        try:
            # 1. Retrieve Knowledge from Graph
            docs = faiss_retriever.search(item['Question'], k=model_cfg.get('retriever_top_k', 3))
            
            # Format retrieved knowledge
            kg_context = "\n".join([f"- {d['name']} ({d['labels'][0]})" for d in docs])
            
            # 2. Generate Answer (Hybrid Context: Abstract + Graph)
            prompt = f"""
            Abstract Context: {item.get('Context', '')}
            
            Knowledge Graph Info:
            {kg_context}
            
            Question: {item['Question']}
            
            Based on the Abstract and Knowledge Graph, answer with 'yes', 'no', or 'maybe'.
            Answer:
            """
            
            resp = client.chat.completions.create(
                model=model_cfg['llm_config']['model_name'],
                messages=[{"role": "user", "content": prompt}],
                temperature=0
            )
            outputs.append(resp.choices[0].message.content.strip())
            
        except Exception as e:
            outputs.append(f"RAG Error: {e}")
            
    return outputs

def run_local_inference(model_cfg, test_data):
    """Ch·∫°y Local Adapter (MedCOT SFT/DPO, Default DPO)."""
    adapter_path = model_cfg['adapter_path']
    base_model_id = model_cfg['base_model']
    print(f"\nüñ•Ô∏è  Running Local Model: {model_cfg['id']} (Adapter: {adapter_path})")

    if not os.path.exists(adapter_path):
        print(f"   ‚ö†Ô∏è Adapter path not found: {adapter_path}")
        return ["Adapter Missing"] * len(test_data)

    # Load Model (4-bit quantization ƒë·ªÉ ti·∫øt ki·ªám VRAM)
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_quant_type="nf4"
    )
    
    try:
        print("   ‚è≥ Loading Base Model...")
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_id, 
            quantization_config=bnb_config, 
            device_map="auto"
        )
        print(f"   ‚è≥ Loading Adapter from {adapter_path}...")
        model = PeftModel.from_pretrained(base_model, adapter_path)
        tokenizer = AutoTokenizer.from_pretrained(base_model_id)
        if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token
        model.eval()
    except Exception as e:
        print(f"   ‚ùå Load Error: {e}")
        return [f"Load Error: {e}"] * len(test_data)

    outputs = []
    print("   üöÄ Generating...")
    for item in tqdm(test_data, desc=f"Evaluating {model_cfg['id']}"):
        # Prompt ƒë∆°n gi·∫£n cho m√¥ h√¨nh local
        prompt = f"Context: {item.get('Context', '')}\nQuestion: {item['Question']}\nAnswer:"
        
        inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
        with torch.no_grad():
            out_ids = model.generate(
                **inputs, 
                max_new_tokens=128, 
                do_sample=False, 
                pad_token_id=tokenizer.eos_token_id
            )
        
        full_text = tokenizer.decode(out_ids[0], skip_special_tokens=True)
        # L·∫•y ph·∫ßn m·ªõi sinh ra sau prompt
        answer = full_text[len(prompt):].strip()
        outputs.append(answer)

    # D·ªçn d·∫πp VRAM sau khi ch·∫°y xong model n√†y
    del model, base_model, tokenizer
    torch.cuda.empty_cache()
    gc.collect()
    
    return outputs

# ==============================================================================
# 3. H√ÄM CH·∫§M ƒêI·ªÇM (JUDGE)
# ==============================================================================
def evaluate_pubmedqa(item, generated_answer):
    """
    S·ª≠ d·ª•ng GPT-4o ƒë·ªÉ ƒë√°nh gi√° xem c√¢u tr·∫£ l·ªùi c√≥ kh·ªõp v·ªõi nh√£n (yes/no/maybe) kh√¥ng.
    """
    client = OpenAI(api_key=OPENAI_KEY)
    correct_label = item['Correct Answer'] # yes/no/maybe
    
    # Prompt cho Th·∫©m ph√°n (Judge)
    judge_prompt = f"""
    You are a biomedical expert evaluator.
    
    Question: {item['Question']}
    Ground Truth Label: {correct_label}
    
    Model Prediction: "{generated_answer}"
    
    Task: 
    1. Determine if the Model Prediction implies 'yes', 'no', or 'maybe'.
    2. Check if it matches the Ground Truth Label.
    
    Return ONLY a JSON object: {{"predicted_label": "yes/no/maybe/unknown", "match": true/false}}
    """
    
    try:
        resp = client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": judge_prompt}],
            response_format={"type": "json_object"},
            temperature=0
        )
        res = json.loads(resp.choices[0].message.content)
        return res.get("match", False), res.get("predicted_label", "unknown")
    except Exception:
        return False, "error"

# ==============================================================================
# 4. H√ÄM MAIN
# ==============================================================================
def main(config_path):
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)

    # 1. T·∫£i D·ªØ li·ªáu
    test_data = load_test_data(config['test_file'], config['limit'])
    results_df = pd.DataFrame(test_data)
    
    # 2. Ch·∫°y Inference cho t·ª´ng Model
    for model_cfg in config['models_to_evaluate']:
        m_type = model_cfg['type']
        
        if m_type == 'api':
            preds = run_api_inference(model_cfg, test_data)
        elif m_type == 'rag':
            preds = run_rag_inference(model_cfg, test_data)
        elif m_type == 'local':
            preds = run_local_inference(model_cfg, test_data)
        else:
            preds = ["Unknown Type"] * len(test_data)
            
        results_df[f"{model_cfg['id']}_output"] = preds

    # 3. Ch·∫•m ƒëi·ªÉm (Evaluation)
    print("\nüë®‚Äç‚öñÔ∏è  Judging Results with GPT-4o...")
    scores = {}
    
    for model_cfg in config['models_to_evaluate']:
        mid = model_cfg['id']
        corrects = []
        labels = []
        
        # Duy·ªát qua k·∫øt qu·∫£ ƒë·ªÉ ch·∫•m
        for idx, row in tqdm(results_df.iterrows(), total=len(results_df), desc=f"Judging {mid}"):
            is_match, lbl = evaluate_pubmedqa(row.to_dict(), row[f"{mid}_output"])
            corrects.append(is_match)
            labels.append(lbl)
            
        results_df[f"{mid}_correct"] = corrects
        results_df[f"{mid}_pred_label"] = labels
        
        # T√≠nh Accuracy
        acc = sum(corrects) / len(corrects) * 100
        scores[mid] = acc
        print(f"   -> {mid} Accuracy: {acc:.2f}%")

    # 4. L∆∞u k·∫øt qu·∫£
    results_df.to_csv(config['output_csv'], index=False)
    print(f"\n‚úÖ Evaluation Finished! Results saved to: {config['output_csv']}")
    print("üèÜ FINAL LEADERBOARD:")
    for m, s in sorted(scores.items(), key=lambda x: x[1], reverse=True):
        print(f"  - {m}: {s:.2f}%")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--config", type=str, default="configs/evaluate_config.yaml")
    args = parser.parse_args()
    main(args.config)

===== .\scripts\build_umls_db.py =====
import sqlite3
import logging
import os
from pathlib import Path
from tqdm import tqdm

# --- C·∫§U H√åNH ---
# Script n√†y gi·ªù y√™u c·∫ßu 5 file "v√†ng" t·ª´ UMLS Metathesaurus
MRCONSO_PATH = Path("data/umls/MRCONSO.RRF")
MRSTY_PATH = Path("data/umls/MRSTY.RRF")
MRDEF_PATH = Path("data/umls/MRDEF.RRF")
MRREL_PATH = Path("data/umls/MRREL.RRF")
MRSAT_PATH = Path("data/umls/MRSAT.RRF")
OUTPUT_DB_PATH = Path("data/umls/umls_lookup.db")

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger("UMLS_BUILDER_ULTIMATE")

def build_db():
    if not OUTPUT_DB_PATH.parent.exists():
        OUTPUT_DB_PATH.parent.mkdir(parents=True, exist_ok=True)
    if OUTPUT_DB_PATH.exists():
        logger.warning(f"ƒê√£ t√¨m th·∫•y file DB c≈©. S·∫Ω x√≥a v√† x√¢y d·ª±ng l·∫°i.")
        os.remove(OUTPUT_DB_PATH)

    # Ki·ªÉm tra t·∫•t c·∫£ c√°c file ngu·ªìn
    required_files = [MRCONSO_PATH, MRSTY_PATH, MRDEF_PATH, MRREL_PATH, MRSAT_PATH]
    if not all(f.exists() for f in required_files):
        logger.error("‚ùå Kh√¥ng t√¨m th·∫•y ƒë·ªß file ngu·ªìn UMLS! C·∫ßn c√≥:")
        for f in required_files:
            logger.error(f"   - {f} {'(‚úÖ T√åM TH·∫§Y)' if f.exists() else '(‚ùå KH√îNG T√åM TH·∫§Y)'}")
        return

    logger.info(f"üöÄ B·∫Øt ƒë·∫ßu x√¢y d·ª±ng c∆° s·ªü d·ªØ li·ªáu UMLS (Ultimate Version)...")
    conn = sqlite3.connect(str(OUTPUT_DB_PATH))
    cursor = conn.cursor()

    # T·ªëi ∆∞u h√≥a t·ªëc ƒë·ªô ghi
    cursor.execute("PRAGMA synchronous = OFF")
    cursor.execute("PRAGMA journal_mode = MEMORY")

    logger.info("üì¶ ƒêang t·∫°o c·∫•u tr√∫c b·∫£ng (Schema)...")
    cursor.execute('CREATE TABLE IF NOT EXISTS atoms (cui TEXT, str TEXT, str_lower TEXT, is_pref INTEGER, sab TEXT, tty TEXT)')
    cursor.execute('CREATE TABLE IF NOT EXISTS semantic_types (cui TEXT, tui TEXT, sty TEXT)')
    cursor.execute('CREATE TABLE IF NOT EXISTS definitions (cui TEXT, definition TEXT, source TEXT)')
    cursor.execute('CREATE TABLE IF NOT EXISTS relations (cui1 TEXT, rel_type TEXT, cui2 TEXT, source TEXT)')
    cursor.execute('CREATE TABLE IF NOT EXISTS attributes (cui TEXT, attr_name TEXT, attr_value TEXT, source TEXT)')
    conn.commit()

    # --- GIAI ƒêO·∫†N 1/5: X·ª¨ L√ù MRCONSO.RRF (T·ª´ v·ª±ng) ---
    logger.info("‚è≥ GIAI ƒêO·∫†N 1/5: X·ª≠ l√Ω MRCONSO.RRF...")
    with open(MRCONSO_PATH, 'r', encoding='utf-8') as f:
        batch = []
        for line in tqdm(f, desc="Importing Concepts"):
            fields = line.strip().split('|')
            if len(fields) > 14 and fields[1] == 'ENG':
                batch.append((fields[0], fields[14], fields[14].lower(), 1 if fields[2] == 'P' else 0, fields[11], fields[12]))
            if len(batch) >= 100000:
                cursor.executemany("INSERT INTO atoms VALUES (?, ?, ?, ?, ?, ?)", batch); conn.commit(); batch = []
        if batch: cursor.executemany("INSERT INTO atoms VALUES (?, ?, ?, ?, ?, ?)", batch); conn.commit()

    # --- GIAI ƒêO·∫†N 2/5: X·ª¨ L√ù MRSTY.RRF (Lo·∫°i th·ª±c th·ªÉ) ---
    logger.info("‚è≥ GIAI ƒêO·∫†N 2/5: X·ª≠ l√Ω MRSTY.RRF...")
    with open(MRSTY_PATH, 'r', encoding='utf-8') as f:
        batch = []
        for line in tqdm(f, desc="Importing SemTypes"):
            fields = line.strip().split('|')
            if len(fields) > 3: batch.append((fields[0], fields[1], fields[3]))
            if len(batch) >= 100000:
                cursor.executemany("INSERT INTO semantic_types VALUES (?, ?, ?)", batch); conn.commit(); batch = []
        if batch: cursor.executemany("INSERT INTO semantic_types VALUES (?, ?, ?)", batch); conn.commit()

    # --- GIAI ƒêO·∫†N 3/5: X·ª¨ L√ù MRDEF.RRF (ƒê·ªãnh nghƒ©a) ---
    logger.info("‚è≥ GIAI ƒêO·∫†N 3/5: X·ª≠ l√Ω MRDEF.RRF...")
    with open(MRDEF_PATH, 'r', encoding='utf-8') as f:
        batch = []
        for line in tqdm(f, desc="Importing Definitions"):
            fields = line.strip().split('|')
            if len(fields) > 5: batch.append((fields[0], fields[5], fields[4]))
            if len(batch) >= 100000:
                cursor.executemany("INSERT INTO definitions VALUES (?, ?, ?)", batch); conn.commit(); batch = []
        if batch: cursor.executemany("INSERT INTO definitions VALUES (?, ?, ?)", batch); conn.commit()

    # --- GIAI ƒêO·∫†N 4/5: X·ª¨ L√ù MRREL.RRF (Quan h·ªá) ---
    logger.info("‚è≥ GIAI ƒêO·∫†N 4/5: X·ª≠ l√Ω MRREL.RRF...")
    with open(MRREL_PATH, 'r', encoding='utf-8') as f:
        batch = []
        for line in tqdm(f, desc="Importing Relations"):
            fields = line.strip().split('|')
            if len(fields) > 10: batch.append((fields[0], fields[7], fields[4], fields[10])) # CUI1, RELA, CUI2, SAB
            if len(batch) >= 100000:
                cursor.executemany("INSERT INTO relations VALUES (?, ?, ?, ?)", batch); conn.commit(); batch = []
        if batch: cursor.executemany("INSERT INTO relations VALUES (?, ?, ?, ?)", batch); conn.commit()
        
    # --- GIAI ƒêO·∫†N 5/5: X·ª¨ L√ù MRSAT.RRF (Thu·ªôc t√≠nh) ---
    logger.info("‚è≥ GIAI ƒêO·∫†N 5/5: X·ª≠ l√Ω MRSAT.RRF...")
    with open(MRSAT_PATH, 'r', encoding='utf-8') as f:
        batch = []
        for line in tqdm(f, desc="Importing Attributes"):
            fields = line.strip().split('|')
            if len(fields) > 10: batch.append((fields[0], fields[8], fields[10], fields[4])) # CUI, ATN, ATV, SAB
            if len(batch) >= 100000:
                cursor.executemany("INSERT INTO attributes VALUES (?, ?, ?, ?)", batch); conn.commit(); batch = []
        if batch: cursor.executemany("INSERT INTO attributes VALUES (?, ?, ?, ?)", batch); conn.commit()

    # --- T·∫†O INDEX ---
    logger.info("üî® ƒêang t·∫°o Index ƒë·ªÉ tra c·ª©u nhanh...")
    cursor.execute("CREATE INDEX IF NOT EXISTS idx_atoms_str_lower ON atoms (str_lower);")
    cursor.execute("CREATE INDEX IF NOT EXISTS idx_atoms_cui ON atoms (cui);")
    cursor.execute("CREATE INDEX IF NOT EXISTS idx_sem_types_cui ON semantic_types (cui);")
    cursor.execute("CREATE INDEX IF NOT EXISTS idx_defs_cui ON definitions (cui);")
    cursor.execute("CREATE INDEX IF NOT EXISTS idx_rels_cui1 ON relations (cui1);")
    cursor.execute("CREATE INDEX IF NOT EXISTS idx_rels_cui2 ON relations (cui2);")
    cursor.execute("CREATE INDEX IF NOT EXISTS idx_attrs_cui ON attributes (cui);")
    conn.commit()
    conn.close()

    db_size = OUTPUT_DB_PATH.stat().st_size / (1024 * 1024)
    logger.info(f"‚úÖ‚úÖ‚úÖ HO√ÄN T·∫§T! ƒê√£ t·∫°o DB UMLS ƒë·∫ßy ƒë·ªß t·∫°i: {OUTPUT_DB_PATH}")
    logger.info(f"üìä K√≠ch th∆∞·ªõc Database: {db_size:.2f} MB")

if __name__ == "__main__":
    build_db()

===== .\scripts\ingest_custom_data.py =====
# run/5_ingest_custom_data.py
import os
import json
import logging
import glob
import re
from pathlib import Path
from src.utils.neo4j_connect import db_connector
from src.utils.local_llm import local_llm

# --- C·∫§U H√åNH ---
DATA_DIR = Path("data/custom_knowledge")
DATA_DIR.mkdir(parents=True, exist_ok=True)

# Logging ra m√†n h√¨nh
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)
formatter = logging.Formatter('%(asctime)s - %(message)s', datefmt='%H:%M:%S')
console_handler.setFormatter(formatter)

logger = logging.getLogger("KG_BUILDER")
logger.setLevel(logging.INFO)
if not logger.handlers:
    logger.addHandler(console_handler)

class KnowledgeExtractor:
    def __init__(self):
        try:
            local_llm.load_model()
        except Exception as e:
            logger.error(f"‚ùå Kh√¥ng th·ªÉ load Local LLM: {e}")
            raise e

    def clean_json_response(self, text):
        """L√†m s·∫°ch chu·ªói JSON t·ª´ output c·ªßa LLM (Robust Version)"""
        # 1. Lo·∫°i b·ªè th·∫ª <think> v√† markdown
        text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)
        text = re.sub(r'```json', '', text)
        text = re.sub(r'```', '', text)
        
        # 2. T√¨m kh·ªëi JSON th√¥ t·ª´ d·∫•u { ƒë·∫ßu ti√™n ƒë·∫øn d·∫•u } cu·ªëi c√πng
        start_idx = text.find('{')
        end_idx = text.rfind('}')
        
        if start_idx == -1 or end_idx == -1 or end_idx <= start_idx:
            return "{}"
        
        json_str = text[start_idx : end_idx + 1]
        
        # 3. Fix l·ªói ph·ªï bi·∫øn: D·∫•u ph·∫©y th·ª´a ·ªü cu·ªëi list/dict (VD: {"a": 1,})
        # Regex n√†y t√¨m d·∫•u ph·∫©y ƒë·ª©ng tr∆∞·ªõc d·∫•u ƒë√≥ng ngo·∫∑c v√† x√≥a n√≥
        json_str = re.sub(r',\s*([\]}])', r'\1', json_str)
        
        return json_str.strip()

    def extract_graph_from_text(self, text_chunk):
        # One-shot Prompt: Cung c·∫•p v√≠ d·ª• c·ª• th·ªÉ ƒë·ªÉ ƒë·ªãnh h∆∞·ªõng model
        prompt = f"""
        You are a medical data extractor. Convert the text into a JSON Knowledge Graph.
        
        ### EXAMPLE:
        Text: "Metformin treats Type 2 Diabetes but may cause Nausea."
        JSON Output:
        {{
            "nodes": [
                {{"id": "Metformin", "label": "Drug"}},
                {{"id": "Type 2 Diabetes", "label": "Disease"}},
                {{"id": "Nausea", "label": "Symptom"}}
            ],
            "edges": [
                {{"source": "Metformin", "target": "Type 2 Diabetes", "type": "TREATS"}},
                {{"source": "Metformin", "target": "Nausea", "type": "CAUSES"}}
            ]
        }}
        
        ### TASK:
        Text: "{text_chunk}"
        
        Required: Output VALID JSON only. No explanations.
        """
        
        try:
            # --- T·ªêI ∆ØU G·ªåI H√ÄM ---
            messages = [
                {"role": "system", "content": "You are a JSON extractor. Output valid JSON only."},
                {"role": "user", "content": prompt}
            ]
            
            input_ids = local_llm.tokenizer.apply_chat_template(
                messages, add_generation_prompt=True, return_tensors="pt"
            ).to(local_llm.model.device)
            
            attention_mask = import_torch().ones_like(input_ids)
            
            print("   ‚Ü≥ ü§ñ AI ƒëang suy nghƒ©...", end="\r")
            
            with import_torch().no_grad():
                outputs = local_llm.model.generate(
                    input_ids,
                    attention_mask=attention_mask,
                    pad_token_id=local_llm.tokenizer.pad_token_id,
                    max_new_tokens=1024, # ƒê·ªß d√†i cho JSON
                    temperature=0.1,     # Th·∫•p ƒë·ªÉ ·ªïn ƒë·ªãnh
                    do_sample=False      # Greedy search
                )
            
            raw_response = local_llm.tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)
            print("   ‚Ü≥ ‚úÖ AI ƒë√£ tr·∫£ l·ªùi!       ") 

            clean_json = self.clean_json_response(raw_response)
            
            # --- PARSE JSON ---
            try:
                graph_data = json.loads(clean_json)
            except json.JSONDecodeError as e:
                print(f"   ‚Ü≥ ‚ö†Ô∏è JSON Parse Error: {str(e)[:50]}")
                # print(f"DEBUG: {clean_json}") # Uncomment ƒë·ªÉ debug
                return None
            
            # Chu·∫©n h√≥a keys
            if "nodes" not in graph_data: graph_data["nodes"] = []
            if "edges" not in graph_data: graph_data["edges"] = []
            
            return graph_data
            
        except Exception as e:
            print(f"   ‚Ü≥ ‚ùå L·ªói h·ªá th·ªëng: {str(e)[:50]}...")
            return None

    def ingest_to_neo4j(self, graph_data):
        if not graph_data or not db_connector: return

        # ƒê·∫£m b·∫£o source/target trong edges ƒë·ªÅu t·ªìn t·∫°i trong nodes ƒë·ªÉ tr√°nh l·ªói orphan edges
        # Trong th·ª±c t·∫ø, c√≥ th·ªÉ c·∫ßn merge nodes tr∆∞·ªõc
        
        node_query = """
        UNWIND $nodes AS n MERGE (node {name: n.id}) 
        ON CREATE SET node.id = n.id, node.source='User_Upload' 
        WITH node, n CALL apoc.create.addLabels(node, [n.label]) YIELD node as l RETURN count(l)
        """
        edge_query = """
        UNWIND $edges AS e MATCH (s {name: e.source}), (t {name: e.target}) 
        MERGE (s)-[r:RELATED {type: e.type, provenance:'User_Upload'}]->(t) RETURN count(r)
        """

        try:
            if graph_data.get("nodes"):
                db_connector.run_query(node_query, {"nodes": graph_data["nodes"]})
            if graph_data.get("edges"):
                db_connector.run_query(edge_query, {"edges": graph_data["edges"]})
            logger.info(f"   + DB: Saved {len(graph_data.get('nodes', []))} nodes, {len(graph_data.get('edges', []))} edges.")
        except Exception as e:
            logger.error(f"‚ùå DB Error: {e}")

def import_torch():
    import torch
    return torch

def main():
    print("üöÄ B·∫Øt ƒë·∫ßu n·∫°p d·ªØ li·ªáu (Robust Mode)")
    if db_connector is None: 
        print("‚ùå Kh√¥ng c√≥ k·∫øt n·ªëi DB.")
        return

    files = glob.glob(str(DATA_DIR / "*.txt"))
    if not files:
        print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file .txt n√†o trong data/custom_knowledge")
        print("   -> T·∫°o file sample...")
        sample_file = DATA_DIR / "sample_vn.txt"
        with open(sample_file, "w", encoding="utf-8") as f:
            f.write("C√¢y ch√≥ ƒë·∫ª (Di·ªáp h·∫° ch√¢u) h·ªó tr·ª£ tr·ªã vi√™m gan B nh∆∞ng g√¢y h·∫° huy·∫øt √°p.")
        files = [str(sample_file)]

    extractor = KnowledgeExtractor()
    
    for file_path in files:
        logger.info(f"üìÇ File: {os.path.basename(file_path)}")
        with open(file_path, "r", encoding="utf-8") as f: text = f.read()
        
        # Chia nh·ªè text
        chunk_size = 800 
        chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]
        
        for idx, chunk in enumerate(chunks):
            logger.info(f"   Processing chunk {idx+1}/{len(chunks)}...")
            graph_data = extractor.extract_graph_from_text(chunk)
            if graph_data: extractor.ingest_to_neo4j(graph_data)

    local_llm.unload()
    if db_connector: db_connector.close()
    print("\nüéâ Ho√†n t·∫•t!")

if __name__ == "__main__":
    main()

===== .\scripts\prepare_pubmedqa.py =====
import os
import json
import requests
from pathlib import Path

# --- C·∫§U H√åNH ---
OUTPUT_DIR = Path("data/pubmedqa")
OUTPUT_FILE = OUTPUT_DIR / "test.jsonl"

# URL d·ªØ li·ªáu g·ªëc t·ª´ GitHub c·ªßa PubMedQA
URL_DATA = "https://raw.githubusercontent.com/pubmedqa/pubmedqa/master/data/ori_pqal.json"
URL_TEST_SPLIT = "https://raw.githubusercontent.com/pubmedqa/pubmedqa/master/data/test_ground_truth.json"

def download_file(url, save_path):
    """H√†m t·∫£i file t·ª´ URL"""
    if save_path.exists():
        print(f"‚è© File ƒë√£ t·ªìn t·∫°i: {save_path}")
        return
    
    print(f"‚¨áÔ∏è ƒêang t·∫£i {url}...")
    try:
        response = requests.get(url)
        response.raise_for_status()
        with open(save_path, 'wb') as f:
            f.write(response.content)
        print(f"‚úÖ ƒê√£ l∆∞u: {save_path}")
    except Exception as e:
        print(f"‚ùå L·ªói t·∫£i file {url}: {e}")
        exit(1)

def format_context(contexts):
    """N·ªëi c√°c ƒëo·∫°n vƒÉn trong context th√†nh m·ªôt chu·ªói duy nh·∫•t"""
    if isinstance(contexts, list):
        return " ".join(contexts)
    return str(contexts)

def main():
    # 1. T·∫°o th∆∞ m·ª•c data
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    
    raw_data_path = OUTPUT_DIR / "ori_pqal.json"
    test_split_path = OUTPUT_DIR / "test_ground_truth.json"

    # 2. T·∫£i d·ªØ li·ªáu ngu·ªìn
    download_file(URL_DATA, raw_data_path)
    download_file(URL_TEST_SPLIT, test_split_path)

    # 3. ƒê·ªçc d·ªØ li·ªáu
    print("üîÑ ƒêang x·ª≠ l√Ω d·ªØ li·ªáu...")
    with open(raw_data_path, 'r', encoding='utf-8') as f:
        full_data = json.load(f) # Dictionary ch·ª©a to√†n b·ªô 1k m·∫´u PQA-L
    
    with open(test_split_path, 'r', encoding='utf-8') as f:
        test_ids = json.load(f) # Dictionary {PMID: label} c·ªßa t·∫≠p test

    # 4. Chuy·ªÉn ƒë·ªïi sang ƒë·ªãnh d·∫°ng chu·∫©n JSONL
    # Format MedCOT c·∫ßn: {"Question": ..., "Context": ..., "Correct Answer": ...}
    
    processed_count = 0
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f_out:
        # Duy·ªát qua c√°c ID n·∫±m trong t·∫≠p test chu·∫©n
        for pmid, label in test_ids.items():
            if pmid not in full_data:
                continue
                
            original_item = full_data[pmid]
            
            # T·∫°o record chu·∫©n h√≥a
            record = {
                "id": pmid,
                "Question": original_item["QUESTION"],
                # Context trong PubMedQA l√† list c√°c c√¢u, c·∫ßn n·ªëi l·∫°i
                "Context": format_context(original_item["CONTEXTS"]), 
                "Correct Answer": label, # yes, no, ho·∫∑c maybe
                "Long Answer": original_item.get("LONG_ANSWER", ""),
                "Meshes": original_item.get("MESHES", [])
            }
            
            # Ghi d√≤ng JSONL
            f_out.write(json.dumps(record, ensure_ascii=False) + '\n')
            processed_count += 1

    print(f"üéâ Ho√†n t·∫•t! ƒê√£ t·∫°o file dataset t·∫°i: {OUTPUT_FILE}")
    print(f"üìä T·ªïng s·ªë m·∫´u Test: {processed_count}")

if __name__ == "__main__":
    main()

===== .\scripts\setup_import_primekg.sh =====
#!/bin/bash

VOLUME_NAME="medcot_primekg_data"

echo "üõë Stopping Neo4j container if running..."
docker-compose down

echo "üóëÔ∏è  Deleting old Neo4j data volume: $VOLUME_NAME..."
docker volume rm $VOLUME_NAME || true

echo "üöÄ Starting PrimeKG data import into Neo4j (WITH GDS PLUGIN on 5.26.18)..."

MSYS_NO_PATHCONV=1 docker run --interactive --tty --rm \
    --volume "$(pwd)/data/primekg/import":/import \
    --volume $VOLUME_NAME:/data \
    --env NEO4J_PLUGINS='["apoc", "graph-data-science"]' \
    neo4j:5.26.18 \
    neo4j-admin database import full \
    --nodes=/import/nodes.csv \
    --relationships=/import/edges.csv \
    --overwrite-destination \
    neo4j
# -----------------------------------------------------------------------------

if [ $? -eq 0 ]; then
    echo "‚úÖ IMPORT D·ªÆ LI·ªÜU TH√ÄNH C√îNG!"
    echo "üëç D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c n·∫°p v√†o volume '$VOLUME_NAME'."
    
    # ==============================================================================
    # N√ÇNG C·∫§P: T·ª± ƒë·ªông kh·ªüi ƒë·ªông server Neo4j sau khi import
    # ==============================================================================
    echo "üöÄ T·ª± ƒë·ªông kh·ªüi ƒë·ªông Neo4j server b·∫±ng docker-compose..."
    
    # L·ªánh `up -d` s·∫Ω kh·ªüi ƒë·ªông server ·ªü ch·∫ø ƒë·ªô n·ªÅn (detached)
    docker-compose up -d

    echo "‚è≥ ƒêang ƒë·ª£i server kh·ªüi ƒë·ªông (kho·∫£ng 15-20 gi√¢y)..."
    sleep 20

    echo "‚úÖ‚úÖ‚úÖ HO√ÄN T·∫§T! Server Neo4j ƒë√£ ƒë∆∞·ª£c kh·ªüi ƒë·ªông v√† s·∫µn s√†ng."
    echo "üëâ B√¢y gi·ªù b·∫°n c√≥ th·ªÉ ch·∫°y 'python main.py --query \"...\"'"
    # ==============================================================================

else
    echo "‚ùå IMPORT D·ªÆ LI·ªÜU TH·∫§T B·∫†I. Vui l√≤ng ki·ªÉm tra l·ªói ·ªü tr√™n."
    exit 1
fi

===== .\scripts\train_aux_verifier.py =====
# run/train_verifier.py
import torch
import torch.nn as nn
from torch.utils.data import TensorDataset, DataLoader
import numpy as np
from pathlib import Path
from src.models.verifier import MultiSignalVerifier

# --- CONFIG ---
# --- N√ÇNG C·∫§P: TƒÉng INPUT_DIM t·ª´ 7 l√™n 8 ---
INPUT_DIM = 8 # Features: [nli, gcot, in_kg, causality, len, src_deg, tgt_deg, provenance]
MODEL_PATH = Path("models/verifier_weights.pth")
MODEL_PATH.parent.mkdir(exist_ok=True)

def create_dummy_data(num_samples=1000):
    """T·∫°o d·ªØ li·ªáu gi·∫£ l·∫≠p ƒë·ªÉ hu·∫•n luy·ªán."""
    print("... Creating dummy training data ...")
    
    # Positive samples (good paths)
    pos_features = np.random.rand(num_samples // 2, INPUT_DIM)
    pos_features[:, 0] = np.random.uniform(0.7, 1.0, num_samples // 2) # High NLI
    pos_features[:, 2] = 1.0 # In KG
    pos_features[:, 7] = np.random.uniform(0.8, 1.0, num_samples // 2) # High Provenance (PrimeKG, PSG)
    pos_labels = np.ones(num_samples // 2)

    # Negative samples (bad paths)
    neg_features = np.random.rand(num_samples // 2, INPUT_DIM)
    neg_features[:, 0] = np.random.uniform(0.0, 0.4, num_samples // 2) # Low NLI
    neg_features[:, 7] = np.random.uniform(0.3, 0.5, num_samples // 2) # Low Provenance (RTX-KG2, Default)
    neg_labels = np.zeros(num_samples // 2)

    X = np.vstack([pos_features, neg_features])
    y = np.concatenate([pos_labels, neg_labels])

    # Shuffle
    idx = np.random.permutation(len(X))
    X, y = X[idx], y[idx]

    return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)

def main():
    # ... (Logic hu·∫•n luy·ªán gi·ªØ nguy√™n)
    print("üöÄ Starting Verifier Model Training...")
    X_train, y_train = create_dummy_data()
    dataset = TensorDataset(X_train, y_train.unsqueeze(1))
    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)
    
    model = MultiSignalVerifier(input_dim=INPUT_DIM)
    criterion = nn.BCEWithLogitsLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    
    epochs = 20
    for epoch in range(epochs):
        for batch_X, batch_y in dataloader:
            optimizer.zero_grad()
            outputs = model(batch_X)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
        print(f"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}")

    torch.save(model.state_dict(), MODEL_PATH)
    print(f"‚úÖ Model weights saved to {MODEL_PATH}")

if __name__ == "__main__":
    main()

===== .\src\__init__.py =====


===== .\src\core\config.py =====
# T·ªáp: src/core/config.py
LINKING_THRESHOLD = 0.7 
# (C√°c h·∫±ng s·ªë kh√°c gi·ªØ nguy√™n nh∆∞ tr∆∞·ªõc)
SPACY_MODEL_NAME = "xx_sent_ud_sm"
EXTRACTION_MODEL_NAME = "urchade/gliner_multi-v2.1"
ENTITY_LABELS = ["medical condition or disease", "symptom or sign", "drug or medication", "laboratory test", "anatomy or body part", "medical procedure", "gene or protein"]
GLINER_TO_INTERNAL_LABEL_MAP = {"medical condition or disease": "disease", "symptom or sign": "symptom", "drug or medication": "drug", "laboratory test": "lab_test", "medical procedure": "procedure", "anatomy or body part": "anatomy", "gene or protein": "gene"}
INTERNAL_LABEL_TO_KG_TYPE_MAP = {"disease": "Disease", "drug": "Drug", "symptom": "Effect/Phenotype", "anatomy": "Anatomy", "procedure": "Procedure", "lab_test": "Gene/Protein", "gene": "Gene/Protein"}
KG_TYPE_TO_INTERNAL_MAP = {v: k for k, v in INTERNAL_LABEL_TO_KG_TYPE_MAP.items()}
DEFAULT_EXTRACTION_THRESHOLD = 0.35
KNOWN_ENTITIES = {"diabetes": ("disease", "Disease"), "metformin": ("drug", "Drug"), "hypertension": ("disease", "Disease"), "warfarin": ("drug", "Drug"), "aspirin": ("drug", "Drug"), "kidney disease": ("disease", "Disease")}
DENSE_RETRIEVAL_MODEL = "BAAI/bge-small-en-v1.5"
RERANKER_MODEL = "cross-encoder/ms-marco-MiniLM-L-6-v2"
KG_TYPE_TO_UMLS_STY_MAP = {"disease": ["Disease or Syndrome", "Neoplastic Process", "Pathologic Function", "Congenital Abnormality", "Mental or Behavioral Dysfunction", "Injury or Poisoning"], "symptom": ["Sign or Symptom", "Finding", "Laboratory or Test Result"], "drug": ["Pharmacologic Substance", "Clinical Drug", "Antibiotic", "Biologically Active Substance"], "procedure": ["Therapeutic or Preventive Procedure", "Diagnostic Procedure", "Health Care Activity"], "anatomy": ["Body Part, Organ, or Organ Component", "Anatomical Structure", "Body Location or Region", "Tissue"], "gene": ["Gene or Genome", "Amino Acid, Peptide, or Protein", "Enzyme"], "lab_test": ["Laboratory Procedure", "Diagnostic Procedure"]}
NODE_EMBEDDING_DIM = 384  
HGT_HIDDEN_CHANNELS = 128
HGT_NUM_HEADS = 4
NLI_MODEL_NAME = "cross-encoder/nli-distilroberta-base"
WEIGHTS = {"in_kg": 0.35, "link_pred": 0.05, "nli": 0.15, "causality": 0.15, "gcot": 0.10, "trust": 0.20}

===== .\src\core\state.py =====
# src/core/state.py
from typing import List, Optional, Any, Dict, Tuple
from pydantic import BaseModel, Field
from datetime import datetime
import uuid

class Mention(BaseModel):
    text: str
    label: str
    span: Tuple[int, int]
    score: float
    source: str
    kg_type: Optional[str] = None
    attributes: Dict[str, Any] = Field(default_factory=dict)

class LinkedCandidate(BaseModel):
    node_id: str
    node_label: str
    preferred_name: str
    score: float
    source: str

class LinkedEntity(BaseModel):
    source_mention: Mention
    candidates: List[LinkedCandidate] = Field(default_factory=list)
    link_status: str = "unlinked"
    best_candidate: Optional[LinkedCandidate] = None

class MedCOTState(BaseModel):
    query_id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    created_at: datetime = Field(default_factory=datetime.now)

    raw_query: str
    patient_context: Optional[str] = None
    normalized_query: Optional[str] = None
    normalized_patient_context: Optional[str] = None 
    sentences: List[Dict[str, Any]] = Field(default_factory=list)
    mentions: List[Mention] = Field(default_factory=list)
    linked_entities: List[LinkedEntity] = Field(default_factory=list)
    seed_nodes: List[str] = Field(default_factory=list)
    unlinked_mentions: List[Mention] = Field(default_factory=list)
    graph_refs: Dict[str, Any] = Field(default_factory=dict)
    gcot: Dict[str, Any] = Field(default_factory=dict)
    candidate_paths: List[Dict[str, Any]] = Field(default_factory=list)
    verified_path: List[Dict[str, Any]] = Field(default_factory=list)
    global_confidence: float = 0.0
    reasoning_mode: str = "Abstain"
    final_answer: Optional[str] = None
    safety_flags: List[Dict[str, Any]] = Field(default_factory=list)
    logs: List[Dict[str, Any]] = Field(default_factory=list)

    def log(self, step: str, status: str, message: Any = "", metadata: dict = None):
        if metadata is None: metadata = {}
        
        # --- FIX P0-4: Auto-detect metadata ---
        if isinstance(message, dict) and not metadata:
            metadata = message
            message_str = ""
        else:
            message_str = str(message)
            
        self.logs.append({
            "step": step,
            "status": status,
            "message": message_str,
            "timestamp": datetime.now().isoformat(),
            **metadata
        })

===== .\src\core\__init__.py =====


===== .\src\modules\step0_preprocess.py =====
# src/modules/step0_preprocess.py
import re
import unicodedata
import json
from pathlib import Path
from typing import Optional, Dict
import logging

from presidio_analyzer import AnalyzerEngine
from presidio_anonymizer import AnonymizerEngine
from presidio_anonymizer.entities import OperatorConfig
import spacy

from src.core.state import MedCOTState

# Gi·∫£m log th·ª´a t·ª´ c√°c th∆∞ vi·ªán
logging.getLogger("PyRuSH").setLevel(logging.WARNING)
logging.getLogger("presidio-analyzer").setLevel(logging.WARNING)

logger = logging.getLogger("step0_preprocess")

_resources = {}

def load_resources():
    """T·∫£i c√°c t√†i nguy√™n c·∫ßn thi·∫øt cho preprocessing."""
    global _resources
    if _resources:
        return _resources
        
    logger.info("‚è≥ Loading Preprocessing resources...")
    try:
        # T·∫£i t·ª´ ƒëi·ªÉn vi·∫øt t·∫Øt (n·∫øu c√≥)
        dict_path = Path("data/dictionaries/abbreviations.json")
        _resources["abbreviations"] = json.load(open(dict_path, "r", encoding="utf-8")) if dict_path.is_file() else {}
        
        # Kh·ªüi t·∫°o Presidio ƒë·ªÉ ·∫©n th√¥ng tin nh·∫°y c·∫£m (PHI)
        _resources["analyzer"] = AnalyzerEngine()
        _resources["anonymizer"] = AnonymizerEngine()
        _resources["phi_operators"] = {
            "DEFAULT": OperatorConfig("replace", {"new_value": "<PHI>"}),
        }
        
        # T·∫£i model SpaCy ƒë·ªÉ t√°ch c√¢u
        _resources["nlp"] = spacy.load("en_core_web_sm") # D√πng model ti·∫øng Anh
        
    except Exception as e:
        logger.critical(f"‚ùå Failed to load preprocessing resources: {e}")
        # Th·ª≠ t·∫£i l·∫°i model spacy n·∫øu l·ªói
        try:
            import subprocess
            subprocess.run(["python", "-m", "spacy", "download", "en_core_web_sm"])
            _resources["nlp"] = spacy.load("en_core_web_sm")
        except Exception as spacy_e:
            logger.critical(f"‚ùå Spacy model download/load failed again: {spacy_e}")
            return {}
            
    return _resources

def _normalize_text(text: Optional[str]) -> str:
    """Chu·∫©n h√≥a text: b·ªè kho·∫£ng tr·∫Øng th·ª´a, normalize unicode."""
    if not text: return ""
    text = unicodedata.normalize('NFC', text)
    text = re.sub(r"[ \t]+", " ", text)
    text = re.sub(r"\n{3,}", "\n\n", text).strip()
    return text

def _expand_abbreviations(text: str, abbr_dict: Dict[str, str]) -> str:
    """Thay th·∫ø c√°c t·ª´ vi·∫øt t·∫Øt y khoa b·∫±ng d·∫°ng ƒë·∫ßy ƒë·ªß."""
    if not abbr_dict: return text
    for abbr, full_text in abbr_dict.items():
        pattern = r'\b' + re.escape(abbr) + r'\b'
        text = re.sub(pattern, full_text, text, flags=re.IGNORECASE)
    return text

def run(state: MedCOTState, enable_phi_redaction: bool = True) -> MedCOTState:
    """
    H√†m ch√≠nh c·ªßa pipeline Step 0.
    Th·ª±c hi·ªán: Chu·∫©n h√≥a, m·ªü r·ªông vi·∫øt t·∫Øt, ·∫©n th√¥ng tin nh·∫°y c·∫£m, t√°ch c√¢u.
    """
    resources = load_resources()
    if not resources.get("nlp") or not resources.get("analyzer"):
        state.log("0_PREPROCESS", "FAILED", "Resource loading failed.")
        raise RuntimeError("Preprocessing resources failed to load.")

    try:
        # 1. X·ª≠ l√Ω Query
        normalized_query = _normalize_text(state.raw_query)
        expanded_query = _expand_abbreviations(normalized_query, resources.get("abbreviations", {}))
        
        redacted_query = expanded_query
        if enable_phi_redaction:
            analyzer_results = resources["analyzer"].analyze(text=expanded_query, language='en')
            redacted_query = resources["anonymizer"].anonymize(
                text=expanded_query, analyzer_results=analyzer_results, operators=resources["phi_operators"]
            ).text
        state.normalized_query = redacted_query

        # 2. X·ª≠ l√Ω Patient Context
        if state.patient_context:
            normalized_context = _normalize_text(state.patient_context)
            expanded_context = _expand_abbreviations(normalized_context, resources.get("abbreviations", {}))
            
            redacted_context = expanded_context
            if enable_phi_redaction:
                analyzer_results = resources["analyzer"].analyze(text=expanded_context, language='en')
                redacted_context = resources["anonymizer"].anonymize(
                    text=expanded_context, analyzer_results=analyzer_results, operators=resources["phi_operators"]
                ).text
            state.normalized_patient_context = redacted_context
        
        # 3. T√°ch c√¢u (ch·ªâ t√°ch c√¢u c·ªßa query ch√≠nh)
        doc = resources["nlp"](state.normalized_query)
        state.sentences = [{"text": sent.text, "span": (sent.start_char, sent.end_char)} for sent in doc.sents]
        
        state.log("0_PREPROCESS", "SUCCESS", "Text normalized and sentences segmented.")
        
    except Exception as e:
        logger.exception("Error during preprocessing")
        state.log("0_PREPROCESS", "FAILED", message=str(e))
        raise e
        
    return state

===== .\src\modules\step10_logging.py =====
# src/modules/step10_logging.py
import logging
import json
from pathlib import Path
from datetime import datetime, date
from src.core.state import MedCOTState
import numpy as np

logging.basicConfig(level=logging.INFO, format='%(levelname)s:%(name)s:%(message)s')
logger = logging.getLogger("step10_logging")

def clean_for_json(obj):
    """
    H√†m ƒë·ªá quy ƒë·ªÉ l√†m s·∫°ch d·ªØ li·ªáu tr∆∞·ªõc khi l∆∞u v√†o JSON.
    X·ª≠ l√Ω:
    1. C√°c ki·ªÉu s·ªë c·ªßa Numpy (int64, float32, etc.) -> int, float c·ªßa Python
    2. Numpy Arrays -> List
    3. Datetime/Date -> ISO String
    4. C√°c ki·ªÉu d·ªØ li·ªáu c∆° b·∫£n -> Gi·ªØ nguy√™n
    """
    if isinstance(obj, (np.integer, np.int64)):
        return int(obj)
    if isinstance(obj, (np.floating, np.float32, np.float64)):
        return float(obj)
    if isinstance(obj, np.ndarray):
        return obj.tolist()
    if isinstance(obj, (datetime, date)):
        return obj.isoformat()
    if isinstance(obj, dict):
        return {k: clean_for_json(v) for k, v in obj.items()}
    if isinstance(obj, list):
        return [clean_for_json(i) for i in obj]
    return obj

def run(state: MedCOTState, output_dir: str = "output/audit_logs") -> MedCOTState:
    logger.info("üöÄ B·∫Øt ƒë·∫ßu ghi log v√† audit trail...")
    
    try:
        Path(output_dir).mkdir(parents=True, exist_ok=True)
        log_file = Path(output_dir) / f"{state.query_id}.json"
        
        # 1. L·∫•y dictionary thu·∫ßn t·ª´ Pydantic
        state_dict = state.model_dump(mode='python')

        # 2. L√†m s·∫°ch d·ªØ li·ªáu (Numpy + Datetime + c√°c ki·ªÉu s·ªë ƒë·∫∑c bi·ªát)
        clean_state_dict = clean_for_json(state_dict)

        # 3. Ghi file
        with open(log_file, 'w', encoding='utf-8') as f:
            json.dump(clean_state_dict, f, indent=2, ensure_ascii=False)
            
        logger.info(f"‚úÖ ƒê√£ l∆∞u audit trail v√†o {log_file}")
        state.log("10_LOGGING", "SUCCESS", metadata={"log_file": str(log_file)})

    except Exception as e:
        logger.exception("L·ªói trong qu√° tr√¨nh Logging")
        state.log("10_LOGGING", "FAILED", message=str(e))
        
    return state



===== .\src\modules\step1_extraction.py =====
import logging
from typing import List, Dict, Any

# Di chuy·ªÉn c√°c import n·∫∑ng v√†o trong h√†m ƒë·ªÉ ki·ªÉm so√°t l·ªói t·ªët h∆°n
# from gliner import GLiNER
# import medspacy

from spacy.tokens import Span
from spacy.util import filter_spans
from spacy.matcher import PhraseMatcher

from src.core.state import MedCOTState, Mention
from src.core import config

logger = logging.getLogger("step1_hybrid_extraction")

_models = {}
_models_loaded = False

def load_models_bulletproof():
    global _models, _models_loaded
    if _models_loaded:
        return _models

    logger.info("--- Starting Resource Loading for Step 1 ---")
    
    # 1. T·∫£i GLiNER
    try:
        from gliner import GLiNER
        logger.info("  -> Attempting to load GLiNER model...")
        _models["gliner"] = GLiNER.from_pretrained(config.EXTRACTION_MODEL_NAME)
        logger.info("  [SUCCESS] GLiNER model loaded.")
    except Exception as e:
        logger.error(f"  [CRITICAL FAILURE] Could not load GLiNER model. Extraction will be degraded. Error: {e}", exc_info=True)
        _models["gliner"] = None

    # 2. T·∫£i MedSpaCy
    try:
        import medspacy
        logger.info("  -> Attempting to load MedSpaCy model...")
        nlp = medspacy.load(enable=["medspacy_pyrush", "medspacy_target_matcher", "medspacy_context"])
        if not Span.has_extension("source_mention"):
            Span.set_extension("source_mention", default=None)
        _models["medspacy"] = nlp
        logger.info("  [SUCCESS] MedSpaCy model loaded.")
    except Exception as e:
        logger.error(f"  [CRITICAL FAILURE] Could not load MedSpaCy model. Context analysis disabled. Error: {e}", exc_info=True)
        _models["medspacy"] = None

    # 3. X√¢y d·ª±ng PhraseMatcher (nh·∫π, √≠t khi l·ªói)
    if _models.get("medspacy"):
        try:
            logger.info("  -> Building PhraseMatcher...")
            nlp = _models["medspacy"]
            matcher = PhraseMatcher(nlp.vocab, attr="LOWER")
            for term, (label, kg_type) in config.KNOWN_ENTITIES.items():
                pattern_id = f"{label}||{kg_type}"
                matcher.add(pattern_id, [nlp.make_doc(term)])
            _models["matcher"] = matcher
            logger.info("  [SUCCESS] PhraseMatcher built.")
        except Exception as e:
            logger.error(f"  [FAILED] Could not build PhraseMatcher. Error: {e}")
            _models["matcher"] = None
    
    _models_loaded = True
    logger.info("--- Resource Loading for Step 1 Finished ---")
    return _models

def _run_ner_on_text(text: str, models: dict) -> List[Dict[str, Any]]:
    if not text: return []
    
    gliner_ents = []
    if models.get("gliner"):
        try:
            raw_preds = models["gliner"].predict_entities(text, config.ENTITY_LABELS, threshold=config.DEFAULT_EXTRACTION_THRESHOLD)
            for e in raw_preds:
                gliner_ents.append({
                    "text": e["text"], 
                    "label": config.GLINER_TO_INTERNAL_LABEL_MAP.get(e["label"], "unknown"), 
                    "span": (e["start"], e["end"]), 
                    "score": e["score"], 
                    "source": "gliner"
                })
        except Exception as e:
            logger.error(f"GLiNER prediction failed: {e}")

    dict_ents = []
    if models.get("matcher") and models.get("medspacy"):
        nlp = models["medspacy"]
        doc = nlp(text)
        matches = models["matcher"](doc)
        for match_id, start, end in matches:
            string_id = nlp.vocab.strings[match_id]
            label, kg_type = string_id.split("||")
            span_doc = doc[start:end]
            dict_ents.append({
                "text": span_doc.text, "label": label, "kg_type": kg_type,
                "span": (span_doc.start_char, span_doc.end_char), "score": 1.0, "source": "expert_dictionary"
            })
            
    # Merge logic (∆∞u ti√™n dictionary)
    if not dict_ents: return sorted(gliner_ents, key=lambda x: x['span'][0])
    final_entities = list(dict_ents)
    dict_ranges = {i for d in dict_ents for i in range(d['span'][0], d['span'][1])}
    for g_ent in gliner_ents:
        is_overlapped = any(i in dict_ranges for i in range(g_ent['span'][0], g_ent['span'][1]))
        if not is_overlapped:
            final_entities.append(g_ent)
            
    return sorted(final_entities, key=lambda x: x['span'][0])

def run(state: MedCOTState) -> MedCOTState:
    logger.info("--- Running Step 1: Entity Extraction (Bulletproof Version) ---")
    models = load_models_bulletproof()
    
    # D·ª´ng n·∫øu kh√¥ng c√≥ model n√†o t·∫£i ƒë∆∞·ª£c
    if not models.get("gliner") and not models.get("matcher"):
        state.log("1_EXTRACTION", "CRITICAL_FAILURE", "No extraction models could be loaded.")
        logger.critical("CRITICAL: Both GLiNER and PhraseMatcher failed to load. Cannot proceed with extraction.")
        return state

    all_raw_mentions = []
    logger.info("  -> Extracting from query...")
    q_ments = _run_ner_on_text(state.normalized_query, models)
    for m in q_ments: m['source_doc'] = 'query'
    all_raw_mentions.extend(q_ments)

    if state.normalized_patient_context:
        logger.info("  -> Extracting from patient context...")
        c_ments = _run_ner_on_text(state.normalized_patient_context, models)
        for m in c_ments: m['source_doc'] = 'patient_context'
        all_raw_mentions.extend(c_ments)
    
    if not all_raw_mentions:
        state.log("1_EXTRACTION", "SKIPPED", "No entities found.")
        logger.warning("No entities found in any text.")
        return state

    for m in all_raw_mentions:
        if "kg_type" not in m:
            m["kg_type"] = config.INTERNAL_LABEL_TO_KG_TYPE_MAP.get(m["label"], "Phenotype")

    # X·ª≠ l√Ω context (ph·ªß ƒë·ªãnh, etc.) n·∫øu medspacy ƒë√£ t·∫£i
    final_mentions = []
    if models.get("medspacy"):
        logger.info("  -> Running context analysis (negation, etc.)...")
        full_text = (state.normalized_query or "") + "\n" + (state.normalized_patient_context or "")
        nlp = models["medspacy"]
        doc = nlp(full_text)
        
        valid_spans = []
        offset = len(state.normalized_query or "") + 1
        for m in all_raw_mentions:
            start, end = m["span"]
            if m["source_doc"] == 'patient_context':
                start, end = start + offset, end + offset
            
            span = doc.char_span(start, end, label=m["label"])
            if span:
                span._.set("source_mention", m)
                valid_spans.append(span)

        doc.ents = filter_spans(valid_spans)
        for ent in doc.ents:
            src = ent._.get("source_mention")
            attrs = {'negated': ent._.is_negated, 'historical': ent._.is_historical, 'hypothetical': ent._.is_hypothetical}
            final_mentions.append(Mention(text=src["text"], label=src["label"], span=src["span"], score=src["score"], source=src["source_doc"], kg_type=src["kg_type"], attributes=attrs))
    else:
        logger.warning("MedSpaCy not loaded, skipping context analysis.")
        for m in all_raw_mentions:
             final_mentions.append(Mention(text=m["text"], label=m["label"], span=m["span"], score=m["score"], source=m["source_doc"], kg_type=m["kg_type"]))
            
    state.mentions = final_mentions
    state.log("1_EXTRACTION", "SUCCESS", metadata={"count": len(final_mentions)})
    logger.info(f"--- Step 1 Finished. Extracted {len(final_mentions)} mentions. ---")
    return state

===== .\src\modules\step2_linking.py =====
# T·ªáp: src/modules/step2_linking.py (PHI√äN B·∫¢N FIX L·ªñI ID=NONE)
import logging
from src.core.state import MedCOTState, LinkedEntity, LinkedCandidate
from src.utils.neo4j_connect import db_connector
from src.utils.umls_normalizer import umls_service

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("step2_linking")

def _search_neo4j(text: str, kg_type: str = None):
    """H√†m t√¨m ki·∫øm c·ªët l√µi trong Neo4j (Case Insensitive)"""
    if not db_connector: return None
    
    # --- S·ª¨A ƒê·ªîI QUAN TR·ªåNG ---
    # S·ª≠ d·ª•ng h√†m coalesce(n.id, elementId(n))
    # √ù nghƒ©a: N·∫øu n.id b·ªã Null th√¨ l·∫•y elementId(n) (ID n·ªôi b·ªô c·ªßa Neo4j, lu√¥n t·ªìn t·∫°i)
    query = """
    MATCH (n) 
    WHERE toLower(n.name) = toLower($text)
    RETURN 
        coalesce(n.id, elementId(n)) as node_id, 
        labels(n)[0] as node_label, 
        n.name as preferred_name
    LIMIT 1
    """
    try:
        res = db_connector.run_query(query, {"text": text})
        if res: return res[0]
    except Exception as e:
        logger.error(f"Error querying Neo4j: {e}")
        return None
    
    return None

def run(state: MedCOTState) -> MedCOTState:
    # ƒê·∫£m b·∫£o UMLS ƒë√£ k·∫øt n·ªëi
    try:
        umls_service.connect()
    except Exception:
        logger.warning("UMLS service not available, skipping synonyms.")
    
    final_linked = []
    
    for mention in state.mentions:
        le = LinkedEntity(source_mention=mention)
        found_candidate = None
        method = "failed"

        # 1. Th·ª≠ t√¨m tr·ª±c ti·∫øp (Direct Match)
        res = _search_neo4j(mention.text)
        if res:
            found_candidate = res
            method = "direct_exact"
        
        # 2. N·∫øu th·∫•t b·∫°i -> D√πng UMLS ƒë·ªÉ m·ªü r·ªông t·ª´ ƒë·ªìng nghƒ©a (Synonym Expansion)
        if not found_candidate:
            logger.info(f"üîç Direct match failed for '{mention.text}'. Asking UMLS...")
            synonyms = []
            try:
                synonyms = umls_service.get_synonyms(mention.text)
            except Exception as e:
                logger.error(f"UMLS error: {e}")

            if synonyms:
                logger.info(f"   -> UMLS found synonyms: {synonyms[:3]} ...")
                # Th·ª≠ t·ª´ng synonym trong Neo4j
                for syn in synonyms:
                    res = _search_neo4j(syn)
                    if res:
                        found_candidate = res
                        method = f"umls_synonym ({syn})"
                        logger.info(f"   ‚úÖ MATCHED via synonym: '{syn}' -> {res['preferred_name']}")
                        break
            else:
                logger.info("   -> UMLS found no synonyms.")

        # 3. G√°n k·∫øt qu·∫£
        if found_candidate:
            # ƒê·∫£m b·∫£o node_id lu√¥n l√† string (ph√≤ng h·ªù)
            safe_node_id = str(found_candidate["node_id"]) if found_candidate["node_id"] is not None else "UNKNOWN_ID"
            
            candidate = LinkedCandidate(
                node_id=safe_node_id, 
                node_label=found_candidate["node_label"],
                preferred_name=found_candidate["preferred_name"],
                score=1.0,
                source=method
            )
            le.link_status = "linked"
            le.best_candidate = candidate
            le.candidates = [candidate]
            logger.info(f"‚úÖ Linked '{mention.text}' -> '{candidate.preferred_name}' (ID: {candidate.node_id})")
        else:
            logger.warning(f"‚ùå Could not link '{mention.text}' even with UMLS synonyms.")

        final_linked.append(le)

    state.linked_entities = final_linked
    
    # C·∫≠p nh·∫≠t seed nodes cho c√°c b∆∞·ªõc sau
    linked_ids = [le.best_candidate.node_id for le in final_linked if le.link_status == "linked"]
    
    if linked_ids:
        # L·∫•y elementId th·ª±c t·∫ø ƒë·ªÉ query ƒë·ªì th·ªã (B∆∞·ªõc 4)
        # V√¨ node_id b√¢y gi·ªù c√≥ th·ªÉ l√† elementId ho·∫∑c id g·ªëc, ta query l·∫°i ƒë·ªÉ ch·∫Øc ch·∫Øn l·∫•y elementId
        # L∆∞u √Ω: N·∫øu node_id ƒë√£ l√† elementId th√¨ query n√†y v·∫´n ch·∫°y t·ªët n·∫øu ta d√πng WHERE elementId(n) = ... 
        # Nh∆∞ng ƒë·ªÉ ƒë∆°n gi·∫£n v√† an to√†n, ta d√πng name ƒë·ªÉ map l·∫°i elementId m·ªôt l·∫ßn n·ªØa cho danh s√°ch seed
        linked_names = [le.best_candidate.preferred_name for le in final_linked if le.link_status == "linked"]
        q = "MATCH (n) WHERE n.name IN $names RETURN elementId(n) as eid"
        r = db_connector.run_query(q, {"names": linked_names})
        state.seed_nodes = [x['eid'] for x in r]
    else:
        state.seed_nodes = []

    state.log("2_LINKING", "SUCCESS", {"count": len(state.seed_nodes)})
    return state

===== .\src\modules\step4_retrieval.py =====
# T·ªáp: src/modules/step4_retrieval.py (PHI√äN B·∫¢N FIX K·∫æT N·ªêI ID)
import logging
import uuid
from typing import Dict, Any, List
from src.core.state import MedCOTState
from src.utils.neo4j_connect import db_connector
from src.utils.arax_client import arax_client
from src.utils.name_resolver import name_resolver

logging.basicConfig(level=logging.INFO, format="%(levelname)s:%(name)s:%(message)s")
logger = logging.getLogger("step4_retrieval")

def _run_simple_expansion(seed_ids: List[str]) -> Dict[str, Any]:
    if not seed_ids or db_connector is None: return {"nodes": [], "edges": []}
    logger.info(f"   üï∏ [Simple Expansion] Getting seed nodes and direct neighbors...")
    
    # D√πng coalesce ƒë·ªÉ ƒë·∫£m b·∫£o lu√¥n l·∫•y ƒë∆∞·ª£c ID h·ª£p l·ªá (∆∞u ti√™n id g·ªëc, fallback sang elementId)
    query = """
    MATCH (seed) WHERE elementId(seed) IN $seeds OR seed.id IN $seeds
    OPTIONAL MATCH (seed)-[r]-(neighbor)
    WITH collect(DISTINCT seed) + collect(DISTINCT neighbor) as all_nodes_list, collect(DISTINCT r) as all_rels_list
    RETURN [node in all_nodes_list WHERE node IS NOT NULL | 
            {id: coalesce(node.id, elementId(node)), labels: labels(node), name: node.name, provenance: 'PrimeKG', element_id: elementId(node)}] as nodes,
           [rel in all_rels_list WHERE rel IS NOT NULL | 
            {id: elementId(rel), source: coalesce(startNode(rel).id, elementId(startNode(rel))), target: coalesce(endNode(rel).id, elementId(endNode(rel))), type: type(rel), provenance: 'PrimeKG'}] as relationships
    """
    try:
        results = db_connector.run_query(query, {"seeds": seed_ids})
        if not results: return {"nodes": [], "edges": []}
        record = results[0]
        return {"nodes": record.get("nodes", []), "edges": record.get("relationships", [])}
    except Exception as e:
        logger.error(f"Simple Expansion Query on PrimeKG failed: {e}")
        return {"nodes": [], "edges": []}

def _build_patient_state_graph(state: MedCOTState) -> Dict[str, Any]:
    psg_nodes, psg_edges, pid = [], [], f"PATIENT_{state.query_id[:8]}"
    psg_nodes.append({"id": pid, "label": "Patient", "name": "The Patient", "provenance": "PSG"})
    for le in state.linked_entities:
        if le.link_status == "linked" and le.source_mention.source == "patient_context":
            evt_id = f"EVT_{uuid.uuid4().hex[:6]}"
            # Link v√†o node_id ƒë√£ ƒë∆∞·ª£c chu·∫©n h√≥a ·ªü Step 2
            target_id = str(le.best_candidate.node_id)
            psg_nodes.append({"id": evt_id, "label": "Observation", "name": le.source_mention.text, "provenance": "PSG"})
            psg_edges.extend([
                {"source": evt_id, "target": target_id, "type": "GROUNDED_IN", "provenance": "PSG"}, 
                {"source": pid, "target": evt_id, "type": "PRESENTS_WITH", "provenance": "PSG"}
            ])
    return {"nodes": psg_nodes, "edges": psg_edges}

def run(state: MedCOTState, use_arax_fallback: bool = True) -> MedCOTState:
    logger.info("üöÄ Step 4: Hybrid Retrieval (Local + SRI Name Resolution)")
    
    local_graph = _run_simple_expansion(state.seed_nodes)
    arax_graph = {"nodes": [], "edges": []}
    
    # --- LOGIC QUAN TR·ªåNG: L·∫•y d·ªØ li·ªáu t·ª´ ARAX v√† c·∫≠p nh·∫≠t Seed Nodes ---
    if use_arax_fallback and state.mentions:
        entity_names = list(set([m.text for m in state.mentions]))
        logger.info(f"Resolving names externally: {entity_names}")
        
        resolved_curies = name_resolver.resolve_names_to_curies(entity_names)
        logger.info(f"Got resolved CURIEs: {resolved_curies}")
        
        # [FIX] Th√™m CURIEs t·ª´ ARAX v√†o seed_nodes ƒë·ªÉ Step 6 c√≥ th·ªÉ t√¨m ƒë∆∞·ªùng ƒëi
        if resolved_curies:
            state.seed_nodes.extend(resolved_curies)
            state.seed_nodes = list(set(state.seed_nodes)) # Remove duplicates
            logger.info(f"‚úÖ Updated Seed Nodes with External IDs: {resolved_curies}")
        
        if len(resolved_curies) >= 2:
            arax_edges = arax_client.query_kg2(resolved_curies, max_results=10) # TƒÉng max results
            if arax_edges:
                arax_nodes_map = {}
                for edge in arax_edges:
                    s_id, s_name = edge.get('source_id', edge.get('source')), edge.get('source')
                    t_id, t_name = edge.get('target_id', edge.get('target')), edge.get('target')
                    
                    # ƒê·∫£m b·∫£o ID node tr√πng v·ªõi ID trong seed_nodes (l√† CURIE)
                    arax_nodes_map[s_id] = {"id": s_id, "label": "ExternalEntity", "name": s_name, "provenance": "ARAX/KG2"}
                    arax_nodes_map[t_id] = {"id": t_id, "label": "ExternalEntity", "name": t_name, "provenance": "ARAX/KG2"}
                    
                    edge['source'], edge['target'] = s_id, t_id
                    
                arax_graph["nodes"], arax_graph["edges"] = list(arax_nodes_map.values()), arax_edges
                logger.info(f"‚úÖ ARAX returned {len(arax_graph['nodes'])} nodes and {len(arax_graph['edges'])} edges.")

    psg = _build_patient_state_graph(state)
    logger.info("   - Merging graphs from all sources...")
    
    merged_nodes = local_graph.get("nodes", []) + arax_graph.get("nodes", []) + psg.get("nodes", [])
    merged_edges = local_graph.get("edges", []) + arax_graph.get("edges", []) + psg.get("edges", [])
    
    # Kh·ª≠ tr√πng l·∫∑p node theo ID
    final_node_map = {}
    for n in merged_nodes:
        if n.get('id'):
            final_node_map[n['id']] = n
            
    valid_node_ids = set(final_node_map.keys())
    
    # Ch·ªâ gi·ªØ l·∫°i c·∫°nh n√†o m√† c·∫£ 2 ƒë·∫ßu ƒë·ªÅu t·ªìn t·∫°i trong danh s√°ch node
    final_edges = [e for e in merged_edges if e.get('source') in valid_node_ids and e.get('target') in valid_node_ids]
    
    state.graph_refs["ckg_subgraph"] = {"nodes": list(final_node_map.values()), "edges": final_edges}
    
    arax_was_used = use_arax_fallback and bool(arax_graph.get("edges"))
    state.log("4_RETRIEVAL", "SUCCESS", metadata={"nodes": len(final_node_map), "edges": len(final_edges), "arax_used": arax_was_used})
    
    return state

===== .\src\modules\step5_reasoning.py =====
import logging
import torch
import numpy as np
from sentence_transformers import SentenceTransformer
from torch_geometric.data import HeteroData
from pathlib import Path
from src.core.state import MedCOTState
from src.models.dual_tower_gnn import CoGCoT_DualTower_GNN

# --- C·∫§U H√åNH LOGGING ƒê·ªÇ T·∫ÆT R√ÅC ---
# T·∫Øt log DEBUG c·ªßa PyRuSH v√† c√°c th∆∞ vi·ªán kh√°c ƒë·ªÉ log g·ªçn g√†ng
logger = logging.getLogger("step5_dual_tower")
# ------------------------------------

_encoder = None
GNN_MODEL_PATH = Path("models/gnn_dual_tower_weights.pth")

def load_encoder():
    global _encoder
    if _encoder is None: 
        logger.info("loading sentence transformer...")
        _encoder = SentenceTransformer("all-MiniLM-L6-v2") 
    return _encoder

def _prepare_hetero_data_robust(nodes, edges, encoder):
    """
    H√†m n√†y t·∫°o data tr√™n CPU, ta s·∫Ω chuy·ªÉn l√™n GPU sau.
    """
    data = HeteroData()
    if not nodes: return data, {}

    # Group nodes and create embeddings (tr√™n CPU)
    grouped_nodes = {}
    node_id_to_idx = {} 
    
    for n in nodes:
        lbl = n.get("label", "Unknown").replace("/", "_").replace(" ", "_")
        if lbl not in grouped_nodes: grouped_nodes[lbl] = []
        current_idx = len(grouped_nodes[lbl])
        node_id_to_idx[n['id']] = (lbl, current_idx)
        grouped_nodes[lbl].append(n)

    for lbl, nlist in grouped_nodes.items():
        texts = [n.get("name", "Unknown") for n in nlist]
        embs = encoder.encode(texts, show_progress_bar=False)
        data[lbl].x = torch.tensor(embs, dtype=torch.float32)

    # Process Edges
    edge_index_map = {}
    for e in edges:
        s_id, t_id = e["source"], e["target"]
        if s_id not in node_id_to_idx or t_id not in node_id_to_idx:
            continue
        s_lbl, s_idx = node_id_to_idx[s_id]
        t_lbl, t_idx = node_id_to_idx[t_id]
        e_type = e.get("type", "RELATED").upper()
        triplet = (s_lbl, e_type, t_lbl)
        if triplet not in edge_index_map:
            edge_index_map[triplet] = [[], []]
        edge_index_map[triplet][0].append(s_idx)
        edge_index_map[triplet][1].append(t_idx)

    for triplet, indices in edge_index_map.items():
        if len(indices[0]) > 0:
            data[triplet].edge_index = torch.tensor(indices, dtype=torch.long)
    
    legacy_node_map = {}
    for lbl, nlist in grouped_nodes.items():
        legacy_node_map[lbl] = {n['id']: i for i, n in enumerate(nlist)}

    return data, legacy_node_map

def run(state: MedCOTState, num_think_steps: int = 2) -> MedCOTState:
    ug = state.graph_refs.get("ckg_subgraph")
    if not ug or not ug.get("nodes"):
        state.log("5_REASONING", "SKIPPED", "No subgraph")
        return state

    encoder = load_encoder()
    
    # --- S·ª¨A L·ªñI DEVICE ---
    # 1. X√°c ƒë·ªãnh thi·∫øt b·ªã ƒë√≠ch (GPU n·∫øu c√≥, kh√¥ng th√¨ CPU)
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"GNN running on device: {device}")
    
    # 2. T·∫°o Tensors (m·∫∑c ƒë·ªãnh tr√™n CPU ho·∫∑c GPU)
    q_emb = encoder.encode(state.normalized_query, convert_to_tensor=True) if state.normalized_query else torch.zeros(384)

    ckg_nodes = [n for n in ug["nodes"] if n.get("provenance") != "PSG"]
    psg_nodes = [n for n in ug["nodes"] if n.get("provenance") == "PSG"]
    ckg_edges = [e for e in ug["edges"] if e.get("provenance") != "PSG"]
    psg_edges = [e for e in ug["edges"] if e.get("provenance") == "PSG"]

    ckg_d, ckg_m = _prepare_hetero_data_robust(ckg_nodes, ckg_edges, encoder)
    psg_d, psg_m = _prepare_hetero_data_robust(psg_nodes, psg_edges, encoder)

    # 3. Chuy·ªÉn t·∫•t c·∫£ m·ªçi th·ª© l√™n c√πng m·ªôt device
    q_emb = q_emb.to(device)
    ckg_d = ckg_d.to(device)
    psg_d = psg_d.to(device)
    # -----------------------

    state.graph_refs["node_map"] = ckg_m

    if not ckg_d.node_types: 
        state.log("5_REASONING", "SKIPPED", "Empty CKG Data")
        return state

    try:
        # Load model v√† chuy·ªÉn n√≥ l√™n device
        model = CoGCoT_DualTower_GNN(ckg_d.metadata(), psg_d.metadata(), 128, 384, num_think_steps)
        model.to(device) # <--- QUAN TR·ªåNG
        
        if GNN_MODEL_PATH.exists(): 
            try:
                # Load weights v√†o model ƒë√£ ·ªü tr√™n GPU
                model.load_state_dict(torch.load(GNN_MODEL_PATH, map_location=device), strict=False)
            except Exception as load_err:
                logger.warning(f"Could not load GNN weights: {load_err}")
        
        model.eval()
        with torch.no_grad():
            # B√¢y gi·ªù t·∫•t c·∫£ input v√† model ƒë·ªÅu ·ªü tr√™n GPU
            final_x, thoughts = model(ckg_d, psg_d, q_emb)

        # Chuy·ªÉn k·∫øt qu·∫£ v·ªÅ l·∫°i CPU ƒë·ªÉ l∆∞u tr·ªØ (numpy/json kh√¥ng ƒë·ªçc ƒë∆∞·ª£c tensor GPU)
        final_node_embeddings = {}
        for nt, feat in final_x.items():
            final_node_embeddings[nt] = feat.cpu().numpy()
        state.graph_refs["final_node_embeddings"] = final_node_embeddings
            
        state.gcot["thought_vectors"] = thoughts
        state.log("5_REASONING", "SUCCESS", {"thoughts": len(thoughts)})

    except Exception as e:
        logger.error(f"GNN Error handled gracefully: {e}", exc_info=True) # exc_info=True ƒë·ªÉ in traceback
        state.log("5_REASONING", "FAILED_BUT_CONTINUED", str(e))

    return state

===== .\src\modules\step6_path_generation.py =====
# src/modules/step6_path_generation.py
import logging
import numpy as np
from sentence_transformers import SentenceTransformer, util, CrossEncoder
from src.core.state import MedCOTState

logging.basicConfig(level=logging.INFO, format='%(levelname)s:%(name)s:%(message)s')
logger = logging.getLogger("step6_constrained_path_gen")
_models = {}

# --- N√ÇNG C·∫§P THEO CH·ªêT 6: R√ÄNG BU·ªòC T√åM KI·∫æM THEO NG·ªÆ NGHƒ®A ---
SEMANTIC_CONSTRAINTS = {
    "TREATMENT": ["indication", "treats", "prevents", "mitigates"],
    "SAFETY": ["contraindication", "side effect", "adverse reaction", "risk_of", "causes", "interacts_with"],
    "DIAGNOSIS": ["biomarker", "associated_with", "has_symptom", "presents_with"],
    "GENERIC": [] # Ch·∫ø ƒë·ªô Fallback kh√¥ng c√≥ r√†ng bu·ªôc
}

def load_models():
    if not _models:
        _models["embedder"] = SentenceTransformer("all-MiniLM-L6-v2")
        _models["reranker"] = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")
    return _models["embedder"], _models["reranker"]

def detect_query_intent(query: str) -> str:
    q = query.lower()
    if any(w in q for w in ["treat", "cure", "therapy", "manage", "medication", "drug for"]): return "TREATMENT"
    if any(w in q for w in ["safe", "risk", "contraindicat", "bad", "interaction", "warn", "avoid"]): return "SAFETY"
    if any(w in q for w in ["diagnos", "test", "check", "symptom", "sign", "cause"]): return "DIAGNOSIS"
    return "GENERIC"

class ConstrainedPathGenerator:
    def __init__(self, state: MedCOTState, embedder):
        self.state = state
        self.embedder = embedder
        self.query_emb = embedder.encode(state.normalized_query, convert_to_tensor=True) if state.normalized_query else None
        self.intent = detect_query_intent(state.normalized_query)
        self.meta = {n['id']: n for n in state.graph_refs.get("ckg_subgraph", {}).get("nodes", [])}
        self.adj = self._build_adj(strict_mode=True)
        self.used_fallback = False

    def _build_adj(self, strict_mode=True):
        adj, edges = {}, self.state.graph_refs.get("ckg_subgraph", {}).get("edges", [])
        allowed = SEMANTIC_CONSTRAINTS.get(self.intent, []) if strict_mode else []
        
        valid_count = 0
        for e in edges:
            s, t, raw_type = e["source"], e["target"], e["type"].lower()
            if strict_mode and self.intent != "GENERIC" and allowed and not any(valid in raw_type for valid in allowed): continue
            valid_count += 1
            info = {"node": t, "edge_raw": e["type"], "edge_text": raw_type.replace("_", " "), "provenance": e.get("provenance", "DEFAULT")}
            adj.setdefault(s, []).append(info)
        
        logger.info(f"üï∏ Adj built (Strict={strict_mode}, Intent={self.intent}). Valid edges: {valid_count}/{len(edges)}")
        return adj

    def enable_fallback(self):
        logger.warning("‚ö†Ô∏è No paths found with strict constraints. Switching to GENERIC mode.")
        self.intent = "GENERIC"
        self.adj = self._build_adj(strict_mode=False)
        self.used_fallback = True

    def search(self, width=50, depth=3):
        if self.query_emb is None or not self.state.seed_nodes: return []
        seeds = [s for s in self.state.seed_nodes if s in self.adj]
        if not seeds: return []
        
        beam = [(0.0, [{"node_id": s}]) for s in seeds]
        final = []
        
        for _ in range(depth): # Max path length = depth
            candidates = []
            for score, path in beam:
                curr_node_id = path[-1]['node_id']
                if len(path) > 1: final.append((score, path))
                
                neighbors = self.adj.get(curr_node_id, [])
                current_path_nodes = {step['node_id'] for step in path}
                valid_neighbors = [n for n in neighbors if n['node'] not in current_path_nodes] # Cycle Control
                if not valid_neighbors: continue
                
                texts = [f"{n['edge_text']} {self.meta.get(n['node'], {}).get('name', '')}" for n in valid_neighbors]
                sem_sims = util.cos_sim(self.query_emb, self.embedder.encode(texts, convert_to_tensor=True))[0].cpu().numpy()
                
                for i, nb in enumerate(valid_neighbors):
                    new_step = {"node_id": nb['node'], "edge_raw": nb["edge_raw"], "edge_text": nb["edge_text"], "provenance": nb["provenance"]}
                    candidates.append((score + float(sem_sims[i]), path + [new_step]))
            
            if not candidates: break
            candidates.sort(key=lambda x: x[0], reverse=True)
            beam = candidates[:width]
        
        results, seen_paths = [], set()
        for score, path in sorted(final + beam, key=lambda x: x[0], reverse=True):
            if len(path) < 2: continue
            clean_path, parts = [], []
            for i in range(len(path) - 1):
                s_name = self.meta.get(path[i]['node_id'], {}).get('name', 'Unknown')
                t_name = self.meta.get(path[i+1]['node_id'], {}).get('name', 'Unknown')
                step_info = {"source": path[i]['node_id'], "target": path[i+1]['node_id'], "edge": path[i+1]['edge_raw'], "edge_text": path[i+1]['edge_text'], "provenance": path[i+1]['provenance']}
                clean_path.append(step_info)
                parts.append(f"{s_name} --[{step_info['edge_text']}]--> {t_name}")
            
            text_repr = " ".join(parts)
            if text_repr not in seen_paths:
                seen_paths.add(text_repr)
                results.append({"path": clean_path, "text_repr": text_repr, "score": float(score)})
        return results[:width]

def run(state: MedCOTState, beam_width: int = 50, max_path_length: int = 3) -> MedCOTState: # Max hops = 3-1 = 2
    try:
        embedder, reranker = load_models()
        gen = ConstrainedPathGenerator(state, embedder)
        
        paths = gen.search(width=beam_width, depth=max_path_length)
        if not paths:
            gen.enable_fallback()
            paths = gen.search(width=beam_width, depth=max_path_length)
        
        if not paths: 
            state.log("6_PATH_GEN", "SKIPPED", {"msg": "No paths found even with fallback"})
            return state
        
        path_texts = [[state.normalized_query, p["text_repr"]] for p in paths]
        scores = reranker.predict(path_texts)
        for i, p in enumerate(paths): 
            p['final_score'] = 0.3 * p['score'] + 0.7 * (1 / (1 + np.exp(-scores[i])))
        
        state.candidate_paths = sorted(paths, key=lambda x: x['final_score'], reverse=True)[:10]
        state.log("6_PATH_GEN", "SUCCESS", {"count": len(state.candidate_paths), "intent": gen.intent, "fallback_used": gen.used_fallback})
        
    except Exception as e:
        logger.exception("Path Gen Error")
        state.log("6_PATH_GEN", "FAILED", str(e))
    return state

===== .\src\modules\step7_verification.py =====
# src/modules/step7_verification.py
import logging
import numpy as np
import torch
from sentence_transformers import CrossEncoder
from pathlib import Path

from src.core.state import MedCOTState
from src.core import config
from src.models.verifier import MultiSignalVerifier

logging.basicConfig(level=logging.INFO, format='%(levelname)s:%(name)s:%(message)s')
logger = logging.getLogger("step7_verification_provenance")
_resources = {}
VERIFIER_MODEL_PATH = Path("models/verifier_weights.pth")

# --- N√ÇNG C·∫§P THEO CH·ªêT 7: TH·ª® T·ª∞ ∆ØU TI√äN NGU·ªíN G·ªêC ---
PROVENANCE_SCORES = {
    "PSG": 1.0,           # Patient State Graph (S·ª± th·∫≠t c·ªßa ca b·ªánh)
    "PrimeKG": 0.85,      # KG Local, ƒë√£ ƒë∆∞·ª£c curate
    "ARAX/KG2": 0.6,      # KG t·ª´ xa, ngu·ªìn ƒëa d·∫°ng
    "User_Upload": 0.9,   # D·ªØ li·ªáu do ng∆∞·ªùi d√πng n·∫°p, ∆∞u ti√™n
    "DEFAULT": 0.3        # M·∫∑c ƒë·ªãnh n·∫øu kh√¥ng c√≥ ngu·ªìn
}

def load_resources():
    global _resources
    if _resources: return _resources
    _resources['nli_model'] = CrossEncoder(config.NLI_MODEL_NAME)
    # --- N√ÇNG C·∫§P: INPUT_DIM TƒÇNG T·ª™ 7 L√äN 8 ƒê·ªÇ TH√äM PROVENANCE ---
    _resources['verifier_model'] = MultiSignalVerifier(input_dim=8)
    if VERIFIER_MODEL_PATH.exists():
        _resources['verifier_model'].load_state_dict(torch.load(VERIFIER_MODEL_PATH))
    _resources['verifier_model'].eval()
    return _resources

def _get_node_meta(node_id, state):
    for node in state.graph_refs.get("ckg_subgraph", {}).get("nodes", []):
        if node["id"] == node_id: return node
    return None

def _extract_path_features(path, state, nli_model):
    path_features = []
    node_map = {n['id']: n for n in state.graph_refs.get("ckg_subgraph", {}).get("nodes", [])}
    
    for step in path:
        src_meta = node_map.get(step['source'])
        tgt_meta = node_map.get(step['target'])
        if not src_meta or not tgt_meta: continue

        step_text = f"{src_meta['name']} {step.get('edge_text', step['edge'])} {tgt_meta['name']}"
        try:
            scores = nli_model.predict([(state.normalized_query, step_text)])
            probs = torch.softmax(torch.tensor(scores), dim=-1)
            nli_score = float(probs[0][-1]) # L·∫•y ƒëi·ªÉm c·ªßa "entailment"
        except Exception: nli_score = 0.5

        # --- N√ÇNG C·∫§P: TH√äM T√çN HI·ªÜU PROVENANCE V√ÄO VECTOR ---
        provenance_score = PROVENANCE_SCORES.get(step.get("provenance", "DEFAULT"), 0.3)
        
        # [nli, gcot, in_kg, causality, len, src_deg, tgt_deg, provenance]
        features = [ nli_score, 0.5, 1.0, 0.5, len(path), 1, 1, provenance_score ]
        path_features.append(features)
        
    return np.mean(path_features, axis=0) if path_features else None

def run(state: MedCOTState) -> MedCOTState:
    if not state.candidate_paths:
        state.reasoning_mode = "Abstain"
        return state

    resources = load_resources()
    path_vectors, valid_candidates = [], []
    for cand in state.candidate_paths:
        feats = _extract_path_features(cand['path'], state, resources['nli_model'])
        if feats is not None:
            path_vectors.append(feats)
            valid_candidates.append(cand)
            
    if not path_vectors:
        state.reasoning_mode = "Abstain"
        return state

    with torch.no_grad():
        logits = resources['verifier_model'](torch.tensor(np.array(path_vectors), dtype=torch.float32))
        confidences = torch.sigmoid(logits).squeeze().cpu().numpy()
        if np.ndim(confidences) == 0: confidences = [float(confidences)]

    for i, cand in enumerate(valid_candidates):
        cand['verification_confidence'] = float(confidences[i])

    verified_results = sorted(valid_candidates, key=lambda x: x['verification_confidence'], reverse=True)
    
    if verified_results and verified_results[0]['verification_confidence'] > 0.5:
        best = verified_results[0]
        state.verified_path = best['path']
        state.global_confidence = best['verification_confidence']
        state.reasoning_mode = "Graph-Strict" if state.global_confidence > 0.8 else "Cautious"
        state.gcot['verified_path_text'] = best.get('text_repr', '')
    else:
        state.reasoning_mode = "Abstain"
        
    state.log("7_VERIFICATION", "SUCCESS", {"mode": state.reasoning_mode, "conf": state.global_confidence})
    return state

===== .\src\modules\step8_synthesis.py =====
import logging
import os
import re
from src.core.state import MedCOTState
# N√ÇNG C·∫§P: Import umls_service ƒë·ªÉ c√≥ th·ªÉ g·ªçi h√†m l·∫•y ƒë·ªãnh nghƒ©a
from src.utils.umls_normalizer import umls_service
from src.utils.local_llm import local_llm # Gi·∫£ ƒë·ªãnh d√πng Local LLM

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("step8_synthesis")

def clean_llm_output(text: str) -> str:
    """Lo·∫°i b·ªè c√°c th·∫ª <think> v√† c√°c th·∫ª XML kh√°c kh·ªèi output c·ªßa LLM."""
    if not text: return ""
    text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)
    text = re.sub(r'<[^>]+>', '', text, flags=re.DOTALL)
    return text.strip()

def run(state: MedCOTState) -> MedCOTState:
    # --- 1. T·ªïng h·ª£p b·∫±ng ch·ª©ng t·ª´ GRAPH (Gi·ªØ nguy√™n) ---
    evidence_lines = []
    node_map = {n['id']: n.get('name', n['id']) for n in state.graph_refs.get("ckg_subgraph", {}).get("nodes", [])}

    if state.verified_path:
        evidence_lines.append("‚úÖ **Verified Reasoning Path:**")
        for step in state.verified_path:
            s_name = node_map.get(step['source'], step['source'])
            t_name = node_map.get(step['target'], step['target'])
            rel = step.get('edge_text', step.get('edge', 'related_to'))
            evidence_lines.append(f"- {s_name} --[{rel}]--> {t_name}")
    elif state.candidate_paths:
        evidence_lines.append("‚öñÔ∏è **Potential Graph Connections:**")
        for p in state.candidate_paths[:5]:
            evidence_lines.append(f"- {p.get('text_repr', '')} (Score: {p.get('final_score', 0):.2f})")

    graph_evidence = "\n".join(evidence_lines)

    # ==============================================================================
    # N√ÇNG C·∫§P: L·∫•y ƒë·ªãnh nghƒ©a t·ª´ UMLS ƒë·ªÉ l√†m gi√†u ng·ªØ c·∫£nh cho LLM
    # ==============================================================================
    context_definitions = []
    processed_names = set()
    
    # ƒê·∫£m b·∫£o umls_service ƒë√£ k·∫øt n·ªëi
    umls_service.connect()
    
    for le in state.linked_entities:
        if le.link_status == "linked" and le.best_candidate.preferred_name not in processed_names:
            name = le.best_candidate.preferred_name
            processed_names.add(name)
            
            # D√πng t√™n ƒë√£ link ƒë·ªÉ t√¨m l·∫°i CUI chu·∫©n nh·∫•t
            norm_results = umls_service.normalize(name, top_k=1)
            if norm_results:
                cui = norm_results[0].get('cui')
                definition = umls_service.get_definition(cui)
                if definition:
                    context_definitions.append(f"- **{name}:** {definition}")
    
    context_evidence = ""
    if context_definitions:
        context_evidence = "\n\n**Additional Context from Medical Dictionary:**\n" + "\n".join(context_definitions)
    # ==============================================================================

    # Gh√©p n·ªëi t·∫•t c·∫£ b·∫±ng ch·ª©ng
    final_evidence_text = graph_evidence + context_evidence
    state.gcot['compiled_cot'] = final_evidence_text

    if not final_evidence_text.strip():
        state.final_answer = "I could not find sufficient evidence in the Knowledge Graph to answer this question."
        state.log("8_SYNTHESIS", "SKIPPED", "No evidence found")
        return state

    # --- 2. X√¢y d·ª±ng PROMPT ƒë√£ ƒë∆∞·ª£c l√†m gi√†u ---
    prompt = f"""
    You are a medical AI assistant. Your task is to summarize the evidence into a direct answer.

    **USER QUESTION:** 
    {state.normalized_query}

    **PROVIDED EVIDENCE:**
    {final_evidence_text}

    **INSTRUCTIONS:**
    1.  Answer the user's question directly based **ONLY** on the "PROVIDED EVIDENCE".
    2.  If the evidence lists treatments, state them clearly.
    3.  If the evidence lists contraindications or warnings, state them first.
    4.  Do not add any information not present in the evidence.
    5.  Keep the answer concise and to the point.

    **Final Answer:**
    """

    # --- 3. TH·ª∞C THI (S·ª≠ d·ª•ng Local LLM) ---
    raw_answer = ""
    try:
        logger.info("‚ö° Using Local LLM for synthesis with enriched context...")
        raw_answer = local_llm.generate_cot(prompt)
        state.final_answer = clean_llm_output(raw_answer)
        state.log("8_SYNTHESIS", "SUCCESS", {"model_used": "Local-LLM", "context_enriched": bool(context_definitions)})
    except Exception as e:
         logger.error(f"‚ùå Local LLM failed: {e}", exc_info=True)
         state.final_answer = f"**Raw Evidence found:**\n\n{final_evidence_text}"
         state.log("8_SYNTHESIS", "FAILED", {"error": str(e)})

    return state

===== .\src\modules\step9_safety.py =====
# T·ªáp: src/modules/step9_safety.py (Phi√™n b·∫£n cu·ªëi c√πng, d·ª±a tr√™n ID t·ª´ seed_nodes)
import logging
from src.core.state import MedCOTState
from src.utils.neo4j_connect import db_connector

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("step9_safety")

def run(state: MedCOTState) -> MedCOTState:
    # --- S·ª¨A ƒê·ªîI D·ª®T ƒêI·ªÇM: S·ª¨ D·ª§NG state.seed_nodes L√Ä NGU·ªíN ID DUY NH·∫§T ---
    # state.seed_nodes ƒë√£ ƒë∆∞·ª£c step4 c·∫≠p nh·∫≠t v√† ch·ª©a T·∫§T C·∫¢ c√°c ID li√™n quan (c·∫£ n·ªôi b·ªô v√† b√™n ngo√†i).
    query_entity_ids = set(state.seed_nodes)
    # --------------------------------------------------------------------
    
    # X√≥a c√°c safety flags c≈© ƒë·ªÉ ch·∫°y l·∫°i logic, tr√°nh ghi ƒë√® sai
    state.safety_flags = []
    
    subgraph = state.graph_refs.get("ckg_subgraph", {})
    edges = subgraph.get("edges", [])
    nodes = {n['id']: n.get('name', 'Unknown') for n in subgraph.get("nodes", [])}

    logger.info(f"üõ°Ô∏è Safety Check on {len(edges)} retrieved edges. Focusing on interactions between IDs: {query_entity_ids}")

    risk_keywords = ["INTERACT", "CONTRAINDICAT", "ADVERSE", "RISK", "SIDE_EFFECT", "AFFECTS"]
    
    direct_alerts = set()
    for edge in edges:
        rel_type = edge.get("type", "").upper()
        if any(risk in rel_type for risk in risk_keywords):
            source_id = edge.get('source')
            target_id = edge.get('target')
            
            # So s√°nh tr·ª±c ti·∫øp b·∫±ng ID
            if source_id in query_entity_ids and target_id in query_entity_ids:
                src_name = nodes.get(source_id, str(source_id))
                tgt_name = nodes.get(target_id, str(target_id))
                
                # B·ªè qua c√°c t∆∞∆°ng t√°c kh√¥ng c√≥ √Ω nghƒ©a (v√≠ d·ª•: Aspirin RELATED_TO Aspirin)
                if src_name.lower() == tgt_name.lower():
                    continue

                sorted_pair = tuple(sorted((src_name, tgt_name)))
                alert_msg = f"Direct Interaction Detected: {sorted_pair[0]} --[{rel_type}]--> {sorted_pair[1]}"
                direct_alerts.add(alert_msg)

    all_alerts = list(direct_alerts)

    # Fallback: N·∫øu kh√¥ng c√≥ t∆∞∆°ng t√°c tr·ª±c ti·∫øp, t√¨m c·∫£nh b√°o chung
    if not all_alerts:
        logger.info("No direct interactions found. Looking for general contraindications for query entities.")
        other_alerts = []
        for edge in edges:
            rel_type = edge.get("type", "").upper()
            if "CONTRAINDICATION" in rel_type:
                 source_id = edge.get('source')
                 target_id = edge.get('target')
                 if source_id in query_entity_ids or target_id in query_entity_ids:
                    src_name = nodes.get(source_id, str(source_id))
                    tgt_name = nodes.get(target_id, str(target_id))
                    other_alerts.append(f"General Warning: {src_name} --[{rel_type}]--> {tgt_name}")
        all_alerts.extend(other_alerts[:5])

    # G√°n c·ªù an to√†n v√† ch√®n v√†o c√¢u tr·∫£ l·ªùi cu·ªëi c√πng
    if all_alerts:
        state.reasoning_mode = "Safety-Alert"
        state.safety_flags = [{"type": "CLINICAL_RISK", "msg": msg} for msg in all_alerts]
        
        warning_block = "**üö® SAFETY WARNING:**\n" + "\n".join(f"- {msg}" for msg in all_alerts)
        if state.final_answer:
            # Ch·ªâ ch√®n v√†o n·∫øu n√≥ ch∆∞a t·ªìn t·∫°i ƒë·ªÉ tr√°nh l·∫∑p l·∫°i
            if warning_block not in state.final_answer:
                state.final_answer = warning_block + "\n\n" + state.final_answer
        else:
            state.final_answer = warning_block

    state.log("9_SAFETY", "SUCCESS", {"alerts": len(all_alerts)})
    return state

===== .\src\modules\__init__.py =====


===== .\src\utils\arax_client.py =====
# T·ªáp: src/utils/arax_client.py (PHI√äN B·∫¢N RETRY + T·ªêI ∆ØU)
import requests
import logging
import json
import time
import hashlib
from pathlib import Path
from itertools import combinations

logger = logging.getLogger("ARAX_CLIENT")
ARAX_BASE_URL = "https://arax.ncats.io/api/arax/v1.4"

CACHE_DIR = Path(".cache/arax_queries")
CACHE_DIR.mkdir(parents=True, exist_ok=True)
CACHE_EXPIRATION_SECONDS = 86400 * 7

class ARAXClient:
    def __init__(self):
        self.headers = {"accept": "application/json", "Content-Type": "application/json"}
        self.curie_cache = {}

    def resolve_names_to_curies(self, names: list) -> list:
        # (Gi·ªØ nguy√™n logic resolve name v√¨ ƒë√£ ho·∫°t ƒë·ªông t·ªët)
        unique_names = list(set([n for n in names if n and n not in self.curie_cache]))
        if unique_names:
            try:
                params = [('q', name) for name in unique_names]
                response = requests.get(f"{ARAX_BASE_URL}/entity", params=params, timeout=30)
                if response.status_code == 200:
                    data = response.json()
                    if isinstance(data, list):
                        for item in data:
                            label = item.get('label') or item.get('name')
                            curie = item.get('id')
                            if label and curie:
                                self._cache_result(label, curie)
                                for input_name in unique_names:
                                    if input_name.lower() == label.lower():
                                        self._cache_result(input_name, curie)
                    elif isinstance(data, dict):
                        for key, val in data.items():
                            self._cache_result(key, val)
            except Exception as e:
                logger.error(f"Error resolving names via ARAX: {e}")

        results = []
        for name in names:
            curie = self.curie_cache.get(name) or self.curie_cache.get(name.lower())
            if curie: results.append(curie)
        return list(set(results))

    def _cache_result(self, name, result):
        curie = None
        if isinstance(result, str): curie = result
        elif isinstance(result, dict): curie = result.get('identifier') or result.get('id') or result.get('curie')
        if curie and isinstance(curie, str):
            self.curie_cache[name] = curie
            self.curie_cache[name.lower()] = curie

    def _get_cache_key(self, identifiers):
        key_str = "arax_v1.4_optimized_" + "_".join(sorted([str(i).lower() for i in identifiers]))
        return hashlib.md5(key_str.encode()).hexdigest()

    def query_kg2(self, identifiers: list, max_results=5):
        if len(identifiers) < 2: return []
            
        cache_key = self._get_cache_key(identifiers)
        cache_file = CACHE_DIR / f"{cache_key}.json"

        if cache_file.exists() and time.time() - cache_file.stat().st_mtime < CACHE_EXPIRATION_SECONDS:
            logger.info(f"‚ö° [Cache Hit] Loading ARAX connecting path for {identifiers}")
            try:
                with open(cache_file, 'r', encoding='utf-8') as f: return json.load(f)
            except Exception: pass

        logger.info(f"üåç Querying ARAX (v1.4) for interactions between: {identifiers}...")
        
        id_pairs = list(combinations(identifiers, 2))
        all_results = []
        
        for id1, id2 in id_pairs:
            # --- T·ªêI ∆ØU H√ìA QUERY ---
            # Ch·ªâ t√¨m c√°c c·∫°nh c√≥ √Ω nghƒ©a t∆∞∆°ng t√°c thu·ªëc ƒë·ªÉ gi·∫£m t·∫£i server
            payload = {
                "message": {
                    "query_graph": {
                        "nodes": {
                            "n0": {"ids": [id1]},
                            "n1": {"ids": [id2]}
                        },
                        "edges": {
                            "e0": {
                                "subject": "n0", 
                                "object": "n1",
                                # Ch·ªâ t√¨m t∆∞∆°ng t√°c (interacts_with) ho·∫∑c li√™n quan (related_to)
                                # ƒêi·ªÅu n√†y gi√∫p query ch·∫°y NHANH H∆†N v√† √≠t b·ªã l·ªói 503
                                "predicates": [
                                    "biolink:interacts_with",
                                    "biolink:affects",
                                    "biolink:related_to"
                                ]
                            }
                        }
                    }
                },
                "max_results": max_results,
                "submitter": "MedCOT_Agent"
            }

            # --- C∆† CH·∫æ RETRY (TH·ª¨ L·∫†I) ---
            max_retries = 3
            for attempt in range(max_retries):
                try:
                    response = requests.post(f"{ARAX_BASE_URL}/query", headers=self.headers, json=payload, timeout=120)
                    
                    if response.status_code == 200:
                        data = response.json()
                        if "message" in data and "knowledge_graph" in data["message"]:
                            kg = data["message"]["knowledge_graph"]
                            if kg:
                                parsed_edges = self._parse_trapi_to_medcot(kg.get("nodes", {}), kg.get("edges", {}))
                                all_results.extend(parsed_edges)
                                logger.info(f"  -> Found {len(parsed_edges)} interaction edges between '{id1}' and '{id2}'.")
                            else:
                                logger.info(f"  -> ARAX returned 0 paths (Graph empty) for '{id1}' and '{id2}'.")
                        break # Th√†nh c√¥ng th√¨ tho√°t v√≤ng l·∫∑p retry
                    
                    elif response.status_code == 503:
                        logger.warning(f"‚ö†Ô∏è ARAX Server busy (503). Retrying {attempt+1}/{max_retries} in 5s...")
                        time.sleep(5) # ƒê·ª£i 5 gi√¢y r·ªìi th·ª≠ l·∫°i
                    else:
                        logger.warning(f"ARAX query failed {response.status_code}: {response.text[:200]}")
                        break # L·ªói kh√°c 503 th√¨ kh√¥ng th·ª≠ l·∫°i

                except Exception as e:
                    logger.error(f"‚ùå Exception querying ARAX: {e}")
                    if attempt < max_retries - 1:
                        time.sleep(5)
                    else:
                        logger.error("‚ùå Max retries exceeded.")
        
        try:
            with open(cache_file, 'w', encoding='utf-8') as f: json.dump(all_results, f, ensure_ascii=False)
        except Exception: pass
            
        return all_results

    def _parse_trapi_to_medcot(self, nodes, edges):
        if not nodes or not edges: return []
        
        medcot_edges, node_map = [], {nid: n_info.get("name", nid) for nid, n_info in nodes.items()}
        for eid, e_info in edges.items():
            predicate = e_info.get("predicate", "related_to")
            if ":" in predicate: predicate = predicate.split(":")[-1]
            predicate = predicate.upper()

            primary_source = "ARAX/KG2"
            if "attributes" in e_info:
                for attr in e_info["attributes"]:
                    if attr.get("attribute_type_id") == "biolink:primary_knowledge_source":
                        primary_source = attr.get("value")
                        break

            medcot_edges.append({
                "source": node_map.get(e_info.get("subject"), e_info.get("subject")), 
                "target": node_map.get(e_info.get("object"), e_info.get("object")), 
                "type": predicate,
                "source_id": e_info.get("subject"), 
                "target_id": e_info.get("object"),
                "provenance": "ARAX/KG2", 
                "remote_source": primary_source
            })
        return medcot_edges

arax_client = ARAXClient()

===== .\src\utils\faiss_search.py =====
# src/utils/faiss_search.py
import logging
import json
from pathlib import Path
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np

logger = logging.getLogger("FAISS_SEARCH")

class FaissSearch:
    def __init__(self, index_dir: str = "data/kg_index", model_name: str = "BAAI/bge-small-en-v1.5"):
        self.index_dir = Path(index_dir)
        self.index_path = self.index_dir / "kg_faiss.index"
        self.meta_path = self.index_dir / "kg_nodes_meta.json"
        self.model_name = model_name
        
        self.index = None
        self.meta = None
        self.encoder = None
        self._load_resources()

    def _load_resources(self):
        if not self.index_path.exists() or not self.meta_path.exists():
            msg = f"FAISS index not found at {self.index_dir}. Please run 'scripts/2_build_faiss.py'."
            logger.error(msg)
            raise FileNotFoundError(msg)
        
        logger.info("Loading FAISS resources...")
        self.index = faiss.read_index(str(self.index_path))
        with open(self.meta_path, "r", encoding="utf-8") as f:
            self.meta = json.load(f)
        self.encoder = SentenceTransformer(self.model_name)
        logger.info(f"‚úÖ FAISS resources loaded ({self.index.ntotal} vectors).")

    def search(self, query_text: str, k: int = 5) -> list[dict]:
        """Searches the index and returns top-k metadata."""
        if self.index is None:
            raise RuntimeError("Index is not loaded.")
        
        query_vector = self.encoder.encode([query_text], normalize_embeddings=True)
        distances, indices = self.index.search(np.asarray(query_vector, dtype="float32"), k)
        
        results = []
        for i in indices[0]:
            if i != -1: # FAISS returns -1 for empty slots
                results.append(self.meta[i])
        return results

# Singleton instance for easy import
try:
    faiss_retriever = FaissSearch()
except FileNotFoundError:
    faiss_retriever = None

===== .\src\utils\local_llm.py =====
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import logging
import gc

logger = logging.getLogger("LOCAL_LLM")

# Model 1.5B t·ªëi ∆∞u cho 3050 Ti
MODEL_ID = "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"

class LocalCoTGenerator:
    _instance = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(LocalCoTGenerator, cls).__new__(cls)
            cls._instance.model = None
            cls._instance.tokenizer = None
        return cls._instance

    def load_model(self):
        """Load model v·ªõi c·∫•u h√¨nh t·ªëi ∆∞u VRAM"""
        if self.model is not None:
            return

        logger.info(f"‚è≥ Loading Local CoT Model tr√™n RTX 3050 Ti: {MODEL_ID}...")
        try:
            # C·∫•u h√¨nh 4-bit
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type='nf4'
            )

            # --- S·ª¨A L·∫†I: B·ªé force_download=True N·∫æU ƒê√É T·∫¢I XONG ---
            self.tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
            
            # --- FIX C·∫¢NH B√ÅO PAD TOKEN ---
            # Qwen/DeepSeek th∆∞·ªùng kh√¥ng c√≥ pad token m·∫∑c ƒë·ªãnh, ta g√°n n√≥ b·∫±ng EOS token
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
                self.tokenizer.pad_token_id = self.tokenizer.eos_token_id

            self.model = AutoModelForCausalLM.from_pretrained(
                MODEL_ID,
                quantization_config=bnb_config,
                device_map="auto",
                trust_remote_code=True
            )
            logger.info(f"‚úÖ Model ƒë√£ load! VRAM d·ª± ki·∫øn: ~1.5GB")
            
        except Exception as e:
            logger.critical(f"‚ùå L·ªói load model: {e}")
            raise e

    def generate_cot(self, prompt: str) -> str:
        if self.model is None:
            self.load_model()

        messages = [
            {"role": "system", "content": "You are a helpful AI assistant."},
            {"role": "user", "content": prompt}
        ]
        
        # Tokenize inputs
        input_ids = self.tokenizer.apply_chat_template(
            messages, 
            add_generation_prompt=True, 
            return_tensors="pt"
        ).to(self.model.device)

        # --- FIX C·∫¢NH B√ÅO ATTENTION MASK ---
        # T·∫°o mask: 1 cho token th·∫≠t, 0 cho padding (·ªü ƒë√¢y to√†n b·ªô l√† th·∫≠t v√¨ batch=1)
        attention_mask = torch.ones_like(input_ids)

        with torch.no_grad():
            outputs = self.model.generate(
                input_ids,
                attention_mask=attention_mask,    # Truy·ªÅn mask v√†o
                pad_token_id=self.tokenizer.pad_token_id, # Truy·ªÅn pad_token_id
                max_new_tokens=2048,
                temperature=0.6,
                do_sample=True,
                top_p=0.9
            )

        response = self.tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)
        return response.strip()

    def unload(self):
        """Gi·∫£i ph√≥ng VRAM"""
        if self.model is not None:
            del self.model
            del self.tokenizer
            self.model = None
            torch.cuda.empty_cache()
            gc.collect()
            logger.info("üóëÔ∏è ƒê√£ gi·∫£i ph√≥ng Model kh·ªèi GPU")

# Singleton
local_llm = LocalCoTGenerator()

===== .\src\utils\name_resolver.py =====
# T·ªáp: src/utils/name_resolver.py (PHI√äN B·∫¢N X·ª¨ L√ù LIST AN TO√ÄN)
import requests
import logging
from typing import List

logger = logging.getLogger("NAME_RESOLVER")
SRI_LOOKUP_URL = "https://name-resolution-sri.renci.org/lookup"

class NameResolver:
    def __init__(self):
        self.cache = {}

    def resolve_names_to_curies(self, names: List[str]) -> List[str]:
        unique_names = list(set([n.strip() for n in names if n.strip()]))
        resolved_curies = []

        for name in unique_names:
            if name.lower() in self.cache:
                val = self.cache[name.lower()]
                if val: resolved_curies.append(val)
                continue

            try:
                params = {"string": name, "limit": 1}
                response = requests.post(SRI_LOOKUP_URL, params=params, timeout=10)
                
                if response.status_code == 200:
                    data = response.json()
                    best_curie = None

                    # LOGIC X·ª¨ L√ù LIST/DICT LINH HO·∫†T
                    if isinstance(data, dict) and data:
                        # {"CURIE": "Label"}
                        best_curie = list(data.keys())[0]
                    elif isinstance(data, list) and len(data) > 0:
                        first = data[0]
                        if isinstance(first, dict):
                            # [{"curie": "...", ...}] HO·∫∂C [{"CURIE": "Label"}]
                            best_curie = first.get('curie') or first.get('id')
                            if not best_curie:
                                best_curie = list(first.keys())[0]
                        elif isinstance(first, str):
                            best_curie = first

                    if best_curie:
                        self.cache[name.lower()] = best_curie
                        resolved_curies.append(best_curie)
                        logger.info(f"‚úÖ SRI Resolved '{name}' -> {best_curie}")
                    else:
                        logger.warning(f"‚ùå No ID found for name: '{name}' (Response: {data})")
                        self.cache[name.lower()] = None
                else:
                    self.cache[name.lower()] = None
            except Exception as e:
                logger.error(f"Name Resolver failed for '{name}': {e}")
                self.cache[name.lower()] = None

        return list(set(resolved_curies))

name_resolver = NameResolver()

===== .\src\utils\neo4j_connect.py =====
# utils/neo4j_connect.py
import os
import time
from neo4j import GraphDatabase, Driver
from dotenv import load_dotenv

load_dotenv()

class Neo4jConnection:
    """
    Qu·∫£n l√Ω k·∫øt n·ªëi Neo4j v·ªõi c·∫•u h√¨nh Timeout cao h∆°n v√† Retry.
    """
    def __init__(self, uri, user, password):
        self._uri = uri
        self._user = user
        self._password = password
        self._driver: Driver = None
        self.connect()

    def connect(self):
        """Kh·ªüi t·∫°o driver v·ªõi c·∫•u h√¨nh m·∫°nh m·∫Ω h∆°n."""
        # N·∫øu driver ƒë√£ t·ªìn t·∫°i, kh√¥ng t·∫°o m·ªõi
        if self._driver is not None:
            return

        for i in range(3):
            try:
                self._driver = GraphDatabase.driver(
                    self._uri, 
                    auth=(self._user, self._password),
                    max_connection_lifetime=300,
                    keep_alive=True,
                    connection_acquisition_timeout=60,
                    connection_timeout=60
                )
                self._driver.verify_connectivity()
                print("‚úÖ K·∫øt n·ªëi Neo4j th√†nh c√¥ng!")
                return
            except Exception as e:
                print(f"‚ö†Ô∏è L·ªói k·∫øt n·ªëi l·∫ßn {i+1}: {e}. ƒêang th·ª≠ l·∫°i...")
                time.sleep(2)
        print("‚ùå Kh√¥ng th·ªÉ k·∫øt n·ªëi Neo4j sau 3 l·∫ßn th·ª≠.")

    def close(self):
        # --- FIX: Reset _driver v·ªÅ None sau khi ƒë√≥ng ---
        if self._driver is not None:
            self._driver.close()
            self._driver = None
            print("üîå K·∫øt n·ªëi Neo4j ƒë√£ ƒë√≥ng.")

    def run_query(self, query, parameters=None):
        if self._driver is None:
            self.connect()
            if self._driver is None: return []

        for attempt in range(3):
            try:
                with self._driver.session() as session:
                    result = session.run(query, parameters)
                    return list(result)
            except Exception as e:
                msg = str(e)
                if any(x in msg for x in ["ServiceUnavailable", "SessionExpired", "defunct", "Connection reset", "Closed"]):
                    print(f"‚ö†Ô∏è Connection drop detected ({msg}). Reconnecting ({attempt+1}/3)...")
                    self.close() # Reset driver
                    self.connect() # Re-init
                else:
                    print(f"‚ùå Query Error: {msg}")
                    raise e
        return []

# --- Singleton Instance ---
db_connector = None
try:
    db_uri = os.getenv("NEO4J_URI", "bolt://localhost:7687")
    db_user = os.getenv("NEO4J_USER", "neo4j")
    db_password = os.getenv("NEO4J_PASSWORD")

    if not db_password:
        print("‚ö†Ô∏è C·∫¢NH B√ÅO: Bi·∫øn m√¥i tr∆∞·ªùng NEO4J_PASSWORD ch∆∞a ƒë∆∞·ª£c thi·∫øt l·∫≠p.")
    
    db_connector = Neo4jConnection(uri=db_uri, user=db_user, password=db_password)
except Exception as e:
    print(f">> L·ªñI NGHI√äM TR·ªåNG: Kh√¥ng th·ªÉ kh·ªüi t·∫°o k·∫øt n·ªëi database. {e}")
    db_connector = None

===== .\src\utils\test_connection.py =====
# test_connection.py
import pandas as pd
# M·ªõi: Import th·∫≥ng ƒë·ªëi t∆∞·ª£ng connector ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o s·∫µn
from src.utils.neo4j_connect import db_connector

def main_test():
    """
    H√†m test ch√≠nh, s·ª≠ d·ª•ng connector ƒë√£ ƒë∆∞·ª£c t√°i c·∫•u tr√∫c.
    """
    # M·ªõi: Ki·ªÉm tra xem connector c√≥ ƒë∆∞·ª£c t·∫°o th√†nh c√¥ng kh√¥ng
    if db_connector is None:
        print("‚ùå Kh√¥ng th·ªÉ ch·∫°y test v√¨ k·∫øt n·ªëi database th·∫•t b·∫°i.")
        return

    print("\n--- B·∫Øt ƒë·∫ßu ch·∫°y test query ---")
    try:
        query = """
        MATCH (d:Disease)
        RETURN d.id AS ID, d.name AS Name
        LIMIT 5
        """
        # M·ªõi: Ch·∫°y query c·ª±c k·ª≥ ƒë∆°n gi·∫£n
        data = db_connector.run_query(query)
        
        if data:
            print(f"üìä T√¨m th·∫•y d·ªØ li·ªáu m·∫´u ({len(data)} records):")
            df = pd.DataFrame(data)
            print(df)
        else:
            print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y node :Disease n√†o.")
            
    except Exception as e:
        print(f"‚ùå L·ªói khi ƒëang ch·∫°y query: {e}")
    finally:
        # M·ªõi: ƒê√≥ng k·∫øt n·ªëi (quan tr·ªçng khi ·ª©ng d·ª•ng k·∫øt th√∫c)
        if db_connector:
            db_connector.close()

if __name__ == "__main__":
    main_test()



===== .\src\utils\umls_normalizer.py =====
# T·ªáp: src/utils/umls_normalizer.py (PHI√äN B·∫¢N CHU·∫®N ƒê·ªÇ S·ª¨ D·ª§NG)
import logging
import sqlite3
from pathlib import Path
from tqdm import tqdm
from functools import lru_cache

# --- CONFIG ---
SAB_RANKING = {
    "RXNORM": 1, "SNOMEDCT_US": 2, "NCI": 3, "MSH": 4,
    "HGNC": 5, "GO": 6, "MDR": 7
}
logger = logging.getLogger("UMLS_NORMALIZER")

class UMLSNormalizer:
    _instance = None
    
    def __new__(cls, db_path="data/umls/umls_lookup.db"):
        if cls._instance is None:
            cls._instance = super(UMLSNormalizer, cls).__new__(cls)
            cls._instance.db_path = Path(db_path)
            cls._instance.conn = None
        return cls._instance

    def connect(self):
        if self.conn is None and self.db_path.exists():
            try:
                self.conn = sqlite3.connect(self.db_path, check_same_thread=False)
                self.conn.row_factory = sqlite3.Row
                logger.info(f"‚úÖ ƒê√£ k·∫øt n·ªëi t·ªõi c∆° s·ªü d·ªØ li·ªáu UMLS t·∫°i: {self.db_path}")
            except sqlite3.Error as e:
                logger.error(f"‚ùå L·ªói k·∫øt n·ªëi SQLite: {e}")
                self.conn = None

    def disconnect(self):
        if self.conn:
            self.conn.close()
            self.conn = None

    @lru_cache(maxsize=1024)
    def normalize(self, text: str, target_stys: tuple = None, top_k: int = 5) -> list[dict]:
        if not self.conn: return []
        text_lower = text.lower()
        query = "SELECT a.cui, a.str, a.is_pref, a.sab, a.tty, s.sty FROM atoms a LEFT JOIN semantic_types s ON a.cui = s.cui WHERE a.str_lower = ?"
        params = [text_lower]
        if target_stys:
            placeholders = ','.join('?' for _ in target_stys)
            query += f" AND s.sty IN ({placeholders})"
            params.extend(target_stys)
        try:
            cursor = self.conn.cursor()
            cursor.execute(query, params)
            rows = cursor.fetchall()
        except sqlite3.Error: return []
        if not rows: return []
        cui_candidates = {}
        for row in rows:
            cui = row['cui']
            if cui not in cui_candidates: cui_candidates[cui] = {"cui": cui, "atoms": [], "stys": set()}
            cui_candidates[cui]["atoms"].append(dict(row))
            if row['sty']: cui_candidates[cui]["stys"].add(row['sty'])
        scored_results = []
        for cui, data in cui_candidates.items():
            best_atom = sorted(data['atoms'], key=lambda x: (-x['is_pref'], SAB_RANKING.get(x['sab'], 99)))[0]
            score = (best_atom['is_pref'] * 100) + (10 - SAB_RANKING.get(best_atom['sab'], 99))
            scored_results.append({"cui": cui, "pref_name": best_atom['str'], "stys": list(data['stys']), "sab": best_atom['sab'], "score": score})
        return sorted(scored_results, key=lambda x: x['score'], reverse=True)[:top_k]

    # ==============================================================================
    # N√ÇNG C·∫§P: Th√™m h√†m l·∫•y ƒë·ªãnh nghƒ©a t·ª´ b·∫£ng definitions
    # ==============================================================================
    @lru_cache(maxsize=1024)
    def get_definition(self, cui: str) -> str:
        """
        L·∫•y ƒë·ªãnh nghƒ©a c·ªßa m·ªôt CUI t·ª´ database.
        Tr·∫£ v·ªÅ chu·ªói ƒë·ªãnh nghƒ©a ho·∫∑c chu·ªói r·ªóng n·∫øu kh√¥ng t√¨m th·∫•y.
        """
        if not self.conn or not cui:
            return ""
        try:
            cursor = self.conn.cursor()
            # L·∫•y ƒë·ªãnh nghƒ©a t·ª´ ngu·ªìn ƒë√°ng tin c·∫≠y nh·∫•t (∆∞u ti√™n NCI)
            cursor.execute("""
                SELECT definition FROM definitions 
                WHERE cui = ? 
                ORDER BY CASE WHEN source = 'NCI' THEN 1 ELSE 2 END 
                LIMIT 1
            """, (cui,))
            row = cursor.fetchone()
            return row['definition'] if row else ""
        except sqlite3.Error as e:
            logger.error(f"L·ªói truy v·∫•n `get_definition` cho CUI {cui}: {e}")
            return ""

# Kh·ªüi t·∫°o singleton
umls_service = UMLSNormalizer()

===== .\src\utils\__init__.py =====


===== .\tests\test_full_pipeline.py =====
# tests/test_full_pipeline.py
import logging
import sys
import time
import os

# --- CRITICAL: CONFIGURE LOGGING FIRST ---
# Ph·∫£i ƒë·∫∑t tr∆∞·ªõc t·∫•t c·∫£ c√°c import kh√°c ƒë·ªÉ c√≥ hi·ªáu l·ª±c
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)],
    force=True  # Ghi ƒë√® m·ªçi c·∫•u h√¨nh logging ƒë√£ c√≥
)

# T·∫Øt ti·∫øng ·ªìn t·ª´ c√°c th∆∞ vi·ªán c·ª• th·ªÉ b·∫±ng c√°ch set level CRITICAL
for lib in ["PyRuSH", "presidio-analyzer", "medspacy", "urllib3", "sentence_transformers", "httpx", "httpcore", "hpack", "google.ai"]:
    logging.getLogger(lib).setLevel(logging.CRITICAL)
# ------------------------------------------

from src.core.state import MedCOTState
from src.modules import (
    step0_preprocess, step1_extraction, step2_linking,
    step4_retrieval, step5_reasoning, step6_path_generation,
    step7_verification, step8_synthesis, step9_safety, step10_logging
)
from src.utils.neo4j_connect import db_connector

logger = logging.getLogger("TEST_PIPELINE")

def print_section(title):
    print(f"\n{'='*60}\nüöÄ {title}\n{'='*60}")

def inspect_advanced_features(state: MedCOTState):
    print(f"\nüîç --- KI·ªÇM TRA T√çNH NƒÇNG N√ÇNG CAO ---")
    linked = [e for e in state.linked_entities if e.link_status == "linked"]
    if linked: 
        names = [c.best_candidate.preferred_name for c in linked[:3]]
        print(f"‚úÖ [Step 2] Linked: {names}")
    
    graph = state.graph_refs.get("ckg_subgraph", {})
    nodes = graph.get("nodes", [])
    edges = graph.get("edges", [])
    print(f"‚úÖ [Step 4] Subgraph: {len(nodes)} nodes, {len(edges)} edges")
    
    print(f"‚úÖ [Step 6] Candidate Paths Found: {len(state.candidate_paths)}")
    
    if state.verified_path:
        print(f"‚úÖ [Step 7] Verification Confidence: {state.global_confidence:.4f}")
    else:
        print("‚ö†Ô∏è [Step 7] No path passed verification threshold.")
        
    print(f"‚úÖ [Step 9] Safety Flags Triggered: {len(state.safety_flags)}")

def run_test_case(query, context=None):
    if db_connector is None: 
        logger.critical("Database connector not available. Aborting.")
        return
        
    start_t = time.time()
    state = MedCOTState(raw_query=query, patient_context=context)

    try:
        state = step0_preprocess.run(state)
        state = step1_extraction.run(state)
        state = step2_linking.run(state)
        state = step4_retrieval.run(state)
        state = step5_reasoning.run(state)
        state = step6_path_generation.run(state)
        state = step7_verification.run(state)
        state = step8_synthesis.run(state)
        state = step9_safety.run(state)
        state = step10_logging.run(state)
        
        print_section("K·∫æT QU·∫¢ CU·ªêI C√ôNG")
        print(f"‚ùì Query: {query}")
        print(f"\nüí° ANSWER:\n{state.final_answer}\n")
        
        inspect_advanced_features(state)
        
    except Exception as e:
        logger.exception(f"Pipeline crashed: {e}")
    
    print(f"\n‚è±Ô∏è Total Time: {time.time() - start_t:.2f}s")

if __name__ == "__main__":
    print_section("TEST CASE 1: SIMPLE QUERY")
    run_test_case("What are the treatments for hypertension?")

    print_section("TEST CASE 2: SAFETY & DDI CHECK")
    run_test_case("Is it safe to take Warfarin and Aspirin together?")



===== .\tests\test_step_0_preprocess.py =====
# tests/test_step_0_preprocess.py
from src.core.state import MedCOTState
from src.modules import step0_preprocess
from pprint import pprint

def main():
    print("="*50)
    print("üß™ B·∫ÆT ƒê·∫¶U TEST B∆Ø·ªöC 0: PREPROCESSING")
    print("="*50)

    test_query = "   B·ªánh nh√¢n John Doe, 50 tu·ªïi, c√≥ ti·ªÅn s·ª≠ ƒêTƒê type 2.   \n\n C·∫ßn t∆∞ v·∫•n th√™m.  "
    state = MedCOTState(raw_query=test_query)

    print(f"üîπ Query g·ªëc:\n'{state.raw_query}'")

    # Ch·∫°y b∆∞·ªõc 0
    state = step0_preprocess.run(state, enable_phi_redaction=True)

    print("\n‚úÖ K·∫æT QU·∫¢:")
    print(f"üî∏ Query ƒë√£ chu·∫©n h√≥a (·∫©n PHI):\n'{state.normalized_query}'")
    print("üî∏ C√°c c√¢u ƒë√£ t√°ch:")
    pprint(state.sentences)
    
    assert state.normalized_query == "B·ªánh nh√¢n <PERSON>, 50 tu·ªïi, c√≥ ti·ªÅn s·ª≠ ƒêTƒê type 2. \n\nC·∫ßn t∆∞ v·∫•n th√™m."
    assert len(state.sentences) > 1

    print("\nüéâ TEST B∆Ø·ªöC 0 TH√ÄNH C√îNG!")

if __name__ == "__main__":
    main()



===== .\tests\test_step_10_logging.py =====
# tests/test_step_10_logging.py
import os
from pathlib import Path
from src.core.state import MedCOTState
from src.modules import step10_logging

def main():
    print("="*50)
    print("üß™ B·∫ÆT ƒê·∫¶U TEST B∆Ø·ªöC 10: PROVENANCE LOGGING")
    print("="*50)
    
    state = MedCOTState(raw_query="final test")
    state.final_answer = "This is the final answer."
    state.global_confidence = 0.95

    print(f"üîπ Test v·ªõi query_id: {state.query_id}")

    # Ch·∫°y b∆∞·ªõc 10
    state = step10_logging.run(state)
    
    log_file_path = Path("output/audit_logs") / f"{state.query_id}.json"

    print("\n‚úÖ K·∫æT QU·∫¢:")
    print(f"üî∏ File log d·ª± ki·∫øn ƒë∆∞·ª£c t·∫°o t·∫°i: {log_file_path}")
    
    assert log_file_path.exists(), f"File log {log_file_path} kh√¥ng ƒë∆∞·ª£c t·∫°o!"
    
    # D·ªçn d·∫πp file test
    os.remove(log_file_path)
    print("üî∏ File log test ƒë√£ ƒë∆∞·ª£c x√≥a.")

    print("\nüéâ TEST B∆Ø·ªöC 10 TH√ÄNH C√îNG!")

if __name__ == "__main__":
    main()



===== .\tests\test_step_1_extraction.py =====
# tests/test_step_1_extraction.py
from src.core.state import MedCOTState
from src.modules import step1_extraction
from pprint import pprint

def main():
    print("="*50)
    print("üß™ B·∫ÆT ƒê·∫¶U TEST B∆Ø·ªöC 1: HYBRID EXTRACTION")
    print("="*50)

    # S·ª≠ d·ª•ng query ti·∫øng Anh ƒë·ªÉ ƒë·∫£m b·∫£o model ho·∫°t ƒë·ªông t·ªët nh·∫•t
    test_query = "The patient does not have fever, but has a history of hypertension and is taking metformin."
    state = MedCOTState(raw_query=test_query, normalized_query=test_query)

    print(f"üîπ Text ƒë·∫ßu v√†o:\n'{state.normalized_query}'")

    # Ch·∫°y b∆∞·ªõc 1
    state = step1_extraction.run(state)

    print("\n‚úÖ K·∫æT QU·∫¢:")
    print(f"üî∏ S·ªë th·ª±c th·ªÉ t√¨m th·∫•y: {len(state.mentions)}")
    for mention in state.mentions:
        print(f"  - Text: '{mention.text}', Label: {mention.label}, Attrs: {mention.attributes}")

    assert len(state.mentions) >= 2
    
    # Ki·ªÉm tra medspacy context (negation)
    fever_mention = next((m for m in state.mentions if "fever" in m.text.lower()), None)
    
    if fever_mention:
        # Code step 1 g√°n attrs['negated'] = True (thay v√¨ negated_existence)
        is_neg = fever_mention.attributes.get('negated')
        print(f"  > 'fever' attributes: {fever_mention.attributes}")
        assert is_neg is True, "Fever should be negated"

    print("\nüéâ TEST B∆Ø·ªöC 1 TH√ÄNH C√îNG!")

if __name__ == "__main__":
    main()



===== .\tests\test_step_2_linking.py =====
# tests/test_step_2_linking.py
from src.core.state import MedCOTState
from src.modules import step0_preprocess, step1_extraction, step2_linking
from src.utils.neo4j_connect import db_connector

def main():
    print("="*50)
    print("üß™ B·∫ÆT ƒê·∫¶U TEST B∆Ø·ªöC 2: ENTITY LINKING")
    print("="*50)

    if db_connector is None:
        print("‚ùå K·∫øt n·ªëi Neo4j th·∫•t b·∫°i. D·ª´ng test.")
        return

    # D√πng test case ti·∫øng Anh ƒë·ªÉ ƒë·∫£m b·∫£o c√≥ trong CKG dump
    test_query = "A patient with hypertension was treated with lisinopril."
    state = MedCOTState(raw_query=test_query)

    print(f"üîπ Query: '{test_query}'")

    # Ch·∫°y c√°c b∆∞·ªõc ph·ª• thu·ªôc
    state = step0_preprocess.run(state, enable_phi_redaction=False)
    state = step1_extraction.run(state)
    
    print(f"üî∏ ƒê√£ tr√≠ch xu·∫•t {len(state.mentions)} mentions.")

    # Ch·∫°y b∆∞·ªõc 2
    state = step2_linking.run(state)

    print("\n‚úÖ K·∫æT QU·∫¢:")
    for le in state.linked_entities:
        mention = le.source_mention
        if le.link_status == 'linked':
            best = le.best_candidate
            print(f"  [LINKED]   '{mention.text}' ({mention.kg_type}) -> {best.node_id} ('{best.preferred_name}')")
        else:
            print(f"  [UNLINKED] '{mention.text}' ({mention.kg_type})")

    print("\nüî∏ Seed Nodes cu·ªëi c√πng:")
    print(state.seed_nodes)

    assert len(state.seed_nodes) > 0, "Ph·∫£i link ƒë∆∞·ª£c √≠t nh·∫•t 1 node"
    
    if db_connector:
        db_connector.close()
    print("\nüéâ TEST B∆Ø·ªöC 2 TH√ÄNH C√îNG!")

if __name__ == "__main__":
    main()



===== .\tests\test_step_4_retrieval.py =====
# tests/test_step_4_retrieval.py
import json
from src.core.state import MedCOTState
from src.modules import step0_preprocess, step1_extraction, step2_linking, step4_retrieval
from src.utils.neo4j_connect import db_connector

def main():
    print("="*50)
    print("üß™ B·∫ÆT ƒê·∫¶U TEST B∆Ø·ªöC 4: SUBGRAPH RETRIEVAL")
    print("="*50)

    if db_connector is None:
        print("‚ùå K·∫øt n·ªëi Neo4j th·∫•t b·∫°i. D·ª´ng test.")
        return
        
    print("‚ÑπÔ∏è L∆ØU √ù: Test n√†y y√™u c·∫ßu b·∫°n ph·∫£i ch·∫°y 'python run/build_faiss_index.py' tr∆∞·ªõc.")

    test_query = "What are the treatments for hypertension?"
    state = MedCOTState(raw_query=test_query)

    print(f"üîπ Query: '{test_query}'")

    # Ch·∫°y c√°c b∆∞·ªõc ph·ª• thu·ªôc
    state = step0_preprocess.run(state)
    state = step1_extraction.run(state)
    state = step2_linking.run(state)
    
    if not state.seed_nodes:
        print("‚ùå Kh√¥ng t√¨m th·∫•y seed_nodes. D·ª´ng test.")
        return
    print(f"üî∏ Seed nodes t√¨m th·∫•y: {state.seed_nodes}")

    # Ch·∫°y b∆∞·ªõc 4
    state = step4_retrieval.run(state, top_k_nodes=100)

    subgraph = state.graph_refs.get("ckg_subgraph", {})
    nodes = subgraph.get('nodes', [])
    edges = subgraph.get('edges', [])

    print("\n‚úÖ K·∫æT QU·∫¢:")
    print(f"üî∏ Subgraph retrieved: {len(nodes)} nodes, {len(edges)} edges.")
    
    assert len(nodes) > 0, "Subgraph ph·∫£i c√≥ node"
    # assert len(edges) > 0, "Subgraph n√™n c√≥ c·∫°nh ƒë·ªÉ c√≥ √Ω nghƒ©a" # C√≥ th·ªÉ kh√¥ng c√≥ c·∫°nh n·∫øu c√°c node kh√¥ng li√™n quan tr·ª±c ti·∫øp

    if db_connector:
        db_connector.close()
        
    if len(nodes) > 0:
        print("\nüéâ TEST B∆Ø·ªöC 4 TH√ÄNH C√îNG!")
    else:
        print("\n‚ö†Ô∏è TEST B∆Ø·ªöC 4 HO√ÄN T·∫§T NH∆ØNG KH√îNG L·∫§Y ƒê∆Ø·ª¢C NODE N√ÄO. H√ÉY KI·ªÇM TRA L·∫†I INDEX V√Ä LOGIC.")


if __name__ == "__main__":
    main()



===== .\tests\test_step_5_reasoning.py =====
# tests/test_step_5_reasoning.py
import numpy as np
from src.core.state import MedCOTState
from src.modules import step0_preprocess, step1_extraction, step2_linking, step4_retrieval, step5_reasoning
from src.utils.neo4j_connect import db_connector

def main():
    print("="*50)
    print("üß™ B·∫ÆT ƒê·∫¶U TEST B∆Ø·ªöC 5: GCoT REASONING")
    print("="*50)

    if db_connector is None:
        print("‚ùå K·∫øt n·ªëi Neo4j th·∫•t b·∫°i. D·ª´ng test.")
        return
        
    test_query = "What are the treatments for pterygium?"
    state = MedCOTState(raw_query=test_query)

    state = step0_preprocess.run(state)
    state = step1_extraction.run(state)
    state = step2_linking.run(state)
    state = step4_retrieval.run(state, top_k_nodes=50)

    if not state.graph_refs.get("ckg_subgraph", {}).get("nodes"):
        print("‚ùå Subgraph r·ªóng. Kh√¥ng th·ªÉ ch·∫°y reasoning. D·ª´ng test.")
        return
        
    # Ch·∫°y b∆∞·ªõc 5 v·ªõi 2 b∆∞·ªõc suy lu·∫≠n
    state = step5_reasoning.run(state, num_think_steps=2)

    print("\n‚úÖ K·∫æT QU·∫¢:")
    thought_vectors = state.gcot.get('thought_vectors', [])
    print(f"üî∏ S·ªë l∆∞·ª£ng thought vectors ƒë√£ sinh: {len(thought_vectors)}")
    if thought_vectors:
        print(f"üî∏ Shape c·ªßa thought vector ƒë·∫ßu ti√™n: {np.array(thought_vectors[0]).shape}")

    final_embeddings = state.graph_refs.get('final_node_embeddings', {})
    print(f"üî∏ S·ªë lo·∫°i node c√≥ embedding cu·ªëi c√πng: {len(final_embeddings)}")

    # Test n√†y gi·ªù s·∫Ω PASS v√¨ code Step 5 ƒë√£ c√≥ v√≤ng l·∫∑p
    assert len(thought_vectors) == 2, "Ph·∫£i sinh ƒë·ªß s·ªë thought vectors"
    assert len(final_embeddings) > 0, "Ph·∫£i c√≥ final node embeddings"
    
    if db_connector:
        db_connector.close()

    print("\nüéâ TEST B∆Ø·ªöC 5 TH√ÄNH C√îNG!")

if __name__ == "__main__":
    main()



===== .\tests\test_step_6_path_generation.py =====
# tests/test_step_6_path_generation.py
from src.core.state import MedCOTState
from src.modules import (step0_preprocess, step1_extraction, step2_linking, 
                         step4_retrieval, step5_reasoning, step6_path_generation)
from src.utils.neo4j_connect import db_connector

def main():
    print("="*50)
    print("üß™ B·∫ÆT ƒê·∫¶U TEST B∆Ø·ªöC 6: PATH GENERATION")
    print("="*50)

    if db_connector is None:
        print("‚ùå K·∫øt n·ªëi Neo4j th·∫•t b·∫°i. D·ª´ng test.")
        return

    test_query = "What are the treatments for pterygium?"
    state = MedCOTState(raw_query=test_query)

    # Ch·∫°y c√°c b∆∞·ªõc ph·ª• thu·ªôc
    state = step0_preprocess.run(state)
    state = step1_extraction.run(state)
    state = step2_linking.run(state)
    state = step4_retrieval.run(state, top_k_nodes=50)
    state = step5_reasoning.run(state)

    if not state.graph_refs.get("final_node_embeddings"):
        print("‚ùå Kh√¥ng c√≥ embeddings. D·ª´ng test.")
        return

    # Ch·∫°y b∆∞·ªõc 6
    state = step6_path_generation.run(state, beam_width=3, max_path_length=3)

    print("\n‚úÖ K·∫æT QU·∫¢:")
    print(f"üî∏ S·ªë candidate paths t√¨m th·∫•y: {len(state.candidate_paths)}")

    if state.candidate_paths:
        print("üî∏ V√≠ d·ª• path ƒë·∫ßu ti√™n:")
        path_info = state.candidate_paths[0]
        path_str = " -> ".join([step['node_id'] for step in path_info['path']])
        print(f"  - Path: {path_str}")
        print(f"  - Score: {path_info['score']:.4f}")

    assert len(state.candidate_paths) >= 0 # C√≥ th·ªÉ kh√¥ng t√¨m th·∫•y path n√†o
    
    if db_connector:
        db_connector.close()
    
    print("\nüéâ TEST B∆Ø·ªöC 6 TH√ÄNH C√îNG!")

if __name__ == "__main__":
    main()



===== .\tests\test_step_7_verification.py =====
# tests/test_step_7_verification.py
from src.core.state import MedCOTState
from src.modules import (step0_preprocess, step1_extraction, step2_linking, 
                         step4_retrieval, step5_reasoning, step6_path_generation,
                         step7_verification)
from src.utils.neo4j_connect import db_connector

def main():
    print("="*50)
    print("üß™ B·∫ÆT ƒê·∫¶U TEST B∆Ø·ªöC 7: VERIFICATION")
    print("="*50)

    if db_connector is None: return

    test_query = "What are the treatments for hypertension?"
    state = MedCOTState(raw_query=test_query)

    # Ch·∫°y c√°c b∆∞·ªõc ph·ª• thu·ªôc
    state = step0_preprocess.run(state)
    state = step1_extraction.run(state)
    state = step2_linking.run(state)
    state = step4_retrieval.run(state, top_k_nodes=50)
    state = step5_reasoning.run(state)
    state = step6_path_generation.run(state)

    if not state.candidate_paths:
        print("‚ÑπÔ∏è Kh√¥ng c√≥ candidate path. D·ª´ng test.")
        return

    # Ch·∫°y b∆∞·ªõc 7
    state = step7_verification.run(state)

    print("\n‚úÖ K·∫æT QU·∫¢:")
    print(f"üî∏ Global Confidence: {state.global_confidence:.4f}")
    print(f"üî∏ Reasoning Mode: {state.reasoning_mode}")
    
    # S·ª≠a: Th√™m 'Cautious' v√† 'Safety-Alert' v√†o danh s√°ch h·ª£p l·ªá
    valid_modes = ["Graph-Strict", "Cautious", "Cautious-Generic", "Abstain", "Safety-Alert"]
    assert state.reasoning_mode in valid_modes, f"Mode {state.reasoning_mode} kh√¥ng h·ª£p l·ªá"
    
    if db_connector: db_connector.close()
    print("\nüéâ TEST B∆Ø·ªöC 7 TH√ÄNH C√îNG!")

if __name__ == "__main__":
    main()



===== .\tests\test_step_8_synthesis.py =====
# tests/test_step_8_synthesis.py
from src.core.state import MedCOTState
from src.modules import step8_synthesis

def main():
    print("="*50)
    print("üß™ B·∫ÆT ƒê·∫¶U TEST B∆Ø·ªöC 8: ANSWER SYNTHESIS")
    print("="*50)
    
    state = MedCOTState(raw_query="test", normalized_query="What causes Disease_A?")
    state.reasoning_mode = "Graph-Strict"
    state.verified_path = [
        {'source': 'Gene_X', 'edge': 'ASSOCIATED_WITH', 'target': 'Disease_A', 'step_confidence': 0.9}
    ]
    state.graph_refs["ckg_subgraph"] = {"nodes": [], "edges": []}

    state = step8_synthesis.run(state)

    print("\n‚úÖ K·∫æT QU·∫¢:")
    print(state.final_answer)
    
    # S·ª≠a: Assert l·ªèng h∆°n v√¨ format prompt thay ƒë·ªïi
    compiled = state.gcot.get('compiled_cot', '')
    assert "Verified Chain" in compiled or "Evidence" in compiled
    
    print("\nüéâ TEST B∆Ø·ªöC 8 TH√ÄNH C√îNG!")
    
if __name__ == "__main__":
    main()



===== .\tests\test_step_9_safety.py =====
# tests/test_step_9_safety.py
from src.core.state import MedCOTState, Mention
from src.modules import step9_safety

def main():
    print("="*50)
    print("üß™ B·∫ÆT ƒê·∫¶U TEST B∆Ø·ªöC 9: SAFETY ENGINE")
    print("="*50)
    
    state = MedCOTState(raw_query="test")
    state.final_answer = "Treat with Metformin and Warfarin."
    # Gi·∫£ l·∫≠p 2 thu·ªëc c√≥ t∆∞∆°ng t√°c
    state.mentions = [
        Mention(text="Metformin", label="drug", span=(0,0), score=1.0, source="dict"),
        Mention(text="Warfarin", label="drug", span=(0,0), score=1.0, source="dict"),
    ]

    state = step9_safety.run(state)

    print("\n‚úÖ K·∫æT QU·∫¢:")
    print(state.final_answer)
    print(state.safety_flags)
    
    if state.safety_flags:
        # S·ª≠a: Code safety m·ªõi g√°n type l√† 'CLINICAL_RISK'
        assert state.safety_flags[0]['type'] == 'CLINICAL_RISK'
        assert "SAFETY WARNINGS" in state.final_answer

    print("\nüéâ TEST B∆Ø·ªöC 9 TH√ÄNH C√îNG!")

if __name__ == "__main__":
    main()



===== .\tests\__init__.py =====


===== .\_utils\write_project_from_dump.py =====
import os
import re
from pathlib import Path

DUMP_FILE = "_utils/dump.txt"

def main():
    with open(DUMP_FILE, "r", encoding="utf-8") as f:
        content = f.read()

    # T√°ch theo pattern: ===== .\path\to\file =====
    pattern = re.compile(r"===== \.\\(.+?) =====\n", re.MULTILINE)
    matches = list(pattern.finditer(content))

    for i, match in enumerate(matches):
        rel_path = match.group(1).strip()
        start = match.end()
        end = matches[i + 1].start() if i + 1 < len(matches) else len(content)
        file_content = content[start:end].lstrip("\n")

        out_path = Path(rel_path)
        out_path.parent.mkdir(parents=True, exist_ok=True)

        with open(out_path, "w", encoding="utf-8") as f:
            f.write(file_content)

        print(f"‚úÖ Written: {out_path}")

if __name__ == "__main__":
    main()


===== DIRECTORY TREE =====
./
    .env.example
    app_demo.py
    docker-compose.yml
    main.py
    README.md
    requirements.txt
    setup.py
    .cache/
        arax_queries/
    configs/
        dataset_config.yaml
        evaluate_config.yaml
    scripts/
        0_preprocess_primekg.py
        1_generate_dataset.py
        2_build_faiss.py
        3_prepare_gnn.py
        4_train_gnn.py
        5_train_llm.py
        6_evaluate_models.py
        build_umls_db.py
        ingest_custom_data.py
        prepare_pubmedqa.py
        setup_import_primekg.sh
        train_aux_verifier.py
    src/
        __init__.py
        core/
            config.py
            state.py
            __init__.py
        modules/
            step0_preprocess.py
            step10_logging.py
            step1_extraction.py
            step2_linking.py
            step4_retrieval.py
            step5_reasoning.py
            step6_path_generation.py
            step7_verification.py
            step8_synthesis.py
            step9_safety.py
            __init__.py
        utils/
            arax_client.py
            faiss_search.py
            local_llm.py
            name_resolver.py
            neo4j_connect.py
            test_connection.py
            umls_normalizer.py
            __init__.py
    tests/
        test_full_pipeline.py
        test_step_0_preprocess.py
        test_step_10_logging.py
        test_step_1_extraction.py
        test_step_2_linking.py
        test_step_4_retrieval.py
        test_step_5_reasoning.py
        test_step_6_path_generation.py
        test_step_7_verification.py
        test_step_8_synthesis.py
        test_step_9_safety.py
        __init__.py
    _utils/
        dump.txt
        dump1.txt
        write_project_from_dump.py
